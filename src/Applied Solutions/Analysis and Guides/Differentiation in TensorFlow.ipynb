{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6facf42",
   "metadata": {},
   "source": [
    "Automatic differentiation is useful for implementing machine learning algorithms such as backpropagation for training neural networks. In this guide, we will explore ways to compute gradients with TensorFlow in eager execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf81a4e",
   "metadata": {},
   "source": [
    "## Gradients and Automatic Differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c223ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "317b3e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-28 18:53:47.361164: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "w = tf.Variable(tf.random.normal((3, 2)), name='w')\n",
    "b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n",
    "x = [[1., 2., 3.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61b81d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'w:0' shape=(3, 2) dtype=float32, numpy=\n",
       "array([[ 0.5979703 , -0.42594454],\n",
       "       [ 0.44298685,  0.67796963],\n",
       "       [-0.00147911, -1.0101731 ]], dtype=float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3a52961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'b:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e127f399",
   "metadata": {},
   "source": [
    "TensorFlow provides the tf.GradientTape API for automatic differentiation; that is, computing the gradient of a computation with respect to some inputs, usually tf.Variables. TensorFlow \"records\" relevant operations executed inside the context of a tf.GradientTape onto a \"tape\". TensorFlow then uses that tape to compute the gradients of a \"recorded\" computation using reverse mode differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca183440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w : [[ 1.4795066 -2.1005244]\n",
      " [ 2.9590132 -4.201049 ]\n",
      " [ 4.43852   -6.3015733]] \n",
      "\n",
      "b : [ 1.4795066 -2.1005244]\n",
      "\n",
      "\n",
      "w : [[ 1.4795066 -2.1005244]\n",
      " [ 2.9590132 -4.201049 ]\n",
      " [ 4.43852   -6.3015733]] \n",
      "\n",
      "b : [ 1.4795066 -2.1005244]\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y = x @ w + b\n",
    "    loss = tf.reduce_mean(y**2)\n",
    "\n",
    "# Using lists\n",
    "grad = tape.gradient(loss, [w, b])\n",
    "print(f'w : {grad[0]} \\n\\nb : {grad[1]}\\n\\n')\n",
    "\n",
    "# Using dictionaries\n",
    "grad = tape.gradient(loss, {'w': w, 'b': b})\n",
    "print(f'w : {grad[\"w\"]} \\n\\nb : {grad[\"b\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b63c4717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# A trainable variable\n",
    "x0 = tf.Variable(3.0, name='x0')\n",
    "\n",
    "# Not trainable\n",
    "x1 = tf.Variable(3.0, name='x1', trainable=False)\n",
    "\n",
    "# Not a Variable: A variable + tensor returns a tensor.\n",
    "x2 = tf.Variable(2.0, name='x2') + 1.0\n",
    "\n",
    "# Not a variable\n",
    "x3 = tf.constant(3.0, name='x3')\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = (x0**2) + (x1**2) + (x2**2)\n",
    "\n",
    "grad = tape.gradient(y, [x0, x1, x2, x3])\n",
    "\n",
    "for g in grad:\n",
    "    print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97579326",
   "metadata": {},
   "source": [
    "tape.watched_variables is used to get the list of all variables which tensorflow is watching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "674e1c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x0:0']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[var.name for var in tape.watched_variables()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e337f7",
   "metadata": {},
   "source": [
    "To disable the default behavior of watching all tf.Variables, set watch_accessed_variables=False when creating the gradient tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bb0c43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx0: None\n",
      "dy/dx1: 0.9999546\n"
     ]
    }
   ],
   "source": [
    "x0 = tf.Variable(0.0)\n",
    "x1 = tf.Variable(10.0)\n",
    "\n",
    "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "    # set only x1 to be watched not x0\n",
    "    tape.watch(x1)\n",
    "    y0 = tf.math.sin(x0)\n",
    "    y1 = tf.nn.softplus(x1)\n",
    "    y = y0 + y1\n",
    "    ys = tf.reduce_sum(y)\n",
    "\n",
    "grad = tape.gradient(ys, {'x0': x0, 'x1': x1})\n",
    "\n",
    "print('dy/dx0:', grad['x0'])\n",
    "print('dy/dx1:', grad['x1'].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf4e484",
   "metadata": {},
   "source": [
    "By default, the resources held by a GradientTape are released as soon as the GradientTape.gradient method is called. To compute multiple gradients over the same computation, create a gradient tape with persistent=True. This allows multiple calls to the gradient method as resources are released when the tape object is garbage collected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "702a26d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.6000000e+01 2.2958251e+08]\n",
      "[  4. 108.]\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([1, 3.0])\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch(x)\n",
    "    y = x * x * x * x \n",
    "    z = y * y * y * y\n",
    "\n",
    "print(tape.gradient(z, x).numpy()) \n",
    "print(tape.gradient(y, x).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79656d76",
   "metadata": {},
   "source": [
    "### Notes on performance\n",
    "\n",
    "- There is a tiny overhead associated with doing operations inside a gradient tape context. For most eager execution this will not be a noticeable cost, but you should still use tape context around the areas only where it is required.\n",
    "\n",
    "- Gradient tapes use memory to store intermediate results, including inputs and outputs, for use during the backwards pass.\n",
    "\n",
    "- For efficiency, some ops (like ReLU) don't need to keep their intermediate results and they are pruned during the forward pass. However, if you use persistent=True on your tape, nothing is discarded and your peak memory usage will be higher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc4ad82",
   "metadata": {},
   "source": [
    "## Control Flow\n",
    "Here a different variable is used on each branch of an if. The gradient only connects to the variable that was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a577834a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(1.0)\n",
    "\n",
    "v0 = tf.Variable(2.0)\n",
    "v1 = tf.Variable(2.0)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch(x)\n",
    "    if x > 0.0:\n",
    "        result = v0\n",
    "    else:\n",
    "        result = v1**2 \n",
    "\n",
    "dv0, dv1 = tape.gradient(result, [v0, v1])\n",
    "\n",
    "print(dv0)\n",
    "print(dv1)\n",
    "\n",
    "dx = tape.gradient(result, x)\n",
    "print(dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fa2f2f",
   "metadata": {},
   "source": [
    "Control statements themselves are not differentiable, so they are invisible to gradient-based optimizers. Depending on the value of x in the above example, the tape either records result = v0 or result = v1**2. The gradient with respect to x is always None."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
