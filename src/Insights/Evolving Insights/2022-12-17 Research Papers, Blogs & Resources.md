---
title: Research Papers, Blogs & Resources
date: 2022-12-17 00:00:00
description: We are here to help you optimise the way you do business and scale your business to the globe using Quality Data, Machine Learning and Automation.
tags: 
 - Data Science
 - Machine Learning
 - Deep Learning
---

## Research Papers and Blogs

### Graph Based Method
1. [DeepWalk: Online Learning of Social Representations](http://www.perozzi.net/publications/14_kdd_deepwalk.pdf)
2. [HOPE : Asymmetric Transitivity Preserving Graph Embedding](https://www.kdd.org/kdd2016/papers/files/rfp0184-ouA.pdf)
3. [Feature Extraction for Graphs](https://towardsdatascience.com/feature-extraction-for-graphs-625f4c5fb8cd)
4. [A overview of FE in Graphs](https://www.mdpi.com/2504-4990/2/4/36)

### Discipline the method
1. [A DISCIPLINED APPROACH TO NEURAL NETWORK HYPER-PARAMETERS: PART 1 – LEARNING RATE, BATCH SIZE, MOMENTUM, AND WEIGHT DECAY](https://arxiv.org/pdf/1803.09820.pdf)

### Loss Functions
1. [Comprehensive Survey of Loss Functions in Machine Learning](https://faculty.ist.psu.edu/vhonavar/Courses/ds310/lossfunc.pdf)
2. Classification
	1. [Study of deep learning loss functions for multi-label remote sensing image classification](https://arxiv.org/pdf/2009.13935.pdf)
	2. [Recall Loss for Semantic Segmentation](https://paperswithcode.com/paper/striking-the-right-balance-recall-loss-for)
	3. [Focal Loss for Dense Object Detection](https://paperswithcode.com/method/focal-loss)
	4. [Class Distance Weighted Cross-Entropy Loss](https://paperswithcode.com/paper/class-distance-weighted-cross-entropy-loss)
	5. [Squared Earth Mover’s Distance-based Loss for Training Deep Neural Networks](https://www.arxiv-vanity.com/papers/1611.05916/)
3. Regression
	1. [Regression Based Loss Functions for Time Series Forecasting](https://arxiv.org/pdf/2211.02989.pdf)

### Tech Blogs
1. [AirBnB Engineering](https://medium.com/airbnb-engineering)
2. [Spotify Research](https://research.atspotify.com/blog/)
3. [Netflix Research](https://research.netflix.com/)
4. [DoorDash ML Blog](https://doordash.engineering/category/data-science-and-machine-learning/)
5. [Uber Engineering](https://www.uber.com/blog/honolulu/engineering/)
6. [Lyft Engineering](https://eng.lyft.com/tagged/data-science)
7. [Shopify Engineering](https://shopify.engineering/topics/data-science-engineering)
8. [Meta Engineering](https://engineering.fb.com/)
9. [LinkedIn Engineering](https://engineering.linkedin.com/blog) 
10. [Kaggle Competition Blog](https://medium.com/kaggle-blog/tagged/kaggle-competition)

### Knowledge Distillation
1. [https://arxiv.org/pdf/2006.05525.pdf](https://arxiv.org/pdf/2006.05525.pdf)  
2. [https://arxiv.org/pdf/1503.02531.pdf](https://arxiv.org/pdf/1503.02531.pdf)  
3. [https://arxiv.org/pdf/1910.01108.pdf](https://arxiv.org/pdf/1910.01108.pdf)

### Modalities and Mixture of Experts
- [Language-Image Mixture of Experts](https://arxiv.org/abs/2206.02770)
- [Google AI blog limoe-learning-multiple-modalities-with](https://ai.googleblog.com/2022/06/limoe-learning-multiple-modalities-with.html)
- [Youtube](https://youtu.be/i-V33KEwX00?list=PLINDPtcaHmLY83C5TRf9-icEeW_jwTAIa)

### Deep learning for Tabular data
- [Revisiting Deep Learning Models for Tabular Data](https://arxiv.org/pdf/2106.11959.pdf)
- [wnadb.ai](https://wandb.ai/sauravm/RTDL/reports/Revisiting-Deep-Learning-Models-for-Tabular-Data--VmlldzoxNDE1Njk0)

### Embeddings
- [Time2Vec: Learning a Vector Representation of Time ](https://arxiv.org/pdf/1907.05321.pdf)

### GANs
1. GAN = Generative model + Adversarial model (This model judges the Generative model)
2. GAN tricks and Hacks: https://github.com/soumith/ganhacks
3. https://medium.com/@jonathan_hui/gan-some-cool-applications-of-gans-4c9ecca35900
4. [MNIST GAN](https://medium.com/datadriveninvestor/generative-adversarial-network-gan-using-keras-ce1c05cfdfd3
5. [deepgenerativemodels research](https://deepgenerativemodels.github.io/)
6. https://medium.com/@sanjay035/sketch-to-color-anime-translation-using-generative-adversarial-networks-gans-8f4f69594aeb

### Encoder Decoders 
1. [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)
2. [Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078)
3. [Deep Visual-Semantic Alignments for Generating Image Descriptions](https://arxiv.org/abs/1412.2306)
4. https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html
5. https://ai.googleblog.com/2018/05/smart-compose-using-neural-networks-to.html
6. https://medium.com/@martin.monperrus/sequence-to-sequence-learning-program-repair-e39dc5c0119b
7. https://towardsdatascience.com/image-captioning-with-keras-teaching-computers-to-describe-pictures-c88a46a311b8
8. http://www.manythings.org/anki/
9. https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py

### AutoEncoders
- http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/
- [https://en.wikipedia.org/wiki/Autoencoder](https://en.wikipedia.org/wiki/Autoencoder)

### Attention
1. [NEURAL MACHINE TRANSLATION](https://arxiv.org/pdf/1409.0473.pdf)
2. [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/pdf/1502.03044.pdf)
3. [Attention in Deep Networks with Keras](https://towardsdatascience.com/light-on-math-ml-attention-with-keras-dc8dbc1fad39)
4. Tx is a hyperparam, explained in a 2015 paper [1508.04025](https://arxiv.org/pdf/1409.0473.pdf)  and not in the original 2014 Attention Models paper [1409.0473.pdf](https://arxiv.org/pdf/1409.0473.pdf). In the 2014 paper, Tx is the length of the whole input sentence.  

### Transformers
1. https://jalammar.github.io/illustrated-transformer/
2. [TabNet Transformer](https://paperswithcode.com/method/tabnet)
3. [Pytorch TabNet Youtube](https://www.youtube.com/watch?v=ysBaZO8YmX8&ab_channel=AbhishekThakur)

### Explainanle AI
1. [Explaining the Predictions of Any Classifier](https://arxiv.org/pdf/1602.04938.pdf)
2. [Integrated Gradients](https://www.tensorflow.org/tutorials/interpretability/integrated_gradients)
3. [Robustness of Interpretability Methods](https://arxiv.org/pdf/1806.08049.pdf)
4. [Interpretable Machine Learning Web Book](https://christophm.github.io/interpretable-ml-book/index.html)
5. [LIME TDS 1](https://towardsdatascience.com/lime-how-to-interpret-machine-learning-models-with-python-94b0e7e4432e) | [LIME Blog](https://towardsdatascience.com/interpreting-image-classification-model-with-lime-1e7064a2f2e5) | [LIME Text Explain](https://towardsdatascience.com/what-makes-your-question-insincere-in-quora-26ee7658b010)

### Siamese Networks
- [Intro](https://towardsdatascience.com/siamese-networks-introduction-and-implementation-2140e3443dee)

## Python Libraries

### Graph Analysis
- [networkx](https://networkx.org/documentation/stable/reference/index.html)

### Web Developement 
- [pyweb](https://www.pyweb.io/index.html)
- [anvil.works](https://anvil.works/)
- [pynecone](https://pynecone.io/)
- [streamlit](https://streamlit.io/)

### Explainable AI
- [explainerdashboard](https://explainerdashboard.readthedocs.io/en/latest/index.html)
- [omnixai](https://opensource.salesforce.com/OmniXAI/latest/index.html)
- [InterpretML](https://interpret.ml/docs/getting-started)
- [ELI5](https://eli5.readthedocs.io/en/latest/)
- [Shapash](https://shapash.readthedocs.io/en/latest/)
- [LIME](https://lime-ml.readthedocs.io/en/latest/)
- [SHAP](https://shap.readthedocs.io/en/latest/)

### Trading
- [quantstats](https://github.com/ranaroussi/quantstats)
- [alphalens-reloaded](https://github.com/stefan-jansen/alphalens-reloaded)
- [alphalens-reloaded](https://github.com/stefan-jansen/alphalens-reloaded)
- [tsfresh](https://tsfresh.readthedocs.io/en/latest/text/forecasting.html#parameters-and-implementation-notes)
- [scalecast](https://scalecast.readthedocs.io/en/latest/)

### GitHub pages
- [ml-tooling/best-of-python](https://github.com/ml-tooling/best-of-python)
- [ml-tooling/best-of-ml-python](https://github.com/ml-tooling/best-of-ml-python)
- [ml-tooling/ml-workspace](https://github.com/ml-tooling/ml-workspace)

### Other Resourses
- https://mlops.toys/data-versioning
- https://madewithml.com/courses/mlops/labeling/
- https://mlflow.org/docs/latest/index.html
- https://umap-learn.readthedocs.io/en/latest/
- https://docs.feast.dev/
- https://www.timvink.nl/reproducible-reports-with-mkdocs/
- [Kedro DVC setup](https://youtu.be/ZTrFpeTCnc0)
	```
	kedro new
	```
	```
	kedro mlflow init
	kedro docker init
	echo "dvc" >> src/requirements.txt
	pip install -r src/requirements.txt
	```
	```
	dvc init
	dvc add data/01_raw
	git rm data/01_raw/.gitkeep
	git add -u
	git commit -m "spec the datasets"
	dvc add data/01_raw
	git add data/01_raw.dvc
	git add data/.gitignore
	git commit -m "Added input data to DVC"
	```
