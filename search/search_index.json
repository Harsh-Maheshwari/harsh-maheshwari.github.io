{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Decide With ML","text":""},{"location":"#insights-production-automation","title":"Insights \u00b7 Production \u00b7 Automation","text":"<ul> <li>Extract Insight from Commercial and Open Source Data</li> <li>Bring Automation at the forefront of your Business</li> <li>Build Machine Learning models for Production</li> <li>Advance your business from generating data to using data</li> <li>Capture millions of new leads for your business using data</li> <li>Expand your business into the Industry 4.0 paradigm</li> </ul>"},{"location":"#for-businesses","title":"For Businesses","text":"<p>Whether you are just starting your journey as a business enthusiast or a well seasoned businessman we have a solution for you. We bring technology close to traditional business techniques and we bring core business close to technology. Data is beautiful, It can help solve and understand any problem. We provide different services which can help your grow faster and scale without any issues</p>"},{"location":"#the-process","title":"The Process","text":"<p>Discover The Potential</p> <ol> <li>Initial Consultation: We schedule an initial consultation with the client to understand their business needs and goals for the project. During this consultation, we gather information about their data, current processes, and desired outcomes. We also discuss potential cost savings or revenue increases that can be achieved through the use of machine learning.</li> <li>Data Analysis: We conduct a thorough analysis of the client's data to understand its structure, quality, and potential for use in machine learning models. We also identify any inefficiencies in the client's current data storing and database practices that can be improved through the use of best practices. Additionally, we will discuss the use of open source data and publicly available datasets that can be used to enhance the client's data and improve the performance of the models.</li> <li>Problem Identification: We work with the client to identify any industry-specific or business-specific problems that can be addressed. This includes analyzing the client's business operations and identifying key areas where machine learning / automation can improve efficiency, reduce costs, or increase revenue.</li> <li>Feature Engineering: Based on our experience and your domain knowledge, we create new features and modify existing features in your data that can help improve the performance of a machine learning model. It helps to extract valuable insights and information from raw data that ultimately lead to increased efficiency and accuracy of model and better revenue for your business.</li> <li>Model Selection: Based on the data analysis and the client's goals, we select the appropriate machine learning models and techniques to be used in the project. We also ensure that the selected models are cost-effective and have the potential to generate a positive return on investment for the client.</li> <li>Model Development: We develop and train the chosen models using the client's data and open source data or other publicly available datasets. This may include data preprocessing, feature engineering, and model tuning. We also implement automated processes to reduce manual labor and costs for the client. During this step we also make sure that the data is stored and handled in compliance with the industry standards and best practices for data storage and data base.</li> <li>Model Evaluation: We evaluate the performance of the developed models and make any necessary adjustments to improve their accuracy. We also provide the client with metrics to measure the financial impact of the models and the benefits of implementing best data storing and database practices.</li> <li>Deployment: We deploy the final model(s) in a production environment, and provide the client with instructions on how to use and maintain them. We also provide support to integrate the models with the client's existing systems and processes and data storing and database infrastructure.</li> <li>Maintenance and Support: We provide ongoing support and maintenance for the deployed models to ensure they continue to perform well over time. We also monitor the financial impact of the models and provide updates to the client on cost savings or revenue increases achieved. This also includes addressing any new problems or issues that arise in the industry or business.</li> </ol>"},{"location":"#request-consultation","title":"Request Consultation","text":"<p>Lets connect and build something great</p> <ul> <li>Request a Free Consultation Session to review how we can enhance your business using Data Insights, Machine Learning and Automation.</li> <li>Don't want to wait for the call invite! Don't worry we have got you covered! Fill in the Quick Form and we will start helping you as quickly as possible</li> </ul>"},{"location":"#implemented-solutions","title":"Implemented Solutions","text":"High Impact Solution <ul> <li>Hydroponics Forecasting</li> <li>Automation Template</li> <li>Cancer Treatment</li> <li>Quora Question Pairs</li> </ul>"},{"location":"#solutions-catalog","title":"Solutions Catalog","text":""},{"location":"#data-solutions","title":"Data Solutions","text":"Solutions that directly help you solve business problems! <ul> <li>Data Science for MSMEs : MSMEs (Micro, Small &amp; Medium Enterprises) are often data-rich but insight-poor, meaning they have data on their customers and operations but aren\u2019t able to easily make sense of it. We are here to solve this using our data analytics services which help businesses gain a better understanding of their data. This services also provide businesses with the necessary industry knowledge to identify how they can improve their business operations.</li> <li>Forecasting : All around the glob companies are increasingly relying on data-driven insights when making business decisions. We provide data science expertise necessary to create accurate predictive models and establish advanced data analytics capability that can help you boost your bottom line through forecasting services. These models can be used to anticipate trends and make predictions about future outcomes, which helps businesses stay ahead of the competition and make data-informed decisions.</li> <li>Data Science Consulting : Data is one of the most valuable assets for any business, but many companies lack the expertise needed to make use of their data and derive meaningful insights from it. Consultancy service is focused on helping businesses leverage data in their operations. we help in making data-driven decisions, improve operations, and gain a competitive edge.</li> </ul>"},{"location":"#product-strategy-analysis","title":"Product &amp; Strategy Analysis","text":"Solutions that help in developement of omni-channel products and strategy! <ul> <li>Product and Market Development : Deep-dived and customised growth strategy to accelerate your business. Increase earnings with optimised operations, better returns and higher sales. Efficient and high ROI marketing strategies and digital campaigns. Data-driven strategies for improved customer conversions</li> <li>Data Integration Software Development : We create Data integration platforms which allow businesses to collect, integrate, centralise, clean, analyse, access, and combine data from various sources like streaming data, cloud data, external data systems, etc. and then make this data more accessible for analysis by analysts and data scientists.</li> </ul>"},{"location":"#services","title":"Services","text":"Help is what we offer, Education and Upskilling is a right! <ul> <li>Data Science Education &amp; Training : We offer upskilling services in data science and machine learning to help aspiring and working data scientists alike. We offer 1 on 1 training in a self paced environment. We also provide company wide training to upskill employees in and around data technology.</li> <li>Data Science Mock Interviews : We offer candidates help in preparation for Data Analysis, Data Science and Machine Learning Interviews with specific focuss on use cases for the target company and role. </li> </ul>"},{"location":"#for-knowlege-professionals","title":"For Knowlege Professionals","text":"Insights <ul> <li>Evolving Insights : Contains blogs about the topics which we are learning as we grow in experience. It includes some of the best practices that uplifts the quality of product and most importantly are very easy to follow just with some discipline. The efforts are less and results are huge.</li> <li>Industry Insights : Contains blogs about the topics which we are learning as we grow our understanding of different domains and industries. It helps understand the overall industry better and keeps us upto date industry specific practice and problems.</li> <li>Technical Insights : Contains blogs about the introductory topics and tutorials about some of the widely used tools. These insights are written for ethusiasts to undersatand basics and advanced topic, which have existed and will keep having importance in the technical world</li> <li>Tips &amp; Tricks : Contains information which can be used as easy to access  research material and one liners. These can help you with advanced use cases in certain softwares and tools.</li> <li>Knowledge : Contains a basic machine learning course which is a bit more structured </li> </ul>"},{"location":"#schedule-a-call","title":"Schedule a Call","text":"<p>Lets connect and build something great</p> <ul> <li>Request a Free Consultation Session to review how we can enhance your business using Data Insights, Machine Learning and Automation.</li> <li>Don't want to wait for the call invite! Don't worry we have got you covered! Fill in the Quick Form and we will start helping you as quickly as possible</li> </ul>"},{"location":"#choose-us-because-we-are-committed-to-your-success","title":"Choose Us Because - We are Committed to your Success","text":"<p>We help you make the most of your existing technology, tools and software by developing in-house integration technologies using python. These all-embracing integrations enables you to fast-track your growth without wasting any time and money on costly and tedious proprietary connectors and let\u2019s you develop over your existing systems. We give access to a library of already developed ready-made solutions, customised offerings to suit your requirements for efficacious business management </p>"},{"location":"#our-beliefs","title":"Our Beliefs","text":"<p> As a technology heavy company, we strive to be at the forefront of experimentation with new research, new ideas and development of an open source community. And our beliefs have helped us in achieving this task. The Go-Giver, written by Bob Burg and John David Mann, has been a cornerstone philosophy at our company. This business fable emphasizes the importance of giving in order to achieve success has been a guiding principle for our business practices. By focusing on giving more in value than we take in payment, we have been able to build strong, mutually beneficial relationships with our customers, community, and employees. We have implemented this principle in various ways throughout our business practices, such as by consistently over-delivering on our promises to customers, and by actively seeking out ways to give back to our community. This approach has not only led to greater success and fulfillment for our business, but it has also allowed us to make a positive impact in the world. We are proud to be a Go-Giver company and continue to strive towards this principle in all that we do. Quoting one more important line from the book : \"The more you know, the less you need to say.\" This line emphasizes the importance of listening, learning, and understanding in order to build effective relationships. It suggests that the people who truly know their stuff, don't feel the need to constantly prove it by talking about it. Instead, they listen attentively, ask questions, and take the time to truly understand the needs and perspectives of others. This line is a powerful reminder that effective communication is not just about talking, but also about listening and understanding. As a business we have tried to integrate this in our practices. These values have guided us along a sustainable path towards success and happiness of doing something good in the world. We humbly suggest you to give this book a try and see your life change. </p>"},{"location":"#about-the-founder","title":"About the Founder","text":"<p>Harsh Maheshwari - Founder and CEO </p> <p>Harsh is an IIT Bombay graduate and an enthusiastic Data Scientist with hands-on experience in designing, building and managing the complex production lifecycle of end-to-end machine learning pipelines. He takes pride in leading by showing and knowing how, ready to roll up the sleeves and do anything to get the job done and well - no task is menial or tedious! </p> <p>Know more in  My Journey Blog &amp; Connect with us on  Email   |   LinkedIn   |   Medium </p>"},{"location":"Applied%20Solutions/Cancer%20Treatment/","title":"Cancer Treatment","text":"<pre><code>## importing libraries\nfrom utils.base import *\ncss()\n\n## Defining constants\ntitle = \"Cancer Treatment\"\nraw_training_text_file_path = f\"data/{title}/01_raw/training_text.csv\"\nraw_training_variants_file_path = f\"data/{title}/01_raw/training_variants.csv\"\nraw_training_text_dataprep_report_path = f\"Assets/reports/{title}/raw_training_text_dataprep.html\"\nraw_training_variants_dataprep_report_path = f\"Assets/reports/{title}/raw_training_variants_dataprep.html\"\n</code></pre> <pre><code>df_variants = pd.read_csv(raw_training_variants_file_path)\nglimpse(df_variants)\ncreate_report(df_variants, title=title, progress=False).save(raw_training_variants_dataprep_report_path)\n</code></pre> <pre>\n<code>Shape:  (3321, 4)\n</code>\n</pre> Info ID Gene Variation Class dtype int64 object object int64 # of Nas 0 0 0 0 % of Na 0.0 0.0 0.0 0.0 # of uniques 3321 264 2996 9 % of unique 100.0 7.949413 90.213791 0.271003 Count of dtypes int64 2 object 2 Stats ID Class count 3321.000000 3321.000000 mean 1660.000000 4.365854 std 958.834449 2.309781 min 0.000000 1.000000 25% 830.000000 2.000000 50% 1660.000000 4.000000 75% 2490.000000 7.000000 max 3320.000000 9.000000 Sample ID Gene Variation Class 385 385 TP53 R249W 4 443 443 TP53 G244R 4 2488 2488 BRCA1 W1782C 5 644 644 CDKN2A E69G 4 2084 2084 MYD88 L265P 7 329 329 ROS1 MSN-ROS1 Fusion 2 1604 1604 VHL L188Q 4 1113 1113 FANCA Deletion 1 2543 2543 BRCA1 C27A 4 1628 1628 VHL R167Q 4 <pre>\n<code>Report has been saved to Assets/reports/Cancer Treatment/raw_training_variants_dataprep.html!\n</code>\n</pre> <p>Generated Report</p> <pre><code>df_text = pd.read_csv(raw_training_text_file_path, sep=\"\\|\\|\", engine=\"python\", names=[\"ID\",\"TEXT\"], skiprows=1)\nglimpse(df_text)\ncreate_report(df_text, title=title, progress=False).save(raw_training_text_dataprep_report_path)\n</code></pre> <pre>\n<code>Shape:  (3321, 2)\n</code>\n</pre> Info ID TEXT dtype int64 object # of Nas 0 5 % of Na 0.0 0.150557 # of uniques 3321 1920 % of unique 100.0 57.813911 Count of dtypes int64 1 object 1 Stats ID count 3321.000000 mean 1660.000000 std 958.834449 min 0.000000 25% 830.000000 50% 1660.000000 75% 2490.000000 max 3320.000000 Sample ID TEXT 801 801 Recent efforts to comprehensively characterize... 1945 1945 Mycosis fungoides and S\u00e9zary syndrome are prim... 1526 1526 Anaplastic lymphoma kinase (ALK) tyrosine kina... 2735 2735 Abstract' ' ' ' Mutations in the B-Raf gene ha... 1854 1854 The karyotypic chaos exhibited by human epithe... 2786 2786 Mutation screening of the breast and ovarian c... 665 665 Inherited mutations affecting the INK4a/ARF lo... 2234 2234 The tumor suppressor gene PTEN is frequently m... 430 430 The human p53 gene encodes a nuclear protein t... 1278 1278 The three-dimensional structure of the complex... <pre>\n<code>Report has been saved to Assets/reports/Cancer Treatment/raw_training_text_dataprep.html!\n</code>\n</pre> <p>Generated Report</p> <pre><code># loading stop words from nltk library\nstop_words = set(stopwords.words('english'))\n\ndef nlp_preprocessing(df_text,total_text, index, column):\n    if type(total_text) is not int:\n        string = \"\"\n        # replace every special char with space\n        total_text = re.sub('[^a-zA-Z0-9\\n]', ' ', total_text)\n        # replace multiple spaces with single space\n        total_text = re.sub('\\s+',' ', total_text)\n        # converting all the chars into lower-case.\n        total_text = total_text.lower()\n\n        for word in total_text.split():\n        # if the word is a not a stop word then retain that word from the data\n            if not word in stop_words:\n                string += word + \" \"\n\n        df_text[column][index] = string\n</code></pre> <pre><code>#text processing stage.\nstart_time = time.perf_counter()\nfor index, row in df_text.iterrows():\n    if type(row['TEXT']) is str:\n        nlp_preprocessing(df_text, row['TEXT'], index, 'TEXT')\n    else:\n        print(\"there is no text description for id:\",index)\nprint('Time took for preprocessing the text :',time.perf_counter() - start_time, \"seconds\")\n</code></pre> <pre>\n<code>there is no text description for id: 1109\nthere is no text description for id: 1277\nthere is no text description for id: 1407\nthere is no text description for id: 1639\nthere is no text description for id: 2755\nTime took for preprocessing the text : 24.476304084999995 seconds\n</code>\n</pre> <pre><code># merging both gene_variations and text data based on ID\nresult = pd.merge(df_variants, df_text,on='ID', how='left')\nresult.head()\n</code></pre> ID Gene Variation Class TEXT 0 0 FAM58A Truncating Mutations 1 cyclin dependent kinases cdks regulate variety... 1 1 CBL W802* 2 abstract background non small cell lung cancer... 2 2 CBL Q249E 2 abstract background non small cell lung cancer... 3 3 CBL N454D 3 recent evidence demonstrated acquired uniparen... 4 4 CBL L399V 4 oncogenic mutations monomeric casitas b lineag... <pre><code>result[result.isnull().any(axis=1)]\n</code></pre> ID Gene Variation Class TEXT 1109 1109 FANCA S1088F 1 NaN 1277 1277 ARID5B Truncating Mutations 1 NaN 1407 1407 FGFR3 K508M 6 NaN 1639 1639 FLT1 Amplification 6 NaN 2755 2755 BRAF G596C 7 NaN <pre><code>result.loc[result['TEXT'].isnull(),'TEXT'] = result['Gene'] +' '+result['Variation']\n</code></pre> <pre><code>result[result['ID'].isin([1109,1277,1407,1639,2755])]\n</code></pre> ID Gene Variation Class TEXT 1109 1109 FANCA S1088F 1 FANCA S1088F 1277 1277 ARID5B Truncating Mutations 1 ARID5B Truncating Mutations 1407 1407 FGFR3 K508M 6 FGFR3 K508M 1639 1639 FLT1 Amplification 6 FLT1 Amplification 2755 2755 BRAF G596C 7 BRAF G596C <pre><code>y_true = result['Class'].values\nresult.Gene      = result.Gene.str.replace('\\s+', '_')\nresult.Variation = result.Variation.str.replace('\\s+', '_')\n\n# split by maintaining same distribution of output varaible 'y_true' [stratify=y_true]\nX_train, test_df, y_train, y_test = train_test_split(result, y_true, stratify=y_true, test_size=0.2)\n# split by maintaining same distribution of output varaible 'y_train' [stratify=y_train]\ntrain_df, cv_df, y_train, y_cv = train_test_split(X_train, y_train, stratify=y_train, test_size=0.2)\n</code></pre> <p> We split the data into train, test and cross validation data sets, preserving the ratio of class distribution in the original data set  </p> <pre><code>print('Number of data points in train data:', train_df.shape[0])\nprint('Number of data points in test data:', test_df.shape[0])\nprint('Number of data points in cross validation data:', cv_df.shape[0])\n</code></pre> <pre>\n<code>Number of data points in train data: 2124\nNumber of data points in test data: 665\nNumber of data points in cross validation data: 532\n</code>\n</pre> <pre><code>train_class_distribution = train_df['Class'].value_counts().sort_index()\ncv_class_distribution = cv_df['Class'].value_counts().sort_index()\ntest_class_distribution = test_df['Class'].value_counts().sort_index()\n\ntrain_class_distribution_norm = train_class_distribution/train_class_distribution.sum()\ncv_class_distribution_norm = cv_class_distribution/cv_class_distribution.sum()\ntest_class_distribution_norm = test_class_distribution/test_class_distribution.sum()\n\nnorm_df = pd.concat([train_class_distribution_norm, cv_class_distribution_norm, test_class_distribution_norm], axis=1)\nnorm_df.columns = ['train','cv','test']\nax = norm_df.plot(kind='bar', figsize=(16,8))\n</code></pre> <p>The distribution is really nice and even amongst the train, test and cross validation</p> <pre><code>test_data_len = test_df.shape[0]\ncv_data_len = cv_df.shape[0]\n\ncv_predicted_y = np.zeros((cv_data_len,9))\nfor i in range(cv_data_len):\n\n    rand_probs = np.random.rand(1,9)\n    cv_predicted_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])\nprint(\"Log loss on Cross Validation Data using Random Model\",log_loss(y_cv,cv_predicted_y, eps=1e-15))\n\ntest_predicted_y = np.zeros((test_data_len,9))\nfor i in range(test_data_len):\n    rand_probs = np.random.rand(1,9)\n    test_predicted_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])\nprint(\"Log loss on Test Data using Random Model\",log_loss(y_test,test_predicted_y, eps=1e-15))\n\npredicted_y = np.argmax(test_predicted_y, axis=1)\nplot_confusion_matrix(y_test, predicted_y+1)\n</code></pre> <pre>\n<code>Log loss on Cross Validation Data using Random Model 2.503933530991029\nLog loss on Test Data using Random Model 2.582919708614705\n</code>\n</pre> <pre><code>unique_genes = train_df['Gene'].value_counts()\ns = sum(unique_genes.values);\nh = unique_genes.values/s;\nc = np.cumsum(h)\n\nfig,ax = plt.subplots(1,2, figsize=(16,8))\nax[0].plot(h)\nax[0].set_xlabel('Index of a Gene')\nax[0].set_ylabel('Number of Occurances')\nax[0].set_title(\"Histrogram of Genes\")\nax[0].grid()\n\nax[1].plot(c)\nax[1].set_xlabel('Index of a Gene')\nax[1].set_ylabel('Cumulative fraction')\nax[1].set_title(\"Cumulative distribution of Genes\")\nax[1].grid()\nplt.tight_layout()\n\nunique_variations = train_df['Variation'].value_counts()\ns = sum(unique_variations.values);\nh = unique_variations.values/s;\nc = np.cumsum(h)\n\nfig, ax = plt.subplots(1,2, figsize=(16,8))\nax[0].plot(h)\nax[0].set_xlabel('Index of a Variation')\nax[0].set_ylabel('Number of Occurances')\nax[0].set_title('Histrogram of Variations')\nax[0].grid()\n\nax[1].plot(c)\nax[1].set_xlabel('Cumulative fraction')\nax[1].set_ylabel('Number of Occurances')\nax[1].set_title('Cumulative distribution of Variations')\nax[1].grid()\nplt.tight_layout()\n</code></pre> <pre><code># Response coding\ntarget = 'Class' \nfeature = 'Gene' \nlaplace_alpha = 10\n\ntrain_gene_feature_responseCoding, cv_gene_feature_responseCoding, test_gene_feature_responseCoding = get_response_coded_feature(train_df, cv_df, test_df, feature, target, laplace_alpha)\nprint(\"train_gene_feature_responseCoding : shape of feature:\", train_gene_feature_responseCoding.shape)\n\n# one hot encoding\ngene_vectorizer = CountVectorizer()\ntrain_gene_feature_onehotCoding = gene_vectorizer.fit_transform(train_df[feature])\ntest_gene_feature_onehotCoding = gene_vectorizer.transform(test_df[feature])\ncv_gene_feature_onehotCoding = gene_vectorizer.transform(cv_df[feature])\n\nprint(\"train_gene_feature_onehotCoding : shape of feature:\", train_gene_feature_onehotCoding.shape)\n</code></pre> <pre>\n<code>train_gene_feature_responseCoding : shape of feature: (2124, 9)\ntrain_gene_feature_onehotCoding : shape of feature: (2124, 229)\n</code>\n</pre> <pre><code># Response coding\ntarget = 'Class' \nfeature = 'Variation' \nlaplace_alpha = 10\n\ntrain_variation_feature_responseCoding, cv_variation_feature_responseCoding, test_variation_feature_responseCoding = get_response_coded_feature(train_df, cv_df, test_df, feature, target, laplace_alpha)\nprint(\"train_variation_feature_responseCoding : shape of feature:\", train_variation_feature_responseCoding.shape)\n\n# one hot encoding\nvariation_vectorizer = CountVectorizer()\ntrain_variation_feature_onehotCoding = variation_vectorizer.fit_transform(train_df['Variation'])\ntest_variation_feature_onehotCoding = variation_vectorizer.transform(test_df['Variation'])\ncv_variation_feature_onehotCoding = variation_vectorizer.transform(cv_df['Variation'])\n\nprint(\"train_variation_feature_onehotCoding : shape of feature:\", train_variation_feature_onehotCoding.shape)\n</code></pre> <pre>\n<code>train_variation_feature_responseCoding : shape of feature: (2124, 9)\ntrain_variation_feature_onehotCoding : shape of feature: (2124, 1967)\n</code>\n</pre> <pre><code>feature = 'TEXT'\ntarget = 'Class'\n\n# # One Hot Encoding\n# # building a CountVectorizer with all the words that occured minimum 3 times in train data\ntext_vectorizer = CountVectorizer(min_df=3)\ntrain_text_feature_onehotCoding = text_vectorizer.fit_transform(train_df[feature])\ntrain_text_features = text_vectorizer.get_feature_names_out()\n\n# train_text_feature_onehotCoding.sum(axis=0).A1 will sum every row and returns (1*number of features) vector\ntrain_text_feature_counts = train_text_feature_onehotCoding.sum(axis=0).A1\ntext_feature_dict = dict(zip(train_text_features,train_text_feature_counts))\n\n# Use the same vectorizer that was trained on train data\ncv_text_feature_onehotCoding = text_vectorizer.transform(cv_df[feature])\ntest_text_feature_onehotCoding = text_vectorizer.transform(test_df[feature])\n\n# Normalise\ntrain_text_feature_onehotCoding = normalize(train_text_feature_onehotCoding, axis=0)\ncv_text_feature_onehotCoding = normalize(cv_text_feature_onehotCoding, axis=0)\ntest_text_feature_onehotCoding = normalize(test_text_feature_onehotCoding, axis=0)\n\n# # Response Coding\ndef words_value_count_in_text_df(df, feature):\n    text_split = pd.DataFrame(df[feature].str.split().tolist(), index=df.index)\n    dictionary = defaultdict(int, text_split.unstack().value_counts().to_dict())\n    return dictionary\n\ndef get_text_responsecoding(df, class_dict_list, total_dict, feature, target):\n    no_of_classes = len(df[target].unique())\n    text_feature_responseCoding = np.zeros((df.shape[0],no_of_classes))\n    for i in range(0, no_of_classes):\n        row_index = 0\n        for index, row in df.iterrows():\n            sum_prob = 0\n            for word in row[feature].split():\n                sum_prob += math.log(((class_dict_list[i].get(word,0)+10 )/(total_dict.get(word,0)+90)))\n            text_feature_responseCoding[row_index][i] = math.exp(sum_prob/len(row[feature].split()))\n            row_index += 1\n    return text_feature_responseCoding\n\nclass_dict_list = []\nfor i in range(1,10):\n    class_text_df = train_df[train_df[target] == i]\n    dictionary = words_value_count_in_text_df(class_text_df, feature)\n    class_dict_list.append(dictionary)\ntotal_dict = words_value_count_in_text_df(train_df, feature)\n\ntrain_text_feature_responseCoding  = get_text_responsecoding(train_df, class_dict_list, total_dict, feature, target)\ncv_text_feature_responseCoding  = get_text_responsecoding(cv_df, class_dict_list, total_dict, feature, target)\ntest_text_feature_responseCoding  = get_text_responsecoding(test_df, class_dict_list, total_dict, feature, target)\n\ntrain_text_feature_responseCoding = (train_text_feature_responseCoding.T/train_text_feature_responseCoding.sum(axis=1)).T\ncv_text_feature_responseCoding = (cv_text_feature_responseCoding.T/cv_text_feature_responseCoding.sum(axis=1)).T\ntest_text_feature_responseCoding = (test_text_feature_responseCoding.T/test_text_feature_responseCoding.sum(axis=1)).T\n</code></pre> <pre><code>print('train_gene_feature_responseCoding')\n_ = pd.DataFrame(train_gene_feature_responseCoding).plot(bins=30, kind='hist', subplots=True, figsize=(16,16),layout=(3,3),  title='train_gene_feature_responseCoding', grid=True)\nplt.tight_layout()\nplt.show()\n\nprint('train_variation_feature_responseCoding')\n_ = pd.DataFrame(train_variation_feature_responseCoding).plot(bins=30, kind='hist', subplots=True, figsize=(16,16),layout=(3,3), title='train_variation_feature_responseCoding', grid=True)\nplt.tight_layout()\nplt.show()\n\nprint('train_text_feature_responseCoding')\n_ = pd.DataFrame(train_text_feature_responseCoding).plot(bins=30, kind='hist', subplots=True, figsize=(16,16),layout=(3,3), title='train_text_feature_responseCoding', grid=True)\nplt.tight_layout()\nplt.show()\n\nprint('train_text_feature_onehotCoding')\nsorted_text_feature_dict = dict(sorted(text_feature_dict.items(), key=lambda x: x[1] , reverse=True))\nsorted_text_occur = np.array(list(sorted_text_feature_dict.values()))\n_ = pd.Series(Counter(sorted_text_occur)).sort_values(ascending=False)[:50].plot(kind='bar', figsize=(16,8), title='Number of words for a given frequency')\nplt.tight_layout()\nplt.show()\n</code></pre> <pre>\n<code>train_gene_feature_responseCoding\n</code>\n</pre> <pre>\n<code>train_variation_feature_responseCoding\n</code>\n</pre> <pre>\n<code>train_text_feature_responseCoding\n</code>\n</pre> <pre>\n<code>train_text_feature_onehotCoding\n</code>\n</pre> <pre><code>alpha = [10 ** x for x in range(-5, 1)] # hyperparam for SGD classifier.\n\ncv_log_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(train_gene_feature_onehotCoding, y_train)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_gene_feature_onehotCoding, y_train)\n    predict_y = sig_clf.predict_proba(cv_gene_feature_onehotCoding)\n    cv_log_error_array.append(log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(train_gene_feature_onehotCoding, y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_gene_feature_onehotCoding, y_train)\n\npredict_y = sig_clf.predict_proba(train_gene_feature_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_gene_feature_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_gene_feature_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\nprint('\\n')\n\ntest_coverage=test_df[test_df['Gene'].isin(list(set(train_df['Gene'])))].shape[0]\ncv_coverage=cv_df[cv_df['Gene'].isin(list(set(train_df['Gene'])))].shape[0]\n\nprint('In test data', test_coverage, 'out of',test_df.shape[0], \":\",round((test_coverage/test_df.shape[0])*100,2), '%')\nprint('In cross validation data',cv_coverage, 'out of ',cv_df.shape[0],\":\" ,round((cv_coverage/cv_df.shape[0])*100,2), '%')\n</code></pre> <pre>\n<code>For values of alpha =  1e-05 The log loss is: 1.225157977779788\nFor values of alpha =  0.0001 The log loss is: 1.2186194232146514\nFor values of alpha =  0.001 The log loss is: 1.2698771600985577\nFor values of alpha =  0.01 The log loss is: 1.3892311253795766\nFor values of alpha =  0.1 The log loss is: 1.4786048680300583\nFor values of alpha =  1 The log loss is: 1.517888924555487\n</code>\n</pre> <pre>\n<code>For values of best alpha =  0.0001 The train log loss is: 0.9626738410932817\nFor values of best alpha =  0.0001 The cross validation log loss is: 1.2186194232146514\nFor values of best alpha =  0.0001 The test log loss is: 1.213990889587606\n\n\nIn test data 645 out of 665 : 96.99 %\nIn cross validation data 511 out of  532 : 96.05 %\n</code>\n</pre> <p>Since, the CV and Test errors are similar to the train error, Gene feature is Stable</p> <pre><code>alpha = [10 ** x for x in range(-5, 1)]\n\ncv_log_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(train_variation_feature_onehotCoding, y_train)\n\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_variation_feature_onehotCoding, y_train)\n    predict_y = sig_clf.predict_proba(cv_variation_feature_onehotCoding)\n\n    cv_log_error_array.append(log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(train_variation_feature_onehotCoding, y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_variation_feature_onehotCoding, y_train)\n\npredict_y = sig_clf.predict_proba(train_variation_feature_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_variation_feature_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_variation_feature_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n\nprint('\\n')\ntest_coverage=test_df[test_df['Variation'].isin(list(set(train_df['Variation'])))].shape[0]\ncv_coverage=cv_df[cv_df['Variation'].isin(list(set(train_df['Variation'])))].shape[0]\nprint('In test data',test_coverage, 'out of',test_df.shape[0], \":\",(test_coverage/test_df.shape[0])*100)\nprint('In cross validation data',cv_coverage, 'out of ',cv_df.shape[0],\":\" ,(cv_coverage/cv_df.shape[0])*100)\n</code></pre> <pre>\n<code>For values of alpha =  1e-05 The log loss is: 1.6880748869472166\nFor values of alpha =  0.0001 The log loss is: 1.6848972129901907\nFor values of alpha =  0.001 The log loss is: 1.6873507994191796\nFor values of alpha =  0.01 The log loss is: 1.694110632040056\nFor values of alpha =  0.1 The log loss is: 1.7001843816504443\nFor values of alpha =  1 The log loss is: 1.7015408337552536\n</code>\n</pre> <pre>\n<code>For values of best alpha =  0.0001 The train log loss is: 0.6457223722823235\nFor values of best alpha =  0.0001 The cross validation log loss is: 1.6848972129901907\nFor values of best alpha =  0.0001 The test log loss is: 1.707051708439472\n\n\nIn test data 68 out of 665 : 10.225563909774436\nIn cross validation data 66 out of  532 : 12.406015037593985\n</code>\n</pre> <p>Since, the CV and Test errors are not so similar to the train error, Variation feature is unstable</p> <pre><code>alpha = [10 ** x for x in range(-5, 1)]\n\ncv_log_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(train_text_feature_onehotCoding, y_train)\n\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_text_feature_onehotCoding, y_train)\n    predict_y = sig_clf.predict_proba(cv_text_feature_onehotCoding)\n    cv_log_error_array.append(log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(train_text_feature_onehotCoding, y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_text_feature_onehotCoding, y_train)\n\npredict_y = sig_clf.predict_proba(train_text_feature_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_text_feature_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_text_feature_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n</code></pre> <pre>\n<code>For values of alpha =  1e-05 The log loss is: 1.3417099148783505\nFor values of alpha =  0.0001 The log loss is: 1.2077311513227988\nFor values of alpha =  0.001 The log loss is: 1.2220108736944972\nFor values of alpha =  0.01 The log loss is: 1.3291070684522948\nFor values of alpha =  0.1 The log loss is: 1.4905994716927748\nFor values of alpha =  1 The log loss is: 1.6978774030031254\n</code>\n</pre> <pre>\n<code>For values of best alpha =  0.0001 The train log loss is: 0.6505120004239873\nFor values of best alpha =  0.0001 The cross validation log loss is: 1.2077311513227988\nFor values of best alpha =  0.0001 The test log loss is: 1.2064042366178185\n</code>\n</pre> <p>Text feature is stable across all the data sets (Test, Train, Cross validation)</p> <pre><code>def get_intersec_text(df):\n    df_text_vec = CountVectorizer(min_df=3)\n    df_text_fea = df_text_vec.fit_transform(df['TEXT'])\n    df_text_features = df_text_vec.get_feature_names_out()\n\n    df_text_fea_counts = df_text_fea.sum(axis=0).A1\n    df_text_fea_dict = dict(zip(list(df_text_features),df_text_fea_counts))\n    len1 = len(set(df_text_features))\n    len2 = len(set(train_text_features) &amp;amp; set(df_text_features))\n    return len1,len2\n</code></pre> <pre><code>len1,len2 = get_intersec_text(cv_df)\nprint(np.round((len2/len1)*100, 3), \"% of word of Cross Validation appeared in train data\")\nlen1,len2 = get_intersec_text(test_df)\nprint(np.round((len2/len1)*100, 3), \"% of word of test data appeared in train data\")\n</code></pre> <pre>\n<code>97.986 % of word of Cross Validation appeared in train data\n96.803 % of word of test data appeared in train data\n</code>\n</pre> <pre><code>def predict_and_plot_confusion_matrix(train_x, train_y,test_x, test_y, clf):\n    clf.fit(train_x, train_y)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_x, train_y)\n    pred_y = sig_clf.predict(test_x)\n\n    # for calculating log_loss we willl provide the array of probabilities belongs to each class\n    print(\"Log loss :\",log_loss(test_y, sig_clf.predict_proba(test_x)))\n    # calculating the number of data points that are misclassified\n    print(\"Number of mis-classified points :\", np.count_nonzero((pred_y- test_y))/test_y.shape[0])\n    plot_confusion_matrix(test_y, pred_y)\n\ndef report_log_loss(train_x, train_y, test_x, test_y,  clf):\n    clf.fit(train_x, train_y)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_x, train_y)\n    sig_clf_probs = sig_clf.predict_proba(test_x)\n    return log_loss(test_y, sig_clf_probs, eps=1e-15)\n\n# this function will be used just for naive bayes\n# for the given indices, we will print the name of the features\n# and we will check whether the feature present in the test point text or not\ndef get_impfeature_names(indices, text, gene, var, no_features):\n    gene_count_vec = CountVectorizer()\n    var_count_vec = CountVectorizer()\n    text_count_vec = CountVectorizer(min_df=3)\n\n    gene_vec = gene_count_vec.fit(train_df['Gene'])\n    var_vec  = var_count_vec.fit(train_df['Variation'])\n    text_vec = text_count_vec.fit(train_df['TEXT'])\n\n    fea1_len = len(gene_vec.get_feature_names_out())\n    fea2_len = len(var_count_vec.get_feature_names_out())\n\n    word_present = 0\n    for i,v in enumerate(indices):\n        if (v &amp;lt; fea1_len):\n            word = gene_vec.get_feature_names_out()[v]\n            yes_no = True if word == gene else False\n            if yes_no:\n                word_present += 1\n                print(i, \"Gene feature [{}] present in test data point [{}]\".format(word,yes_no))\n        elif (v &amp;lt; fea1_len+fea2_len):\n            word = var_vec.get_feature_names_out()[v-(fea1_len)]\n            yes_no = True if word == var else False\n            if yes_no:\n                word_present += 1\n                print(i, \"variation feature [{}] present in test data point [{}]\".format(word,yes_no))\n        else:\n            word = text_vec.get_feature_names_out()[v-(fea1_len+fea2_len)]\n            yes_no = True if word in text.split() else False\n            if yes_no:\n                word_present += 1\n                print(i, \"Text feature [{}] present in test data point [{}]\".format(word,yes_no))\n\n    print(\"Out of the top \",no_features,\" features \", word_present, \"are present in query point\")\n</code></pre> <pre><code>train_gene_var_onehotCoding = hstack((train_gene_feature_onehotCoding,train_variation_feature_onehotCoding))\ntest_gene_var_onehotCoding = hstack((test_gene_feature_onehotCoding,test_variation_feature_onehotCoding))\ncv_gene_var_onehotCoding = hstack((cv_gene_feature_onehotCoding,cv_variation_feature_onehotCoding))\n\ntrain_x_onehotCoding = hstack((train_gene_var_onehotCoding, train_text_feature_onehotCoding)).tocsr()\ntrain_y = np.array(list(train_df['Class']))\n\ntest_x_onehotCoding = hstack((test_gene_var_onehotCoding, test_text_feature_onehotCoding)).tocsr()\ntest_y = np.array(list(test_df['Class']))\n\ncv_x_onehotCoding = hstack((cv_gene_var_onehotCoding, cv_text_feature_onehotCoding)).tocsr()\ncv_y = np.array(list(cv_df['Class']))\n\nprint(\"One hot encoding features :\")\nprint(\"(number of data points * number of features) in train data = \", train_x_onehotCoding.shape)\nprint(\"(number of data points * number of features) in test data = \", test_x_onehotCoding.shape)\nprint(\"(number of data points * number of features) in cross validation data =\", cv_x_onehotCoding.shape)\n</code></pre> <pre>\n<code>One hot encoding features :\n(number of data points * number of features) in train data =  (2124, 55944)\n(number of data points * number of features) in test data =  (665, 55944)\n(number of data points * number of features) in cross validation data = (532, 55944)\n</code>\n</pre> <pre><code>train_gene_var_responseCoding = np.hstack((train_gene_feature_responseCoding,train_variation_feature_responseCoding))\ntest_gene_var_responseCoding = np.hstack((test_gene_feature_responseCoding,test_variation_feature_responseCoding))\ncv_gene_var_responseCoding = np.hstack((cv_gene_feature_responseCoding,cv_variation_feature_responseCoding))\n\ntrain_x_responseCoding = np.hstack((train_gene_var_responseCoding, train_text_feature_responseCoding))\ntest_x_responseCoding = np.hstack((test_gene_var_responseCoding, test_text_feature_responseCoding))\ncv_x_responseCoding = np.hstack((cv_gene_var_responseCoding, cv_text_feature_responseCoding))\n\nprint(\" Response encoding features :\")\nprint(\"(number of data points * number of features) in train data = \", train_x_responseCoding.shape)\nprint(\"(number of data points * number of features) in test data = \", test_x_responseCoding.shape)\nprint(\"(number of data points * number of features) in cross validation data =\", cv_x_responseCoding.shape)\n</code></pre> <pre>\n<code> Response encoding features :\n(number of data points * number of features) in train data =  (2124, 27)\n(number of data points * number of features) in test data =  (665, 27)\n(number of data points * number of features) in cross validation data = (532, 27)\n</code>\n</pre> <pre><code>alpha = [0.00001, 0.0001, 0.001, 0.1, 1, 10, 100,1000]\ncv_log_error_array = []\nfor i in alpha:\n    print(\"for alpha =\", i)\n    clf = MultinomialNB(alpha=i)\n    clf.fit(train_x_onehotCoding, train_y)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_x_onehotCoding, train_y)\n    sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n    # to avoid rounding error while multiplying probabilites we use log-probability estimates\n    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs)) \n\nfig, ax = plt.subplots()\nax.plot(np.log10(alpha), cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],str(txt)), (np.log10(alpha[i]),cv_log_error_array[i]))\nplt.grid()\nplt.xticks(np.log10(alpha))\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = MultinomialNB(alpha=alpha[best_alpha])\nclf.fit(train_x_onehotCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_onehotCoding, train_y)\n\n\npredict_y = sig_clf.predict_proba(train_x_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_x_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_x_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n</code></pre> <pre>\n<code>for alpha = 1e-05\nLog Loss : 1.2954975305870766\nfor alpha = 0.0001\nLog Loss : 1.2869374246985383\nfor alpha = 0.001\nLog Loss : 1.2842631205424986\nfor alpha = 0.1\nLog Loss : 1.2815875568364257\nfor alpha = 1\nLog Loss : 1.2995403679730142\nfor alpha = 10\nLog Loss : 1.3512446897455843\nfor alpha = 100\nLog Loss : 1.3322494833632648\nfor alpha = 1000\nLog Loss : 1.2809048617321397\n</code>\n</pre> <pre>\n<code>For values of best alpha =  1000 The train log loss is: 1.0285530527044575\nFor values of best alpha =  1000 The cross validation log loss is: 1.2809048617321397\nFor values of best alpha =  1000 The test log loss is: 1.2867053550420369\n</code>\n</pre> <pre><code>clf = MultinomialNB(alpha=alpha[best_alpha])\nclf.fit(train_x_onehotCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_onehotCoding, train_y)\nsig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n# to avoid rounding error while multiplying probabilites we use log-probability estimates\nprint(\"Log Loss :\",log_loss(cv_y, sig_clf_probs))\nprint(\"Number of missclassified point :\", np.count_nonzero((sig_clf.predict(cv_x_onehotCoding)- cv_y))/cv_y.shape[0])\nplot_confusion_matrix(cv_y, sig_clf.predict(cv_x_onehotCoding.toarray()))\n</code></pre> <pre>\n<code>Log Loss : 1.2809048617321397\nNumber of missclassified point : 0.40789473684210525\n</code>\n</pre> <pre><code>test_point_index = 1\nno_feature = 100\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices=np.argsort(-1*clf.feature_log_prob_)[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n</code></pre> <pre>\n<code>Predicted Class : 1\nPredicted Class Probabilities: [0.4597 0.1588 0.0104 0.0636 0.0274 0.0241 0.2489 0.0059 0.0011](../04597-01588-00104-00636-00274-00241-02489-00059-00011 \"0.4597 0.1588 0.0104 0.0636 0.0274 0.0241 0.2489 0.0059 0.0011\")\nActual Class : 1\n--------------------------------------------------\n12 Text feature [dna] present in test data point [True]\n13 Text feature [type] present in test data point [True]\n14 Text feature [function] present in test data point [True]\n15 Text feature [protein] present in test data point [True]\n17 Text feature [one] present in test data point [True]\n18 Text feature [two] present in test data point [True]\n19 Text feature [containing] present in test data point [True]\n20 Text feature [region] present in test data point [True]\n21 Text feature [wild] present in test data point [True]\n22 Text feature [loss] present in test data point [True]\n23 Text feature [conserved] present in test data point [True]\n24 Text feature [amino] present in test data point [True]\n25 Text feature [sequence] present in test data point [True]\n26 Text feature [possible] present in test data point [True]\n28 Text feature [effect] present in test data point [True]\n29 Text feature [reduced] present in test data point [True]\n30 Text feature [present] present in test data point [True]\n31 Text feature [therefore] present in test data point [True]\n33 Text feature [analysis] present in test data point [True]\n35 Text feature [four] present in test data point [True]\n37 Text feature [least] present in test data point [True]\n38 Text feature [specific] present in test data point [True]\n41 Text feature [structure] present in test data point [True]\n43 Text feature [results] present in test data point [True]\n44 Text feature [large] present in test data point [True]\n46 Text feature [critical] present in test data point [True]\n48 Text feature [used] present in test data point [True]\n50 Text feature [three] present in test data point [True]\n53 Text feature [determined] present in test data point [True]\n54 Text feature [define] present in test data point [True]\n55 Text feature [corresponding] present in test data point [True]\n56 Text feature [control] present in test data point [True]\n57 Text feature [identified] present in test data point [True]\n59 Text feature [using] present in test data point [True]\n60 Text feature [possibility] present in test data point [True]\n61 Text feature [data] present in test data point [True]\n62 Text feature [table] present in test data point [True]\n63 Text feature [six] present in test data point [True]\n65 Text feature [additional] present in test data point [True]\n66 Text feature [shown] present in test data point [True]\n67 Text feature [also] present in test data point [True]\n68 Text feature [result] present in test data point [True]\n69 Text feature [likely] present in test data point [True]\n71 Text feature [discussion] present in test data point [True]\n74 Text feature [different] present in test data point [True]\n75 Text feature [whether] present in test data point [True]\n77 Text feature [reveal] present in test data point [True]\n78 Text feature [highly] present in test data point [True]\n79 Text feature [remains] present in test data point [True]\n80 Text feature [within] present in test data point [True]\n81 Text feature [proteins] present in test data point [True]\n82 Text feature [either] present in test data point [True]\n83 Text feature [human] present in test data point [True]\n84 Text feature [performed] present in test data point [True]\n85 Text feature [significantly] present in test data point [True]\n86 Text feature [expected] present in test data point [True]\n87 Text feature [whereas] present in test data point [True]\n88 Text feature [five] present in test data point [True]\n89 Text feature [nonsense] present in test data point [True]\n91 Text feature [gene] present in test data point [True]\n93 Text feature [may] present in test data point [True]\n94 Text feature [25] present in test data point [True]\n97 Text feature [genetic] present in test data point [True]\n98 Text feature [30] present in test data point [True]\n99 Text feature [panel] present in test data point [True]\nOut of the top  100  features  65 are present in query point\n</code>\n</pre> <pre><code>test_df['TEXT'].iloc[test_point_index]\n</code></pre> <pre>\n<code>'introduction platinum based chemotherapy standard care patients muscle invasive metastatic urothelial carcinoma 20 years 1 3 neoadjuvant cisplatin based chemotherapy leads 14 25 relative risk reduction death muscle invasive urothelial carcinoma ct2 t4an0m0 refs 3 5 pathologic downstaging complete response pt0 carcinoma situ ptis cystectomy occurs 26 38 patients treated neoadjuvant chemotherapy compared 12 3 15 patients undergoing cystectomy alone 3 4 6 5 year survival pt0 ptis patients 85 neoadjuvant chemotherapy 3 compared 43 patients persistent muscle invasive disease pt2 ref 7 therefore benefit neoadjuvant chemotherapy seems dramatic patients found pathologic complete responses time surgical resection however inability predict patients derive clinical benefit limited use toxic approach urologic community 8 10 cisplatin causes accumulation dna cross links interfere dna replication gene transcription eventually promotes cell death repair cisplatin induced dna damage occurs primarily dna repair pathways nucleotide excision repair ner ref 11 homologous recombination includes brca1 brca2 ref 12 ner pathway involves multiple genes including ercc1 5 cdk7 ddb1 2 xpa xpc 13 germline alterations ner genes result multiple recessive inherited disorders including xeroderma pigmentosum xp ref 13 deficiency ner patients xp significantly increased risk developing skin cancers malignancies 14 germline expression based changes several ner genes suggested modulate clinical response cisplatin based chemotherapy 15 16 however prospective studies confirmed observations 17 18 ercc2 ner helicase loss function correlates cisplatin sensitivity preclinical models 19 whereas ercc2 overexpression leads cisplatin resistance 20 data suggest tumors loss ner function may exhibit increased cisplatin sensitivity recent data identified somatic ercc2 mutations 7 12 urothelial carcinomas 21 22 hypothesized somatic mutations ner pathway may correlate response cisplatin based neoadjuvant chemotherapy patients urothelial carcinoma test generally define genomic correlates chemotherapy response performed whole exome sequencing wes urothelial carcinoma tumors patients extreme responses neoadjuvant cisplatin based combination chemotherapy results somatic genetic alterations muscle invasive urothelial carcinoma sequenced pretreatment tumor germline dna 50 patients treated neoadjuvant cisplatin based chemotherapy muscle invasive urothelial carcinoma 25 responders residual invasive disease pt0 ptis pathologic examination following cystectomy 25 nonresponders residual muscle invasive pt2 disease fig 1a methods although multiple chemotherapeutic regimens used common agent among patients cisplatin table 1 supplementary table s1 significant differences clinical characteristics identified responders nonresponders baseline p 0 05 mann whitney test figure 1 download figureopen new tabdownload powerpoint figure 1 study design mutation rates aggregate significant somatic mutations patients muscle invasive urothelial carcinoma cancer split cases controls based pathologic response cisplatin based neoadjuvant chemotherapy turbt transurethral resection bladder tumor nine cases could complete sequencing due technical reasons failed sequencing elevated contamination b data arranged column represents tumor row represents gene center panel divided responders left black nonresponders right yellow mutation rates responders elevated compared nonresponders top alteration landscape center aggregate cohort n 50 patients demonstrates set statistically significant genes altered urothelial carcinoma tp53 rb1 kdm6a arid1a negative log q values significance level mutated genes shown genes q 0 1 right ercc2 mutation status also shown genes although ercc2 significantly mutated across combined cohort additional data allelic fraction ranges case bottom mutation rates top mutational frequency left also summarized view inlineview popup table 1 patient characteristics mean target coverage wes 121 tumors 130 paired germline samples supplementary table s1 median mutation rate 9 7 mutations per megabase mutations mb responders 4 4 mutations mb nonresponders p 0 0003 mann whitney test fig 1b supplementary fig s1a s1b supplementary table s1 raising possibility reduced dna repair fidelity among cisplatin responders observed somatic mutations short insertion deletions reported supplementary table s2 statistical assessment 23 base mutations short insertion deletions across responders nonresponders nominated four significantly altered genes previously implicated urothelial carcinoma 21 22 24 tp53 rb1 kdm6a arid1a fig 1b supplementary tables s3 s4 addition nine nonsynonymous somatic mutations observed ercc2 fig 1b supplementary fig s2 supplementary table s2 although ercc2 reach cohort wide statistical significance known role dna repair report recurrently mutated bladder cancer 21 22 raised possibility ercc2 mutations might associate cisplatin response somatic ercc2 mutations cisplatin based chemotherapy responders performed enrichment analysis identify genes selectively mutated responders compared nonresponders supplementary methods among 3 277 genes least one possibly damaging somatic alteration supplementary methods ercc2 gene significantly enriched responder cohort fig 2a supplementary tables s5 s6 indeed ercc2 nonsynonymous somatic mutations occurred cisplatin sensitive tumors p 0 001 fisher exact test ercc2 remained significantly enriched responders following false discovery analysis performed genes mutation frequency afforded adequate power q 0 007 benjamini hochberg fig 2b moreover enrichment ercc2 mutations responder group significant adjusted differences overall mutation rate responders nonresponders p 0 04 binomial test toward end median background mutation rate ercc2 mutant tumors 15 5 mutations mb significantly elevated compared ercc2 wild type wt tumors 5 1 mutations mb p 0 01 mann whitney test supplementary fig s1b consistent possible dna repair defect prior reports 22 figure 2 download figureopen new tabdownload powerpoint figure 2 three tests examining selective enrichment ercc2 mutations cisplatin responder tumors plot mutsigcv gene level significance log10 mutsigcv p value responder enrichment significance log10 fisher exact test p value size point proportional number responder patients harbor alterations gene genes responder enrichment p value 0 01 shown red others gray dashed line denotes p value 0 01 ercc2 reaches statistical significance responder cohort p 0 001 fisher exact test b among genes sufficient number alterations cohort comparisons n 9 ercc2 somatic mutations occur exclusively cisplatin responders significant accounting elevated mutation rate responders compared nonresponders p 0 05 compared unselected tcga guo et al urothelial carcinoma cohorts c shows ercc2 somatic mutations significantly enriched responder cohort p 0 01 somatic ercc2 mutation frequency responder cohort also compared two unselected bladder cancer populations 130 cases cancer genome atlas tcga ref 21 99 cases chinese patient cohort ref 22 fig 2c compared unselected populations ercc2 mutations significantly enriched cisplatin responder cohort 36 cases p 0 001 binomial test fig 2c ercc2 mutation status seem prognostic impact overall survival tcga cohort p 0 77 log rank supplementary fig s3 determine relative abundance somatic ercc2 mutations tumor types tcga data 19 tumor types n 4 429 queried 25 somatic ercc2 mutations observed low frequencies 4 11 tumor types fig 3a b figure 3 download figureopen new tabdownload powerpoint figure 3 ercc2 mutation mapping distribution across tumor types stick plot ercc2 showing locations somatic mutations responders compared ercc2 mutations observed two separate unselected bladder cancer exome cohorts ercc2 mutations cluster within near conserved helicase motifs b somatic ercc2 mutation frequency multiple tumor types tcga scc squamous cell carcinoma c structure archaebacterial ercc2 pdb code 3crv mutations identified responder cohort mapped equivalent position locations shown context canonical germline ercc2 mutations responsible xp xp cockayne syndrome cs trichothiodystrophy ttd identified somatic ercc2 mutations occurred highly conserved amino acid positions within immediately adjacent helicase domains fig 3a c supplementary fig s4 similarly germline ercc2 mutations patients xp complementation group xp combined cockayne syndrome xp cs two disorders characterized ner function clustered within helicase domains fig 3c conversely mutations causing trichothiodystrophy disease resulting alteration ercc2 role transcription distributed throughout protein 26 ercc2 mutations confer increased cisplatin sensitivity vitro observations raised possibility identified mutations disrupt ercc2 function interfere ner test hypothesis first five identified ercc2 mutants stably expressed immortalized ercc2 deficient cell line derived xp patient cisplatin sensitivity profile cell line measured supplementary methods fig 4a expression wt ercc2 rescued cisplatin sensitivity ercc2 deficient cell line whereas none ercc2 mutants rescued cisplatin sensitivity fig 4b ic50 ercc2 wt complemented cell line significantly higher ercc2 deficient parent cell line p 0 0001 anova whereas ic50 ercc2 mutant complemented cell line significantly different parent ercc2 deficient parent cell line fig 4c similarly ic50 ercc2 wt cell line significantly higher ercc2 deficient mutant cell lines p 0 0001 anova figure 4 download figureopen new tabdownload powerpoint figure 4 ercc2 mutants fail rescue cisplatin sensitivity ercc2 deficient cells immunoblot ercc2 expression cell lines created transfection ercc2 deficient parent cell line gm08207 coriell institute plx304 addgene encoding gfp negative control wt ercc2 mutant ercc2 negative control ercc2 deficient cell line lane 1 expresses endogenous levels inactive ercc2 parent cell genome whereas wt lane 2 mutant lanes 3 7 ercc2 cell lines show increased levels ercc2 expressed transfected gene actin shown loading control b cisplatin sensitivity profiles cell lines expressing wt mutant ercc2 expression wt ercc2 ercc2 deficient background rescues cisplatin sensitivity whereas expression ercc2 mutants fails rescue cisplatin sensitivity ic50 calculated survival data cell line values shown c difference ic50 parent ercc2 deficient cell line cell line expressing wt ercc2 statistically significant difference wt ercc2 cell line mutant ercc2 cell lines p 0 0001 anova difference ercc2 deficient cell line mutant cell lines statistically significant ner pathway repairs dna lesions cisplatin adducts also tested effect identified ercc2 mutations ner mediated repair uv induced dna damage wt mutant ercc2 complemented cell lines exposed increasing doses uv irradiation clonogenic survival measured fig 5a c whereas ercc2 wt complemented cell line rescued uv sensitivity uv sensitivities ercc2 mutant complemented cell lines significantly different ercc2 deficient parent cell line taken together experiments suggest observed ercc2 mutations result loss normal ner capacity figure 5 download figureopen new tabdownload powerpoint figure 5 ercc2 mutants fail rescue uv sensitivity ercc2 deficient cells representative colony formation assay ercc2 deficient cell line top well ercc2 deficient line transfected wt ercc2 middle one ercc2 mutants d609g bottom following increased doses uv irradiation b clonogenic survival data negative control wt ercc2 mutant ercc2 wt ercc2 rescues uv sensitivity ercc2 deficient cell line whereas mutant ercc2s fail rescue uv sensitivity c uv ic50 values cell lines difference ercc2 deficient cell line wt ercc2 cell line significant p 0 0001 anova whereas difference ercc2 deficient cell line ercc2 mutant cell lines statistically significant ns overall mutation rate higher ercc2 mutated tumors hypothesized ercc2 mutations may broadly contributing genomic instability thus measured rates chromosomal aberrations wt mutant ercc2 complemented cell lines cisplatin treatment absence cisplatin background rates chromosomal aberrations slightly lower ercc2 wt complemented cell line ercc2 deficient ercc2 mutant complemented cell lines difference statistically significant fig 6a however following cisplatin exposure significantly fewer chromosomal aberrations observed ercc2 wt complemented cell line ercc2 deficient parent cell line p 0 03 anova whereas expression ercc2 mutants resulted rescue chromosomal stability p 0 5 fig 6d data suggest responder associated ercc2 mutations may contribute overall genomic instability tumors figure 6 download figureopen new tabdownload powerpoint figure 6 ercc2 mutants fail rescue genomic instability following cisplatin exposure representative mitotic spreads ercc2 deficient cell line ercc2 deficient cell line transfected wt ercc2 b one ercc2 mutants v242f c following cisplatin exposure chromosomal aberration data ercc2 deficient wt ercc2 mutant ercc2 cell lines rates chromosomal aberrations following cisplatin exposure significantly lower wt ercc2 cell line ercc2 deficient line cell lines expressing mutant ercc2 p 0 03 anova dna repair gene alterations finally sought determine whether dna repair genes might undergo recurrent mutations cisplatin sensitive tumors significantly recurrent mutations observed ner homologous repair genes responders compared nonresponders however two responder tumors ercc2 mutations somatic nonsense truncating mutations detected homologous recombination dna repair genes brca1 brca2 supplementary table s2 nonsynonymous brca1 brca2 mutations nonresponders supplementary table s2 results consistent known sensitivity brca1 2 mutant tumors e g brca1 2 mutant breast ovarian cancers platinum containing regimens 27 whereas somatic ercc2 mutations dna repair gene mutations enriched responders singleton missense mutations uncertain significance observed dna damage response genes responders nonresponders unknown functional relevance discussion bladder cancer clinical benefit neoadjuvant chemotherapy apparent pathologic downstaging pt0 ptis achieved surgical resection following cisplatin based chemotherapy however lack predictive biomarker clinical benefit neoadjuvant cisplatin based chemotherapy limited use approach clinical community due toxicity concerns using extreme phenotype analysis identified association somatic ercc2 mutations pathologic complete response neoadjuvant cisplatin based chemotherapy muscle invasive urothelial carcinoma although ercc2 mutations occur approximately 12 unselected cases 36 cisplatin based chemotherapy responders cohort harbored somatic ercc2 nonsynonymous mutations moreover ercc2 mutant tumors responded neoadjuvant chemotherapy ner pathway highly conserved dna repair system identifies repairs bulky dna adducts arising genotoxic agents cisplatin ner helicase ercc2 unwinds duplex dna near damage site coordinated action two conserved helicase domains ercc2 mutations identified study occurred conserved positions within adjacent helicase domains identified mutants failed complement cisplatin uv sensitivity ercc2 deficient cell line together data suggest mutations result loss normal ercc2 function leading increased tumor cell sensitivity dna damaging agents cisplatin interestingly seven 78 ercc2 mutant cases ercc2 mutation allelic fraction 0 5 supplementary table s2 suggesting wt ercc2 remains present one allele therefore cisplatin sensitivity phenotype may result haploinsufficient dominant negative effect heterozygous ercc2 mutation rather result biallelic inactivating mutations traditional two hit tumor suppressor model driving force heterozygous mutation ercc2 unknown however prevalence ercc2 mutations study cohorts tcga suggests loss normal ercc2 function may provide selective advantage tumors partial loss dna repair fidelity could aid tumor growth decreasing repair associated delays cell cycle progression addition decreased ner capacity may result higher rates error prone repair large scale genomic changes drive tumor growth despite providing potential growth advantage mutation one ercc2 allele may render tumor cells susceptible dna damaging agent cisplatin inadequate levels wt ercc2 present support ner e haploinsufficiency alternatively mutated version ercc2 may bind efficiently repair damaged dna thereby preventing repair alternative dna repair pathway leading dominant negative phenotype described mutants yeast ercc2 homolog rad3 28 studies necessary explore effects ercc2 loss tumor growth mechanism identified ercc2 mutations confer changes tumor ner capacity one possible explanation findings observed study somatic ercc2 mutation associated good prognosis smaller tumors however supported survival data tcga bladder cancer cohort excluded patients received neoadjuvant chemotherapy difference overall survival observed basis somatic ercc2 mutation status supplementary fig s3 extreme response cohort reported patients generally noted obvious disease left behind initial transurethral resection based operative reports imaging making findings unlikely related complete transurethral resection although data yet used justify avoiding cisplatin based treatment ercc2 wt patients findings raise possibility somatic ercc2 mutation status may provide genetic means select patients likely benefit cisplatin based chemotherapy directing patients toward alternative therapeutic approaches note study focused specifically somatic mutations exclusively tumor germline single nucleotide polymorphisms ercc2 genes thus approach distinct genome wide association studies examined germline ercc1 ercc2 polymorphisms mixed impact cisplatin sensitivity 29 broadly findings require independent clinical validation prospective trials establish clinical predictive power somatic ercc2 mutation status cisplatin response possible nonresponding urothelial tumors harbor somatic ercc2 mutations larger cohorts observed examination post chemotherapy resistant tumor would critical understanding whether tumor heterogeneity played role resistance patients analyzed extreme phenotype analysis treated combination cisplatin based chemotherapy regimens containing non cisplatin agents however cisplatin agent common regimens half patients bladder cancer candidates cisplatin based chemotherapy due preexisting comorbidities less toxic carboplatin based neoadjuvant therapies may warrant study non cisplatin eligible patients ercc2 mutant tumors date exceptional response genomic studies informed genomic mechanisms response targeted therapies response everolimus multiple disease contexts 30 31 however published studies limited individual case reports due rarity events targeted therapies study represents new approach studying extraordinary responses commonly used cytotoxic chemotherapies using case control design may applied therapeutic settings significant minority patients achieve exceptional response e g neoadjuvant chemotherapies clinical settings toward end majority responder cases cohort recurrent genomic determinant cisplatin response possible alterations dna repair genes readily detectable wes e g epigenetic expression based may mediate cisplatin sensitivity cases conclusion work provides new insights relationship somatic genetic alterations clinical response cisplatin based chemotherapy urothelial carcinoma validated results may inform therapeutic decision making novel therapeutic development clinical trial designs urothelial carcinoma possibly ercc2 mutated tumors finally results show somatic genomic alterations may reveal mechanistic underpinnings antitumor response conventional cytotoxic chemotherapy methods patients samples patients muscle invasive locally advanced urothelial carcinoma extreme responses chemotherapy defined residual invasive carcinoma cystectomy presence persistent muscle invasive extravesical disease cystectomy available prechemotherapy tumor tissue enrolled institutional review board irb approved tissue acquisition dna sequencing protocols identified dana farber protocols 02 021 11 334 memorial sloan kettering cancer center protocols 89 076 09 025 patients provided written informed consent genomic testing used study specimens evaluated genitourinary pathologists j barletta signoretti dfci cohort h al ahmadie mskcc cohort identify tumor bearing areas dna extraction minimum percentage neoplastic cellularity regions tumor tissue 60 study specimens frozen formalin fixed paraffin embedded ffpe tissue sections identified dana farber cancer institute dfci memorial sloan kettering cancer center mskcc germline dna extracted either peripheral blood mononuclear cells histologically normal nonurothelial tissue information source germline dna available supplementary table s1 wes statistical analysis dna extraction exome sequencing slides cut ffpe frozen tissue blocks examined board certified pathologist select high density cancer foci ensure high purity cancer dna biopsy cores taken corresponding tissue block dna extraction dna extracted using qiagen qiaamp dna ffpe tissue kit quantitation reagent invitrogen dna stored 20 c whole exome capture libraries constructed 100 ng dna tumor normal tissue sample shearing end repair phosphorylation ligation barcoded sequencing adaptors ligated dna size selected lengths 200 350 bp subjected exonic hybrid capture using sureselect v2 exome bait agilent technologies sample multiplexed sequenced using illumina hiseq technology mean target exome coverage 121 tumors 130 germline samples four cases complete exome sequencing process due sequencing process failure bam files generated study deposited database genotypes phenotypes dbgap phs000771 v1 p1 sequence data processing exome sequence data processing analysis performed using pipelines broad institute bam file aligned hg19 human genome build produced using illumina sequencing reads tumor normal sample picard pipeline bam files uploaded firehose infrastructure 32 managed intermediate analysis files executed analysis pipelines bam files uploaded dbgap phs000771 v1 p1 sequencing quality control sequencing data incorporated quality control modules firehose compare tumor normal genotypes ensure concordance samples cross contamination samples individuals sequenced flow cell monitored contest algorithm 33 samples 4 contamination excluded n 4 alteration identification annotation mutect algorithm 34 applied identify somatic single nucleotide variants targeted exons indelocator applied identify small insertions deletions 35 gene level coverage determined depthofcoverage genome analysis tool kit 36 alterations annotated using oncotator 37 power calculations coverage determined using mutect coverage file requires minimum 14 coverage tumor samples median allelic fractions less 0 05 insufficient dna orthogonal validation fluidigm access array excluded n 1 alteration significance mutsigcv 23 applied aggregate cohort 50 cases determine statistically altered genes cohort alterations nominated significant genes mutsigcv manually reviewed integrated genomics viewer igv refs 38 39 alterations invalid based igv review result misalignment artifacts viewable igv subsequently excluded final results resulted exclusion tgfbr1 depdc4 final result supplementary table s3 selective gene enrichment analysis somatic mutations short insertion deletions aggregated cohort split responders nonresponders alterations coverage tumor site 30 allelic fraction 0 1 considered analyses missense nonsense splice site mutations along short insertion deletions assigned damaging score range 0 1 following previously reported methods 40 missense mutations scored using polyphen2 score 41 amino acid substitution missense mutations without available polyphen2 scores due mapping errors dinucleotide status listed unavailable supplementary table s5 excluded nonsense mutations splice site mutations short insertion deletions assigned damaging score 1 damaging scores listed supplementary table s5 alterations damaging score 0 5 tabulated occurrence responders nonresponders altered gene would counted per patient fisher exact test performed compare cohorts derive p value gene minimum six alterations required observe p value 0 01 genes 6 alterations cohort thereby representing 10 cohort highest potential clinical significance considered multiple hypothesis testing results summarized supplementary table s6 fig 2a ercc2 mutation frequency responders compared unselected tcga 21 guo colleagues 22 cohorts binomial test results analysis made available fig 2c comparison ercc2 mutation distribution responders nonresponders adjusted elevated mutation rate responders performed binomial test conditional observing nine mutations using estimated ratio mutation rates responders nonresponders expected frequency ercc2 mutations responders null hypothesis e g ercc2 binomial test x 9 n 9 p mutation rateresponders mutation ratenonresponders alternative greater statistical calculations performed using r statistical package mutation validation orthogonal validation selected mutations short insertion deletions presented article including ercc2 performed using fluidigm access array 50 cases 35 sufficient dna generate sufficient read depth analysis total 85 candidate targets submitted fluidigm single plex pcr primer assay design resulted design 65 assays covering 85 targets assay amplicons ranged 163 199 bp size average 183 bp available samples run access array system fluidigm using three 48 48 access array ifc chips following manufacturer recommendations using 4 primer amplicon tagging protocol access array fluidigm p n 100 3770 rev c1 exception access array ifc chips loaded harvested using bravo automated liquid handling platform agilent technologies following manufacturer recommendations resulting amplicons containing sample specific barcodes illumina adapter sequencers pooled sequenced miseq sequencer illumina two runs 150 base paired end reads v2 sequencing chemistry using custom fluidigm sequencing primers following manufacturer recommendations fluidigm sites manually reviewed igv determine presence absence nonreference reads details validation results ercc2 additional variants given supplementary table s2 variants inadequate sample validation insufficient sequencing reads validation data interpret manually igv listed unavailable cloning cell lines site directed pcr mutagenesis bp attb attp recombination method 42 used generate wt mutant ercc2 open reading frames orf mutant pcr products generated fragments overlap region desired mutation fragments introduced pdonr vector bp reaction bp reaction mixture transformed escherichia coli recombined generate pentr vector pentr vector used perform lr attl attr reaction create expression plasmid expression plasmids harboring wt ercc2 gfp negative control mutant ercc2 constructs expanded e coli top10 cells invitrogen purified using anion exchange kit qiagen lentiviruses propagated 293t cells cotransfection expression plasmid plasmids encoding viral packaging envelope proteins unless otherwise noted human cell lines cultured dmem invitrogen supplemented 10 fbs sigma 1 l glutamine grown 37 c 5 co2 293t cell supernatants containing virus collected 48 hours filtered twice 0 45 syringe filter millipore added directly growing cultures sv40 transformed pseudodiploid ercc2 deficient fibroblast cell line derived xp patient genetic complementation group gm08207 coriell institute cell line compound heterozygote harboring ercc2r683w ercc2 del 36 61 mutations polybrene sigma added final concentration 8 g ml increase efficiency infection stable integrates selected incubation 5 days media containing 10 g ml blastocidin physical biologic containment procedures recombinant dna followed institutional protocols accordance nih guidelines research involving recombinant dna molecules cell line obtained march 2013 corriell cell repositories camden nj authenticated microsatellite genotyping cisplatin sensitivity assays cells transferred 96 well plates density 500 cells per well six hours later cisplatin sigma serially diluted media added wells 96 hours celltiterglo reagent promega added wells plates scanned using luminescence microplate reader biotek survival cisplatin concentration plotted percentage survival cisplatin free media data point graph represents average three independent measurements error bars represent standard deviation ic50 concentrations calculated using four parameter sigmoidal model plots generated using prism graphpad one way anova test used compare ic50 negative control cell line ic50 wt mutant ercc2 cell lines compare ic50 wt line ic50 negative control mutant cell lines uv clonogenic survival assays cells seeded 6 well plates nunc density 1 500 cells per well following day cells washed pbs exposed increasing uv doses using uv b irradiator stratagene medium replaced cells allowed grow 9 days day 10 cells fixed using 1 5 acetic acid methanol solution 20 minutes room temperature cells stained 45 minutes using 1 crystal violet methanol solution plates rinsed vigorously water allowed dry colonies manually counted number colonies present uv dose plotted ratio number colonies present mock irradiated wells data point represents average three independent measurements error bars represent standard deviation chromosomal breakage analysis approximately 1 106 cells seeded per 10 cm dish 24 hours 400 nmol l cisplatin added cells allowed grow additional 48 hours cells exposed colcemid 2 hours harvested using 0 075 mol l kcl fixed 3 1 methanol acetic acid slides stained wright stain 25 50 metaphases analyzed chromosomal aberrations chromosome chromatid breaks rings translocations deletions fragments double minute chromosomes tri quadraradials di tricentrics premature chromatid separation immunoblots frozen cell pellets thawed resuspended ripa buffer 50 mmol l tris ph 7 3 150 mmol l nacl 1 mmol l edta 1 triton x 100 0 5 na deoxycholate 0 1 sds supplemented complete protease inhibitor roche navo4 naf cell suspensions centrifuged total protein concentration supernatant determined colorimetry bio rad samples boiled loading buffer bio rad electrophoresed 3 8 gradient tris acetate gel life technologies resolved proteins transferred polyvinylidene difluoride pvdf membrane millipore 90 v 2 hours 4 c membrane blocked 1 hour blocking solution 5 milk tris buffered saline incubated primary antibody blocking solution 4 c overnight mouse ercc2 abcam rabbit actin cell signaling technology following day membrane rinsed incubated 1 hour peroxidase conjugated secondary antibody blocking solution anti mouse anti rabbit cell signaling technology rinsed enhanced chemiluminescent substrate solution perkinelmer added signal detected film exposure ge healthcare '</code>\n</pre> <pre><code>no_feature\n</code></pre> <pre>\n<code>100</code>\n</pre> <pre><code>test_df['Gene'].iloc[test_point_index]\n</code></pre> <pre>\n<code>'ERCC2'</code>\n</pre> <pre><code>test_df['Variation'].iloc[test_point_index]\n</code></pre> <pre>\n<code>'E606G'</code>\n</pre> <pre><code>clf.feature_log_prob_.shape\n</code></pre> <pre>\n<code>(9, 55944)</code>\n</pre> <pre><code>indices=np.argsort(-1*abs(clf.feature_log_prob_))[predicted_cls-1][:,:no_feature]\nindices[0]\n</code></pre> <pre>\n<code>array([    0, 36561, 36568, 36571, 36582, 36583, 36592, 16154, 16153,\n       36558, 36594, 36596, 36597, 36601, 36604, 36607, 36609, 36637,\n       36638, 36595, 36557, 36556, 36552, 36530, 36531, 36532, 36533,\n       36534, 36535, 36536, 16180, 16179, 36537, 36538, 16176, 36539,\n       36540, 36541, 36542, 36547, 36551, 16169, 36639, 36643, 36647,\n       16139, 16111, 36718, 36719, 36721, 16106, 36722, 16104, 36725,\n       36726, 16101, 36727, 36729, 16097, 36734, 36735, 36737, 16092,\n       36738, 36740, 36715, 36529, 36713, 36704, 16138, 36657, 36660,\n       16133, 36666, 36671, 36675, 16129, 36676, 16127, 36678, 36687,\n       36688, 36693, 36694, 36695, 36698, 16117, 36702, 36709, 36742,\n       36528, 36527, 36445, 36447, 16256, 36448, 36449, 36452, 36455,\n       36456])</code>\n</pre> <pre><code># this function will be used just for naive bayes\n# for the given indices, we will print the name of the features\n# and we will check whether the feature present in the test point text or not\ndef get_impfeature_names(indices, text, gene, var, no_features):\n    gene_count_vec = CountVectorizer()\n    var_count_vec = CountVectorizer()\n    text_count_vec = CountVectorizer(min_df=3)\n\n    gene_vec = gene_count_vec.fit(train_df['Gene'])\n    var_vec  = var_count_vec.fit(train_df['Variation'])\n    text_vec = text_count_vec.fit(train_df['TEXT'])\n\n    fea1_len = len(gene_vec.get_feature_names_out())\n    fea2_len = len(var_count_vec.get_feature_names_out())\n\n    word_present = 0\n    for i,v in enumerate(indices):\n        if (v &amp;lt; fea1_len):\n            word = gene_vec.get_feature_names_out()[v]\n            yes_no = True if word == gene else False\n            if yes_no:\n                word_present += 1\n                print(i, \"Gene feature [{}] present in test data point [{}]\".format(word,yes_no))\n        elif (v &amp;lt; fea1_len+fea2_len):\n            word = var_vec.get_feature_names_out()[v-(fea1_len)]\n            yes_no = True if word == var else False\n            if yes_no:\n                word_present += 1\n                print(i, \"variation feature [{}] present in test data point [{}]\".format(word,yes_no))\n        else:\n            word = text_vec.get_feature_names_out()[v-(fea1_len+fea2_len)]\n            yes_no = True if word in text.split() else False\n            if yes_no:\n                word_present += 1\n                print(i, \"Text feature [{}] present in test data point [{}]\".format(word,yes_no))\n\n    print(\"Out of the top \",no_features,\" features \", word_present, \"are present in query point\")\n</code></pre> <pre><code>for i in range(10):\n    test_point_index = i\n    no_feature = 100\n    predicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\n    print(\"Predicted Class :\", predicted_cls[0])\n    print(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\n    print(\"Actual Class :\", test_y[test_point_index])\n    indices=np.argsort(-1*abs(clf.feature_log_prob_))[predicted_cls-1][:,:no_feature]\n    print(\"-\"*50)\n    get_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index].lower(),test_df['Variation'].iloc[test_point_index], no_feature)\n</code></pre> <pre>\n<code>Predicted Class : 6\nPredicted Class Probabilities: [4.090e-02 1.070e-02 5.400e-03 5.280e-02 7.330e-02 7.635e-01 4.870e-02\n  4.300e-03 3.000e-04](../4090e-02-1070e-02-5400e-03-5280e-02-7330e-02-7635e-01-4870e-02--4300e-03-3000e-04 \"4.090e-02 1.070e-02 5.400e-03 5.280e-02 7.330e-02 7.635e-01 4.870e-02\n  4.300e-03 3.000e-04\")\nActual Class : 6\n--------------------------------------------------\nOut of the top  100  features  0 are present in query point\nPredicted Class : 1\nPredicted Class Probabilities: [0.4597 0.1588 0.0104 0.0636 0.0274 0.0241 0.2489 0.0059 0.0011](../04597-01588-00104-00636-00274-00241-02489-00059-00011 \"0.4597 0.1588 0.0104 0.0636 0.0274 0.0241 0.2489 0.0059 0.0011\")\nActual Class : 1\n--------------------------------------------------\nOut of the top  100  features  0 are present in query point\nPredicted Class : 7\nPredicted Class Probabilities: [2.480e-02 8.020e-02 5.000e-04 8.200e-03 3.200e-03 1.100e-03 8.782e-01\n  3.800e-03 0.000e+00](../2480e-02-8020e-02-5000e-04-8200e-03-3200e-03-1100e-03-8782e-01--3800e-03-0000e00 \"2.480e-02 8.020e-02 5.000e-04 8.200e-03 3.200e-03 1.100e-03 8.782e-01\n  3.800e-03 0.000e+00\")\nActual Class : 2\n--------------------------------------------------\nOut of the top  100  features  0 are present in query point\nPredicted Class : 7\nPredicted Class Probabilities: [0.0345 0.3528 0.0149 0.0416 0.0342 0.0314 0.4819 0.0067 0.0019](../00345-03528-00149-00416-00342-00314-04819-00067-00019 \"0.0345 0.3528 0.0149 0.0416 0.0342 0.0314 0.4819 0.0067 0.0019\")\nActual Class : 2\n--------------------------------------------------\nOut of the top  100  features  0 are present in query point\nPredicted Class : 7\nPredicted Class Probabilities: [0.0698 0.0858 0.0264 0.1075 0.0507 0.0511 0.5949 0.0094 0.0044](../00698-00858-00264-01075-00507-00511-05949-00094-00044 \"0.0698 0.0858 0.0264 0.1075 0.0507 0.0511 0.5949 0.0094 0.0044\")\nActual Class : 7\n--------------------------------------------------\nOut of the top  100  features  0 are present in query point\nPredicted Class : 4\nPredicted Class Probabilities: [0.0705 0.0504 0.0164 0.6325 0.0329 0.03   0.1588 0.0063 0.0021](../00705-00504-00164-06325-00329-003---01588-00063-00021 \"0.0705 0.0504 0.0164 0.6325 0.0329 0.03   0.1588 0.0063 0.0021\")\nActual Class : 7\n--------------------------------------------------\nOut of the top  100  features  0 are present in query point\nPredicted Class : 7\nPredicted Class Probabilities: [0.0673 0.0258 0.0018 0.1074 0.0071 0.0048 0.7812 0.0047 0.    ](../00673-00258-00018-01074-00071-00048-07812-00047-0---- \"0.0673 0.0258 0.0018 0.1074 0.0071 0.0048 0.7812 0.0047 0.    \")\nActual Class : 7\n--------------------------------------------------\n59 Text feature [bub1] present in test data point [True]\nOut of the top  100  features  1 are present in query point\nPredicted Class : 7\nPredicted Class Probabilities: [0.0984 0.2299 0.0324 0.1407 0.06   0.0666 0.356  0.0094 0.0067](../00984-02299-00324-01407-006---00666-0356--00094-00067 \"0.0984 0.2299 0.0324 0.1407 0.06   0.0666 0.356  0.0094 0.0067\")\nActual Class : 2\n--------------------------------------------------\nOut of the top  100  features  0 are present in query point\nPredicted Class : 7\nPredicted Class Probabilities: [0.0556 0.1443 0.054  0.0837 0.0528 0.0493 0.5469 0.009  0.0045](../00556-01443-0054--00837-00528-00493-05469-0009--00045 \"0.0556 0.1443 0.054  0.0837 0.0528 0.0493 0.5469 0.009  0.0045\")\nActual Class : 7\n--------------------------------------------------\nOut of the top  100  features  0 are present in query point\nPredicted Class : 7\nPredicted Class Probabilities: [5.400e-03 5.200e-03 3.200e-03 6.800e-03 8.700e-03 4.900e-03 9.607e-01\n  5.000e-03 1.000e-04](../5400e-03-5200e-03-3200e-03-6800e-03-8700e-03-4900e-03-9607e-01--5000e-03-1000e-04 \"5.400e-03 5.200e-03 3.200e-03 6.800e-03 8.700e-03 4.900e-03 9.607e-01\n  5.000e-03 1.000e-04\")\nActual Class : 7\n--------------------------------------------------\nOut of the top  100  features  0 are present in query point\n</code>\n</pre> <pre><code>test_point_index = 100\nno_feature = 100\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-1*abs(clf.feature_log_prob_))[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n</code></pre> <pre>\n<code>Predicted Class : 7\nPredicted Class Probabilities: [3.800e-02 7.340e-02 3.700e-03 4.260e-02 1.190e-02 9.300e-03 8.155e-01\n  5.500e-03 1.000e-04](../3800e-02-7340e-02-3700e-03-4260e-02-1190e-02-9300e-03-8155e-01--5500e-03-1000e-04 \"3.800e-02 7.340e-02 3.700e-03 4.260e-02 1.190e-02 9.300e-03 8.155e-01\n  5.500e-03 1.000e-04\")\nActual Class : 7\n--------------------------------------------------\nOut of the top  100  features  0 are present in query point\n</code>\n</pre> <pre><code>alpha = [5, 11, 15, 21, 31, 41, 51, 99]\ncv_log_error_array = []\nfor i in alpha:\n    print(\"for alpha =\", i)\n    clf = KNeighborsClassifier(n_neighbors=i)\n    clf.fit(train_x_responseCoding, train_y)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_x_responseCoding, train_y)\n    sig_clf_probs = sig_clf.predict_proba(cv_x_responseCoding)\n    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n    # to avoid rounding error while multiplying probabilites we use log-probability estimates\n    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs)) \n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = KNeighborsClassifier(n_neighbors=alpha[best_alpha])\nclf.fit(train_x_responseCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_responseCoding, train_y)\n\npredict_y = sig_clf.predict_proba(train_x_responseCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_x_responseCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_x_responseCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n</code></pre> <pre>\n<code>for alpha = 5\nLog Loss : 1.1036779125472609\nfor alpha = 11\nLog Loss : 1.0683198189507492\nfor alpha = 15\nLog Loss : 1.0580822704500141\nfor alpha = 21\nLog Loss : 1.0563308294497766\nfor alpha = 31\nLog Loss : 1.0560537205374698\nfor alpha = 41\nLog Loss : 1.0826667857571959\nfor alpha = 51\nLog Loss : 1.0901258431039602\nfor alpha = 99\nLog Loss : 1.1253028140655437\n</code>\n</pre> <pre>\n<code>For values of best alpha =  31 The train log loss is: 0.7840473307464905\nFor values of best alpha =  31 The cross validation log loss is: 1.0560537205374698\nFor values of best alpha =  31 The test log loss is: 1.1103171665646643\n</code>\n</pre> <pre><code>clf = KNeighborsClassifier(n_neighbors=alpha[best_alpha])\npredict_and_plot_confusion_matrix(train_x_responseCoding, train_y, cv_x_responseCoding, cv_y, clf)\n</code></pre> <pre>\n<code>Log loss : 1.0560537205374698\nNumber of mis-classified points : 0.3684210526315789\n</code>\n</pre> <pre><code>clf = KNeighborsClassifier(n_neighbors=alpha[best_alpha])\nclf.fit(train_x_responseCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_responseCoding, train_y)\n\ntest_point_index = 1\npredicted_cls = sig_clf.predict(test_x_responseCoding[0].reshape(1,-1))\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Actual Class :\", test_y[test_point_index])\nneighbors = clf.kneighbors(test_x_responseCoding[test_point_index].reshape(1, -1), alpha[best_alpha])\nprint(\"The \",alpha[best_alpha],\" nearest neighbours of the test points belongs to classes\",train_y[neighbors[1][0]])\nprint(\"Fequency of nearest points :\",Counter(train_y[neighbors[1][0]]))\n</code></pre> <pre>\n<code>Predicted Class : 6\nActual Class : 1\nThe  31  nearest neighbours of the test points belongs to classes [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 4 1 1 1 1 1 1 1 5 1 1 1 1]\nFequency of nearest points : Counter({1: 28, 2: 1, 4: 1, 5: 1})\n</code>\n</pre> <pre><code>clf = KNeighborsClassifier(n_neighbors=alpha[best_alpha])\nclf.fit(train_x_responseCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_responseCoding, train_y)\n\ntest_point_index = 100\n\npredicted_cls = sig_clf.predict(test_x_responseCoding[test_point_index].reshape(1,-1))\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Actual Class :\", test_y[test_point_index])\nneighbors = clf.kneighbors(test_x_responseCoding[test_point_index].reshape(1, -1), alpha[best_alpha])\nprint(\"the k value for knn is\",alpha[best_alpha],\"and the nearest neighbours of the test points belongs to classes\",train_y[neighbors[1][0]])\nprint(\"Fequency of nearest points :\",Counter(train_y[neighbors[1][0]]))\n</code></pre> <pre>\n<code>Predicted Class : 7\nActual Class : 7\nthe k value for knn is 31 and the nearest neighbours of the test points belongs to classes [7 2 7 7 2 7 2 2 2 7 6 7 7 2 7 7 7 7 6 2 7 7 2 7 7 7 2 6 7 2 2]\nFequency of nearest points : Counter({7: 17, 2: 11, 6: 3})\n</code>\n</pre> <pre><code>alpha = [10 ** x for x in range(-6, 3)]\ncv_log_error_array = []\nfor i in alpha:\n    print(\"for alpha =\", i)\n    clf = SGDClassifier(class_weight='balanced', alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(train_x_onehotCoding, train_y)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_x_onehotCoding, train_y)\n    sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n    # to avoid rounding error while multiplying probabilites we use log-probability estimates\n    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs)) \n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(train_x_onehotCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_onehotCoding, train_y)\n\npredict_y = sig_clf.predict_proba(train_x_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_x_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_x_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n</code></pre> <pre>\n<code>for alpha = 1e-06\nLog Loss : 1.3549611984629366\nfor alpha = 1e-05\nLog Loss : 1.3243830764474769\nfor alpha = 0.0001\nLog Loss : 1.1484035371750703\nfor alpha = 0.001\nLog Loss : 1.1469421660609054\nfor alpha = 0.01\nLog Loss : 1.2277813159484565\nfor alpha = 0.1\nLog Loss : 1.443192254191518\nfor alpha = 1\nLog Loss : 1.6513624798101794\nfor alpha = 10\nLog Loss : 1.6801064048265375\nfor alpha = 100\nLog Loss : 1.6832847660500232\n</code>\n</pre> <pre>\n<code>For values of best alpha =  0.001 The train log loss is: 0.5051957904999469\nFor values of best alpha =  0.001 The cross validation log loss is: 1.1469421660609054\nFor values of best alpha =  0.001 The test log loss is: 1.1285955963127317\n</code>\n</pre> <pre><code>clf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\npredict_and_plot_confusion_matrix(train_x_onehotCoding, train_y, cv_x_onehotCoding, cv_y, clf)\n</code></pre> <pre>\n<code>Log loss : 1.1469421660609054\nNumber of mis-classified points : 0.35902255639097747\n</code>\n</pre> <pre><code>def get_imp_feature_names(text, indices, removed_ind = []):\n    word_present = 0\n    tabulte_list = []\n    incresingorder_ind = 0\n    for i in indices:\n        if i &amp;lt; train_gene_feature_onehotCoding.shape[1]:\n            tabulte_list.append([incresingorder_ind, \"Gene\", \"Yes\"])\n        elif i&amp;lt; 18:\n            tabulte_list.append([incresingorder_ind,\"Variation\", \"Yes\"])\n        if ((i &amp;gt; 17) &amp;amp; (i not in removed_ind)) :\n            word = train_text_features[i]\n            yes_no = True if word in text.split() else False\n            if yes_no:\n                word_present += 1\n            tabulte_list.append([incresingorder_ind,train_text_features[i], yes_no])\n        incresingorder_ind += 1\n    print(word_present, \"most importent features are present in our query point\")\n    print(\"-\"*50)\n    print(\"The features that are most importent of the \",predicted_cls[0],\" class:\")\n    print (tabulate(tabulte_list, headers=[\"Index\",'Feature name', 'Present or Not']))\n</code></pre> <pre><code># from tabulate import tabulate\nclf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(train_x_onehotCoding,train_y)\ntest_point_index = 1\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-1*abs(clf.coef_))[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n</code></pre> <pre>\n<code>Predicted Class : 1\nPredicted Class Probabilities: [0.8104 0.1318 0.0037 0.0126 0.0098 0.0058 0.0193 0.0046 0.002 ](../08104-01318-00037-00126-00098-00058-00193-00046-0002- \"0.8104 0.1318 0.0037 0.0126 0.0098 0.0058 0.0193 0.0046 0.002 \")\nActual Class : 1\n--------------------------------------------------\n252 Text feature [aggregated] present in test data point [True]\n292 Text feature [processing] present in test data point [True]\n342 Text feature [expressing] present in test data point [True]\n367 Text feature [archaebacterial] present in test data point [True]\n400 Text feature [contest] present in test data point [True]\n412 Text feature [premature] present in test data point [True]\nOut of the top  500  features  6 are present in query point\n</code>\n</pre> <pre><code>test_point_index = 100\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-1*abs(clf.coef_))[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n</code></pre> <pre>\n<code>Predicted Class : 7\nPredicted Class Probabilities: [0.0168 0.1361 0.0015 0.0186 0.0058 0.0051 0.8105 0.0043 0.0013](../00168-01361-00015-00186-00058-00051-08105-00043-00013 \"0.0168 0.1361 0.0015 0.0186 0.0058 0.0051 0.8105 0.0043 0.0013\")\nActual Class : 7\n--------------------------------------------------\n87 Text feature [constitutive] present in test data point [True]\n119 Text feature [tx] present in test data point [True]\n123 Text feature [thyroid] present in test data point [True]\n133 Text feature [constitutively] present in test data point [True]\n135 Text feature [3t3] present in test data point [True]\n159 Text feature [expressing] present in test data point [True]\n193 Text feature [ligand] present in test data point [True]\n207 Text feature [function] present in test data point [True]\n213 Text feature [egf] present in test data point [True]\n221 Text feature [activator] present in test data point [True]\n238 Text feature [activating] present in test data point [True]\n272 Text feature [receptors] present in test data point [True]\n276 Text feature [extracellular] present in test data point [True]\n315 Text feature [activation] present in test data point [True]\n320 Text feature [loss] present in test data point [True]\n322 Text feature [beas] present in test data point [True]\n377 Text feature [y1068] present in test data point [True]\n392 Text feature [stably] present in test data point [True]\n452 Text feature [putative] present in test data point [True]\n465 Text feature [purified] present in test data point [True]\n475 Text feature [predicted] present in test data point [True]\nOut of the top  500  features  21 are present in query point\n</code>\n</pre> <pre><code>alpha = [10 ** x for x in range(-5, 3)]\ncv_log_error_array = []\nfor i in alpha:\n    print(\"for C =\", i)\n#     clf = SVC(C=i,kernel='linear',probability=True, class_weight='balanced')\n    clf = SGDClassifier( class_weight='balanced', alpha=i, penalty='l2', loss='hinge', random_state=42)\n    clf.fit(train_x_onehotCoding, train_y)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_x_onehotCoding, train_y)\n    sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs)) \n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\n# clf = SVC(C=i,kernel='linear',probability=True, class_weight='balanced')\nclf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='hinge', random_state=42)\nclf.fit(train_x_onehotCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_onehotCoding, train_y)\n\npredict_y = sig_clf.predict_proba(train_x_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_x_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_x_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n</code></pre> <pre>\n<code>for C = 1e-05\nLog Loss : 1.3253802575992406\nfor C = 0.0001\nLog Loss : 1.2873766645967677\nfor C = 0.001\nLog Loss : 1.1596908320059625\nfor C = 0.01\nLog Loss : 1.1847648563983895\nfor C = 0.1\nLog Loss : 1.3879701310719252\nfor C = 1\nLog Loss : 1.6675922322714856\nfor C = 10\nLog Loss : 1.6838340386799557\nfor C = 100\nLog Loss : 1.6838340836749632\n</code>\n</pre> <pre>\n<code>For values of best alpha =  0.001 The train log loss is: 0.5220013914091015\nFor values of best alpha =  0.001 The cross validation log loss is: 1.1596908320059625\nFor values of best alpha =  0.001 The test log loss is: 1.168790814376124\n</code>\n</pre> <pre><code># clf = SVC(C=alpha[best_alpha],kernel='linear',probability=True, class_weight='balanced')\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='hinge', random_state=42,class_weight='balanced')\npredict_and_plot_confusion_matrix(train_x_onehotCoding, train_y,cv_x_onehotCoding,cv_y, clf)\n</code></pre> <pre>\n<code>Log loss : 1.1596908320059625\nNumber of mis-classified points : 0.3684210526315789\n</code>\n</pre> <pre><code>clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='hinge', random_state=42)\nclf.fit(train_x_onehotCoding,train_y)\ntest_point_index = 1\n# test_point_index = 100\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-1*abs(clf.coef_))[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n</code></pre> <pre>\n<code>Predicted Class : 1\nPredicted Class Probabilities: [0.7576 0.0853 0.009  0.0327 0.023  0.0188 0.0649 0.0035 0.0051](../07576-00853-0009--00327-0023--00188-00649-00035-00051 \"0.7576 0.0853 0.009  0.0327 0.023  0.0188 0.0649 0.0035 0.0051\")\nActual Class : 1\n--------------------------------------------------\n414 Text feature [aggregated] present in test data point [True]\nOut of the top  500  features  1 are present in query point\n</code>\n</pre> <pre><code>test_point_index = 100\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-1*abs(clf.coef_))[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n</code></pre> <pre>\n<code>Predicted Class : 7\nPredicted Class Probabilities: [0.035  0.1263 0.0046 0.0505 0.0199 0.0145 0.7402 0.0037 0.0055](../0035--01263-00046-00505-00199-00145-07402-00037-00055 \"0.035  0.1263 0.0046 0.0505 0.0199 0.0145 0.7402 0.0037 0.0055\")\nActual Class : 7\n--------------------------------------------------\nOut of the top  500  features  0 are present in query point\n</code>\n</pre> <pre><code>train_x_onehotCoding = train_x_onehotCoding.toarray()\ncv_x_onehotCoding = cv_x_onehotCoding.toarray()\ntest_x_onehotCoding = test_x_onehotCoding.toarray()\n\nalpha = [100,200,500,1000,2000]\nmax_depth = [5, 10]\ncv_log_error_array = []\nfor i in alpha:\n    for j in max_depth:\n        print(\"for n_estimators =\", i,\"and max depth = \", j)\n        clf = RandomForestClassifier(n_estimators=i, criterion='gini', max_depth=j, random_state=42, n_jobs=-1)\n        clf.fit(train_x_onehotCoding, train_y)\n        sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n        sig_clf.fit(train_x_onehotCoding, train_y)\n        sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n        cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n        print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs)) \n\n'''fig, ax = plt.subplots()\nfeatures = np.dot(np.array(alpha)[:,None],np.array(max_depth)[None]).ravel()\nax.plot(features, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[int(i/2)],max_depth[int(i%2)],str(txt)), (features[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n'''\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = RandomForestClassifier(n_estimators=alpha[int(best_alpha/2)], criterion='gini', max_depth=max_depth[int(best_alpha%2)], random_state=42, n_jobs=-1)\nclf.fit(train_x_onehotCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_onehotCoding, train_y)\n\npredict_y = sig_clf.predict_proba(train_x_onehotCoding)\nprint('For values of best estimator = ', alpha[int(best_alpha/2)], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_x_onehotCoding)\nprint('For values of best estimator = ', alpha[int(best_alpha/2)], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_x_onehotCoding)\nprint('For values of best estimator = ', alpha[int(best_alpha/2)], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n</code></pre> <pre>\n<code>for n_estimators = 100 and max depth =  5\nLog Loss : 1.2202756659342509\nfor n_estimators = 100 and max depth =  10\nLog Loss : 1.159525801135362\nfor n_estimators = 200 and max depth =  5\nLog Loss : 1.2190339692179335\nfor n_estimators = 200 and max depth =  10\nLog Loss : 1.1575317490127754\nfor n_estimators = 500 and max depth =  5\nLog Loss : 1.2133198177641868\nfor n_estimators = 500 and max depth =  10\nLog Loss : 1.1525485271317322\nfor n_estimators = 1000 and max depth =  5\nLog Loss : 1.2117340996922596\nfor n_estimators = 1000 and max depth =  10\nLog Loss : 1.151182438108908\nfor n_estimators = 2000 and max depth =  5\nLog Loss : 1.2098738302870609\nfor n_estimators = 2000 and max depth =  10\nLog Loss : 1.1505321184953212\nFor values of best estimator =  2000 The train log loss is: 0.6727539823286758\nFor values of best estimator =  2000 The cross validation log loss is: 1.1505321184953212\nFor values of best estimator =  2000 The test log loss is: 1.1632614990596637\n</code>\n</pre> <pre><code>clf = RandomForestClassifier(n_estimators=alpha[int(best_alpha/2)], criterion='gini', max_depth=max_depth[int(best_alpha%2)], random_state=42, n_jobs=-1)\npredict_and_plot_confusion_matrix(train_x_onehotCoding, train_y,cv_x_onehotCoding,cv_y, clf)\n</code></pre> <pre>\n<code>Log loss : 1.1505321184953212\nNumber of mis-classified points : 0.35150375939849626\n</code>\n</pre> <pre><code># test_point_index = 10\nclf = RandomForestClassifier(n_estimators=alpha[int(best_alpha/2)], criterion='gini', max_depth=max_depth[int(best_alpha%2)], random_state=42, n_jobs=-1)\nclf.fit(train_x_onehotCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_onehotCoding, train_y)\n\ntest_point_index = 1\nno_feature = 100\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.feature_importances_)\nprint(\"-\"*50)\nget_impfeature_names(indices[:no_feature], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n</code></pre> <pre>\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n/var/folders/tz/6dwwl4_92xv0hspfrgj59nch0000gp/T/ipykernel_30515/3554025366.py in &lt;cell line: 9&gt;()\n      7 test_point_index = 1\n      8 no_feature = 100\n----&gt; 9 predicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\n     10 print(\"Predicted Class :\", predicted_cls[0])\n     11 print(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\n\n~/miniconda3/envs/jupyter/lib/python3.8/site-packages/sklearn/calibration.py in predict(self, X)\n    499         \"\"\"\n    500         check_is_fitted(self)\n--&gt; 501         return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n    502 \n    503     def _more_tags(self):\n\n~/miniconda3/envs/jupyter/lib/python3.8/site-packages/sklearn/calibration.py in predict_proba(self, X)\n    475         mean_proba = np.zeros((_num_samples(X), len(self.classes_)))\n    476         for calibrated_classifier in self.calibrated_classifiers_:\n--&gt; 477             proba = calibrated_classifier.predict_proba(X)\n    478             mean_proba += proba\n    479 \n\n~/miniconda3/envs/jupyter/lib/python3.8/site-packages/sklearn/calibration.py in predict_proba(self, X)\n    749         n_classes = len(self.classes)\n    750         pred_method, method_name = _get_prediction_method(self.estimator)\n--&gt; 751         predictions = _compute_predictions(pred_method, method_name, X, n_classes)\n    752 \n    753         label_encoder = LabelEncoder().fit(self.classes)\n\n~/miniconda3/envs/jupyter/lib/python3.8/site-packages/sklearn/calibration.py in _compute_predictions(pred_method, method_name, X, n_classes)\n    640         (X.shape[0], 1).\n    641     \"\"\"\n--&gt; 642     predictions = pred_method(X=X)\n    643 \n    644     if method_name == \"decision_function\":\n\n~/miniconda3/envs/jupyter/lib/python3.8/site-packages/sklearn/ensemble/_forest.py in predict_proba(self, X)\n    861         check_is_fitted(self)\n    862         # Check data\n--&gt; 863         X = self._validate_X_predict(X)\n    864 \n    865         # Assign chunk of trees to jobs\n\n~/miniconda3/envs/jupyter/lib/python3.8/site-packages/sklearn/ensemble/_forest.py in _validate_X_predict(self, X)\n    601         Validate X whenever one tries to predict, apply, predict_proba.\"\"\"\n    602         check_is_fitted(self)\n--&gt; 603         X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n    604         if issparse(X) and (X.indices.dtype != np.intc or X.indptr.dtype != np.intc):\n    605             raise ValueError(\"No support for np.int64 index based sparse matrices\")\n\n~/miniconda3/envs/jupyter/lib/python3.8/site-packages/sklearn/base.py in _validate_data(self, X, y, reset, validate_separately, **check_params)\n    533             raise ValueError(\"Validation should be done on X, y or both.\")\n    534         elif not no_val_X and no_val_y:\n--&gt; 535             X = check_array(X, input_name=\"X\", **check_params)\n    536             out = X\n    537         elif no_val_X and not no_val_y:\n\n~/miniconda3/envs/jupyter/lib/python3.8/site-packages/sklearn/utils/validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\n    898             # If input is 1D raise error\n    899             if array.ndim == 1:\n--&gt; 900                 raise ValueError(\n    901                     \"Expected 2D array, got 1D array instead:\\narray={}.\\n\"\n    902                     \"Reshape your data either using array.reshape(-1, 1) if \"\n\nValueError: Expected 2D array, got 1D array instead:\narray=[0. 0. 0. ... 0. 0. 0.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.</pre> <pre><code>test_point_index = 100\nno_feature = 100\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actuall Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.feature_importances_)\nprint(\"-\"*50)\nget_impfeature_names(indices[:no_feature], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)\n</code></pre> <pre><code>alpha = [10,50,100,200,500,1000]\nmax_depth = [2,3,5,10]\ncv_log_error_array = []\nfor i in alpha:\n    for j in max_depth:\n        print(\"for n_estimators =\", i,\"and max depth = \", j)\n        clf = RandomForestClassifier(n_estimators=i, criterion='gini', max_depth=j, random_state=42, n_jobs=-1)\n        clf.fit(train_x_responseCoding, train_y)\n        sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n        sig_clf.fit(train_x_responseCoding, train_y)\n        sig_clf_probs = sig_clf.predict_proba(cv_x_responseCoding)\n        cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n        print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs)) \n'''\nfig, ax = plt.subplots()\nfeatures = np.dot(np.array(alpha)[:,None],np.array(max_depth)[None]).ravel()\nax.plot(features, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[int(i/4)],max_depth[int(i%4)],str(txt)), (features[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n'''\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = RandomForestClassifier(n_estimators=alpha[int(best_alpha/4)], criterion='gini', max_depth=max_depth[int(best_alpha%4)], random_state=42, n_jobs=-1)\nclf.fit(train_x_responseCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_responseCoding, train_y)\n\npredict_y = sig_clf.predict_proba(train_x_responseCoding)\nprint('For values of best alpha = ', alpha[int(best_alpha/4)], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_x_responseCoding)\nprint('For values of best alpha = ', alpha[int(best_alpha/4)], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_x_responseCoding)\nprint('For values of best alpha = ', alpha[int(best_alpha/4)], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n</code></pre> <pre><code>clf = RandomForestClassifier(max_depth=max_depth[int(best_alpha%4)], n_estimators=alpha[int(best_alpha/4)], criterion='gini', max_features='auto',random_state=42)\npredict_and_plot_confusion_matrix(train_x_responseCoding, train_y,cv_x_responseCoding,cv_y, clf)\n</code></pre> <pre><code>clf = RandomForestClassifier(n_estimators=alpha[int(best_alpha/4)], criterion='gini', max_depth=max_depth[int(best_alpha%4)], random_state=42, n_jobs=-1)\nclf.fit(train_x_responseCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_responseCoding, train_y)\n\n\ntest_point_index = 1\nno_feature = 27\npredicted_cls = sig_clf.predict(test_x_responseCoding[test_point_index].reshape(1,-1))\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_responseCoding[test_point_index].reshape(1,-1)),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.feature_importances_)\nprint(\"-\"*50)\nfor i in indices:\n    if i&amp;lt;9:\n        print(\"Gene is important feature\")\n    elif i&amp;lt;18:\n        print(\"Variation is important feature\")\n    else:\n        print(\"Text is important feature\")\n</code></pre> <pre><code>test_point_index = 100\npredicted_cls = sig_clf.predict(test_x_responseCoding[test_point_index].reshape(1,-1))\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_responseCoding[test_point_index].reshape(1,-1)),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.feature_importances_)\nprint(\"-\"*50)\nfor i in indices:\n    if i&amp;lt;9:\n        print(\"Gene is important feature\")\n    elif i&amp;lt;18:\n        print(\"Variation is important feature\")\n    else:\n        print(\"Text is important feature\")\n</code></pre>"},{"location":"Applied%20Solutions/Cancer%20Treatment/#cancer-treatment","title":"Cancer Treatment","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#business-understanding","title":"Business Understanding","text":"<p>Once sequenced, a cancer tumor can have thousands of genetic mutations. But the challenge is distinguishing the mutations that contribute to tumor growth (drivers) from the neutral mutations (passengers). Currently this interpretation of genetic mutations is being done manually. This is a very time-consuming task where a clinical pathologist has to manually review and classify every single genetic mutation based on evidence from text-based clinical literature. For this study we are using MSKCC Data set which has an expert-annotated knowledge base where  researchers and oncologists have manually annotated thousands of mutations. We need to develop a Machine Learning algorithm that, using this knowledge base as a baseline, automatically classifies genetic variations.</p> <p></p> <p>Data Credits : Kaggle - Memorial Sloan Kettering Cancer Center (MSKCC)</p>"},{"location":"Applied%20Solutions/Cancer%20Treatment/#domain-review","title":"Domain Review","text":"<ul> <li>forbes</li> <li>video1</li> <li>video2</li> <li>kaggle</li> </ul>"},{"location":"Applied%20Solutions/Cancer%20Treatment/#business-objectives-and-constraints","title":"Business Objectives and Constraints","text":"<ul> <li>Classify the given genetic variations/mutations based on evidence from text-based clinical literature.</li> <li>This model should be useful to decrease the man hours required to solve the problem</li> <li>Output a probability of the prediced class so that the pathologist can be confident about the classification </li> <li>The cost of a mis-classification and errors is very high.</li> <li>Interpretability is really important.</li> <li>No strict latency concerns.</li> </ul>"},{"location":"Applied%20Solutions/Cancer%20Treatment/#machine-learing-problem-statement","title":"Machine Learing Problem Statement","text":"<ul> <li>Type of Machine Leaning Problem : It is a multi(9) class classification problem</li> <li>Performance Metric :<ul> <li>Cross Entropy or Multi class log-loss</li> <li>Confusion Matrix</li> </ul> </li> </ul>"},{"location":"Applied%20Solutions/Cancer%20Treatment/#we-will-start-by-importing-important-libraries-and-defining-some-constants","title":"we will start by importing important libraries and defining some constants","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#data-overview","title":"Data Overview","text":"<p>There are 2 datasets available in raw files we will load these into <code>df_variants</code> and <code>df_text</code></p>"},{"location":"Applied%20Solutions/Cancer%20Treatment/#glimpse-of-df_variants","title":"Glimpse of <code>df_variants</code>","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#glimpse-of-df_text","title":"Glimpse of <code>df_text</code>","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#preprocessing-of-text","title":"Preprocessing of text","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#test-train-and-cross-validation-split","title":"Test, Train and Cross Validation Split","text":"<p>Splitting the dataset randomly into three parts train, cross validation and test with 64%,16%, 20% of data respectively</p>"},{"location":"Applied%20Solutions/Cancer%20Treatment/#distribution-of-y_is-in-train-test-and-cross-validation-datasets","title":"Distribution of \\(y_i's\\) in Train, Test and Cross Validation datasets","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#random-model-prediction","title":"Random Model Prediction","text":"<p>Now we will create a random model which would serve as a dumb baseline and we can measure the performace of our model against that model </p>"},{"location":"Applied%20Solutions/Cancer%20Treatment/#univariate-analysis","title":"Univariate Analysis","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#process-description","title":"Process Description","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#high-cardinaldimensionality-categorical-feature","title":"High Cardinal/Dimensionality Categorical Feature","text":"<ol> <li> <p>How many categories are there and How they are distributed?</p> </li> <li> <p>How to encode high dimensionality categorical feature ? </p> <ul> <li>One hot Encoding (Higher No of Dimensions)</li> <li>Response coding (Lower No of Dimensions) with laplace smoothing <ul> <li>Laplace smoothing is used during caculation of probability of a feature value belongs to any particular class</li> </ul> </li> </ul> </li> </ol>"},{"location":"Applied%20Solutions/Cancer%20Treatment/#text-based-feature","title":"Text Based Feature","text":"<ol> <li>How many unique words are present in train data?</li> <li>How are word frequencies distributed?</li> <li>How to encode text features to numbers?</li> </ol>"},{"location":"Applied%20Solutions/Cancer%20Treatment/#feature-impact-analysis","title":"Feature Impact Analysis","text":"<ol> <li> <p>How good is a feature in predicting \\(y_i\\)?</p> <ul> <li>Build an ML model using just the feature in question. For Ex. build a logistic regression model using the feature to predict \\(y_i\\). Compare the results and metrics with the Random Model prediction.</li> </ul> </li> <li> <p>Is the feature stable across all the data sets (Test, Train, Cross validation)</p> <ul> <li>Check the distribution across Train and Cross validation sets</li> <li>Check the metrics for Cross validation and Test sets done in Impact Analysis (should be comparable to train)</li> <li>Coverage: How many data points in Test and CV datasets are covered by the unique feature values in train dataset?</li> </ul> </li> </ol>"},{"location":"Applied%20Solutions/Cancer%20Treatment/#eda","title":"EDA","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#feature-transformation","title":"Feature Transformation","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#gene-feature","title":"Gene Feature","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#variation-feature","title":"Variation Feature","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#text-feature","title":"Text Feature","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#transformed-feature-eda","title":"Transformed Feature EDA","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#impact-anlaysis","title":"Impact Anlaysis","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#gene-feature-one-hot-encoded-using-sgd-classifier-calibration-model-with-log-loss","title":"Gene Feature One hot Encoded using SGD classifier + Calibration model with Log Loss","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#variation-feature-one-hot-encoded-using-sgd-classifier-calibration-model-with-log-loss","title":"Variation Feature One hot Encoded using SGD classifier + Calibration model with Log Loss","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#text-feature-one-hot-encoded-using-sgd-classifier-calibration-model-with-log-loss","title":"Text Feature One hot Encoded using SGD classifier  + Calibration model with Log Loss","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#machine-learning-models","title":"Machine Learning Models","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#stacking-all-features","title":"Stacking all features","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#naive-bayes","title":"Naive Bayes","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#model-testing-with-best-hyper-paramters","title":"Model Testing with best hyper paramters","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#feature-importance-correctly-classified-point","title":"Feature Importance, Correctly classified point","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#feature-importance-incorrectly-classified-point","title":"Feature Importance, Incorrectly classified point","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#nearest-neighbour-classification","title":"Nearest Neighbour Classification","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#hyper-parameter-tuning","title":"Hyper parameter tuning","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#testing-the-model-with-best-hyper-paramters","title":"Testing the model with best hyper paramters","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#sample-query-point-1","title":"Sample Query point -1","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#sample-query-point-2","title":"Sample Query Point-2","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#logistic-regression","title":"Logistic Regression","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#hyper-paramter-tuning","title":"Hyper paramter tuning","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#testing-the-model-with-best-hyper-paramters_1","title":"Testing the model with best hyper paramters","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#feature-importance-correctly-classified-point_1","title":"Feature Importance Correctly Classified point","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#feature-importance-incorrectly-classified-point_1","title":"Feature Importance  Incorrectly Classified point","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#linear-support-vector-machines","title":"Linear Support Vector Machines","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#hyper-paramter-tuning_1","title":"Hyper paramter tuning","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#testing-model-with-best-hyper-parameters","title":"Testing model with best hyper parameters","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#feature-importance-correctly-classified-point_2","title":"Feature Importance Correctly classified point","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#feature-importance-incorrectly-classified-point_2","title":"Feature Importance Incorrectly classified point","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#random-forest-classifier","title":"Random Forest Classifier","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#hyper-paramter-tuning-with-one-hot-encoding","title":"Hyper paramter tuning (With One hot Encoding)","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#testing-model-with-best-hyper-parameters-one-hot-encoding","title":"Testing model with best hyper parameters (One Hot Encoding)","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#feature-importance-correctly-classified-point_3","title":"Feature Importance Correctly Classified point","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#feature-importance-inorrectly-classified-point","title":"Feature Importance Inorrectly Classified point","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#hyper-paramter-tuning-with-response-coding","title":"Hyper paramter tuning (With Response Coding)","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#testing-model-with-best-hyper-parameters-response-coding","title":"Testing model with best hyper parameters (Response Coding)","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#feature-importance-correctly-classified-point_4","title":"Feature Importance Correctly Classified point","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#feature-importance-incorrectly-classified-point_3","title":"Feature Importance Incorrectly Classified point","text":""},{"location":"Applied%20Solutions/Cancer%20Treatment/#future-work","title":"Future Work","text":"<ul> <li>Apply All the models with tf-idf features (Replace CountVectorizer with tfidfVectorizer and run the same cells)</li> <li>Instead of using all the words in the dataset, use only the top 1000 words based of tf-idf values</li> <li>Apply Logistic regression with CountVectorizer Features, including both unigrams and bigrams</li> <li>Try any of the feature engineering techniques discussed in the course to reduce the CV and test log-loss to a value less than 1.0</li> </ul>"},{"location":"Applied%20Solutions/Hydroponics%20Forecasting/","title":"Hydroponics Forecasting","text":""},{"location":"Applied%20Solutions/Hydroponics%20Forecasting/#hydroponics-forecasting","title":"Hydroponics Forecasting","text":"<p>We implemented a machine learning solution for a hydroponics client that aimed to improve crop production and predict demand for different crops. The project involved collecting various data sets, including information on nutrient levels, climate conditions, and other relevant features such as humidity and light levels. This data was then used to train machine learning models that could be used to predict crop yields and identify the most important factors that influence growth.</p> <p></p> <p>To begin, we collected data on a variety of factors that could impact crop growth and production. This included:</p> <ul> <li>Nutrient levels data, such as pH levels, presence of specific minerals and nutrient solution strength,</li> <li>Climate conditions data, such as temperature, humidity, light levels and weather forecast,</li> <li>Data on the type of crop, the stage of growth, and the presence of pests or diseases.</li> <li>Historical sales data</li> </ul> <p></p> <p>Once the data had been collected, we used a combination of machine learning techniques to analyze it and make predictions. This included decision trees, random forests, and neural networks. We used these models to predict crop yields and identify the most important factors that influence growth. We also performed demand analysis for different crops at different times of the year. We used historical sales data and market trends to predict which crops would be in high demand and when.</p> <p></p> <p>One example of how this project helped our client was in the prediction of tomato crop yields. Using data on nutrient levels, climate conditions, and other relevant factors, we trained a machine learning model to predict yields for different tomato varieties. By analyzing the data, we were able to identify the most important factors that influence growth, such as the presence of specific minerals and optimal temperature ranges. Our client was able to use this information to optimize their nutrient solutions and improve the overall growth of their tomato crops. This led to a significant increase in yields, which translated into higher revenue for the client.</p> <p></p> <p>Another example was in the demand analysis of bell pepper crops. We used historical sales data and market trends to predict which varieties of bell pepper would be in high demand and when. By analyzing this data, we were able to identify that demand for bell peppers is higher in the summer and fall months. Our client was able to use this information to schedule their planting and harvesting schedule accordingly, which resulted in them being able to sell more of their crops at higher prices and reduce waste from overproduction.</p> <p></p> <p>In addition to these specific crop examples, the client was able to use the data and predictions generated by the solution to make more informed decisions about their operations as a whole. For example, they were able to optimize their nutrient solutions to improve growth for all of their crops, not just tomatoes and bell peppers. They were also able to schedule their planting and harvesting schedule to coincide with peak demand periods for all their crops. this enabled them to manage inventory more efficiently and reduce costs associated with cold storage and fuel.</p>"},{"location":"Applied%20Solutions/Quora%20Question%20Pairs/","title":"Quora Question Pairs","text":"<pre><code>## Importing css\nfrom IPython.core.display import HTML\ncss = lambda : HTML(open(\"Assets/css/custom.css\", \"r\").read())\ncss()\n</code></pre> <pre><code>## Importing libraries\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport time\nimport spacy\nimport sys\nimport os\nimport re\nimport gc\nimport distance\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nfrom os import path\nfrom PIL import Image\nfrom dataprep.eda import create_report\nfrom subprocess import check_output\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom bs4 import BeautifulSoup\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import normalize\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom tqdm import tqdm\nfrom thefuzz import fuzz\n# Import the Required lib packages for WORD-Cloud generation\n# https://stackoverflow.com/questions/45625434/how-to-install-wordcloud-in-python3-6\nfrom wordcloud import WordCloud, STOPWORDS\n\n# exctract word2vec vectors\n# https://github.com/explosion/spaCy/issues/1721\n# http://landinghub.visualstudio.com/visual-cpp-build-tools\n\n\nimport sqlite3\nfrom sqlalchemy import create_engine # database connection\nimport csv\nimport os\nimport datetime as dt\n\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import normalize\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.manifold import TSNE\nimport seaborn as sns\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom collections import Counter\nfrom scipy.sparse import hstack\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import StratifiedKFold \nfrom collections import Counter, defaultdict\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nimport math\nfrom sklearn.metrics import normalized_mutual_info_score\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import SGDClassifier\nfrom mlxtend.classifier import StackingClassifier\n\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_recall_curve, auc, roc_curve\n\n%matplotlib inline\n</code></pre> <pre><code>## Defining constants\ntitle = \"Quora Question Pairs\"\nraw_data_file_path = f\"data/{title}/01_raw/train.csv\"\nraw_data_dataprep_report_path = f\"data/{title}/08_reporting/raw_data_dataprep.html\"\nnlp_features_train = f\"data/{title}/04_feature/nlp_features_train.csv\"\nfe_without_preprocessing_train_path = f\"data/{title}/04_feature/df_fe_without_preprocessing_train.csv\"\n</code></pre> <pre><code>df = pd.read_csv(raw_data_file_path)\ndisplay(df.head())\nreport = create_report(df, title=title, progress=False)\nreport.save(raw_data_dataprep_report_path)\n</code></pre> id qid1 qid2 question1 question2 is_duplicate 0 0 1 2 What is the step by step guide to invest in sh... What is the step by step guide to invest in sh... 0 1 1 3 4 What is the story of Kohinoor (Koh-i-Noor) Dia... What would happen if the Indian government sto... 0 2 2 5 6 How can I increase the speed of my internet co... How can Internet speed be increased by hacking... 0 3 3 7 8 Why am I mentally very lonely? How can I solve... Find the remainder when [math]23^{24}[/math] i... 0 4 4 9 10 Which one dissolve in water quikly sugar, salt... Which fish would survive in salt water? 0 <pre>\n<code>Report has been saved to data/Quora Question Pairs/08_reporting/raw_data_dataprep.html!\n</code>\n</pre> <p>Generated Report</p> <pre><code>info_df = df.info()\n</code></pre> <pre>\n<code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 404290 entries, 0 to 404289\nData columns (total 6 columns):\n #   Column        Non-Null Count   Dtype \n---  ------        --------------   ----- \n 0   id            404290 non-null  int64 \n 1   qid1          404290 non-null  int64 \n 2   qid2          404290 non-null  int64 \n 3   question1     404289 non-null  object\n 4   question2     404288 non-null  object\n 5   is_duplicate  404290 non-null  int64 \ndtypes: int64(4), object(2)\nmemory usage: 18.5+ MB\n</code>\n</pre> <pre><code>df.describe()\n</code></pre> id qid1 qid2 is_duplicate count 404290.000000 404290.000000 404290.000000 404290.000000 mean 202144.500000 217243.942418 220955.655337 0.369198 std 116708.614503 157751.700002 159903.182629 0.482588 min 0.000000 1.000000 2.000000 0.000000 25% 101072.250000 74437.500000 74727.000000 0.000000 50% 202144.500000 192182.000000 197052.000000 0.000000 75% 303216.750000 346573.500000 354692.500000 1.000000 max 404289.000000 537932.000000 537933.000000 1.000000 <pre><code>qids = pd.Series(df['qid1'].tolist() + df['qid2'].tolist())\nunique_qs = len(np.unique(qids))\nqs_morethan_onetime = np.sum(qids.value_counts() &amp;gt; 1)\nprint ('Total number of  Unique Questions are: {}\\n'.format(unique_qs))\nprint ('Number of unique questions that appear more than one time: {} ({}%)\\n'.format(qs_morethan_onetime,round(qs_morethan_onetime/unique_qs*100,2)))\nprint ('Max number of times a single question is repeated: {}\\n'.format(max(qids.value_counts()))) \n\nq_vals=qids.value_counts()\nq_vals=q_vals.values\n\nto_plot = pd.Series([unique_qs , qs_morethan_onetime], index=  [\"unique_questions\" , \"Repeated Questions\"])\n_ = to_plot.plot(kind='bar', title=\"Plot representing unique and repeated questions\", figsize=(16,8), rot = 0)\n</code></pre> <pre>\n<code>Total number of  Unique Questions are: 537933\n\nNumber of unique questions that appear more than one time: 111780 (20.78%)\n\nMax number of times a single question is repeated: 157\n\n</code>\n</pre> <pre><code>#checking whether there are any repeated pair of questions\npair_duplicates = df[['qid1','qid2','is_duplicate']].groupby(['qid1','qid2']).count().reset_index()\nprint (\"Number of duplicate questions\",(pair_duplicates).shape[0] - df.shape[0])\n</code></pre> <pre>\n<code>Number of duplicate questions 0\n</code>\n</pre> <pre><code>plt.figure(figsize=(16, 8))\nplt.hist(qids.value_counts(), bins=160)\nplt.yscale('log')\nplt.title('Log-Histogram of question appearance counts')\nplt.xlabel('Number of occurences of question')\nplt.ylabel('Number of questions')\nprint ('Maximum number of times a single question is repeated: {}\\n'.format(max(qids.value_counts()))) \n</code></pre> <pre>\n<code>Maximum number of times a single question is repeated: 157\n\n</code>\n</pre> <pre><code>#Checking whether there are any rows with null values\nnan_rows = df[df.isnull().any(1)]\nnan_rows\n</code></pre> id qid1 qid2 question1 question2 is_duplicate 105780 105780 174363 174364 How can I develop android app? NaN 0 201841 201841 303951 174364 How can I create an Android app? NaN 0 363362 363362 493340 493341 NaN My Chinese name is Haichao Yu. What English na... 0 <pre><code># Filling the null values with ' '\ndf = df.fillna('')\nnan_rows = df[df.isnull().any(1)]\nnan_rows\n</code></pre> id qid1 qid2 question1 question2 is_duplicate <p>Let us now construct a few features like:</p> <ul> <li>freq_qid1 = Frequency of qid1's</li> <li>freq_qid2 = Frequency of qid2's </li> <li>q1len = Length of q1</li> <li>q2len = Length of q2</li> <li>q1_n_words = Number of words in Question 1</li> <li>q2_n_words = Number of words in Question 2</li> <li>word_Common = (Number of common unique words in Question 1 and Question 2)</li> <li>word_Total =(Total num of words in Question 1 + Total num of words in Question 2)</li> <li>word_share = (word_common)/(word_Total)</li> <li>freq_q1+freq_q2 = sum total of frequency of qid1 and qid2 </li> <li>freq_q1-freq_q2 = absolute difference of frequency of qid1 and qid2 </li> </ul> <pre><code>if os.path.isfile(fe_without_preprocessing_train_path):\n    df = pd.read_csv(fe_without_preprocessing_train_path, encoding='latin-1')\nelse:\n    df['freq_qid1'] = df.groupby('qid1')['qid1'].transform('count') \n    df['freq_qid2'] = df.groupby('qid2')['qid2'].transform('count')\n    df['q1len'] = df['question1'].str.len() \n    df['q2len'] = df['question2'].str.len()\n    df['q1_n_words'] = df['question1'].apply(lambda row: len(row.split(\" \")))\n    df['q2_n_words'] = df['question2'].apply(lambda row: len(row.split(\" \")))\n\n    def normalized_word_Common(row):\n        w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n        w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n        return 1.0 * len(w1 &amp;amp; w2)\n    df['word_Common'] = df.apply(normalized_word_Common, axis=1)\n\n    def normalized_word_Total(row):\n        w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n        w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n        return 1.0 * (len(w1) + len(w2))\n    df['word_Total'] = df.apply(normalized_word_Total, axis=1)\n\n    def normalized_word_share(row):\n        w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n        w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n        return 1.0 * len(w1 &amp;amp; w2)/(len(w1) + len(w2))\n    df['word_share'] = df.apply(normalized_word_share, axis=1)\n\n    df['freq_q1+q2'] = df['freq_qid1']+df['freq_qid2']\n    df['freq_q1-q2'] = abs(df['freq_qid1']-df['freq_qid2'])\n\n    df.to_csv(fe_without_preprocessing_train_path, index=False)\n\ndf.head()\n</code></pre> id qid1 qid2 question1 question2 is_duplicate freq_qid1 freq_qid2 q1len q2len q1_n_words q2_n_words word_Common word_Total word_share freq_q1+q2 freq_q1-q2 0 0 1 2 What is the step by step guide to invest in sh... What is the step by step guide to invest in sh... 0 1 1 66 57 14 12 10.0 23.0 0.434783 2 0 1 1 3 4 What is the story of Kohinoor (Koh-i-Noor) Dia... What would happen if the Indian government sto... 0 4 1 51 88 8 13 4.0 20.0 0.200000 5 3 2 2 5 6 How can I increase the speed of my internet co... How can Internet speed be increased by hacking... 0 1 1 73 59 14 10 4.0 24.0 0.166667 2 0 3 3 7 8 Why am I mentally very lonely? How can I solve... Find the remainder when [math]23^{24}[/math] i... 0 1 1 50 65 11 9 0.0 19.0 0.000000 2 0 4 4 9 10 Which one dissolve in water quikly sugar, salt... Which fish would survive in salt water? 0 3 1 76 39 13 7 2.0 20.0 0.100000 4 2 <pre><code>print (\"Minimum length of the questions in question1 : \" , min(df['q1_n_words']))\nprint (\"Minimum length of the questions in question2 : \" , min(df['q2_n_words']))\nprint (\"Number of Questions with minimum length [question1] :\", df[df['q1_n_words']== 1].shape[0])\nprint (\"Number of Questions with minimum length [question2] :\", df[df['q2_n_words']== 1].shape[0])\n</code></pre> <pre>\n<code>Minimum length of the questions in question1 :  1\nMinimum length of the questions in question2 :  1\nNumber of Questions with minimum length [question1] : 67\nNumber of Questions with minimum length [question2] : 24\n</code>\n</pre> <pre><code>plt.figure(figsize=(16,8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'word_share', data = df[0:])\n\nplt.subplot(1,2,2)\nsns.distplot(df[df['is_duplicate'] == 1.0]['word_share'][0:] , label = \"1\", color = 'red')\nsns.distplot(df[df['is_duplicate'] == 0.0]['word_share'][0:] , label = \"0\" , color = 'blue' )\nplt.show()\n</code></pre> <ul> <li>The distributions for normalized word_share have some overlap on the far right-hand side, i.e., there are quite a lot of questions with high word similarity</li> <li>The average word share and Common no. of words of qid1 and qid2 is more when they are duplicate(Similar)</li> </ul> <pre><code>plt.figure(figsize=(12, 8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'word_Common', data = df[0:])\n\nplt.subplot(1,2,2)\nsns.distplot(df[df['is_duplicate'] == 1.0]['word_Common'][0:] , label = \"1\", color = 'red')\nsns.distplot(df[df['is_duplicate'] == 0.0]['word_Common'][0:] , label = \"0\" , color = 'blue' )\nplt.show()\n</code></pre> <p> The distributions of the word_Common feature in similar and non-similar questions are highly overlapping </p> <ul> <li>Preprocessing:<ul> <li>Removing html tags </li> <li>Removing Punctuations</li> <li>Performing stemming</li> <li>Removing Stopwords</li> <li>Expanding contractions etc.</li> </ul> </li> </ul> <pre><code># To get the results in 4 decemal points\nSAFE_DIV = 0.0001 \nSTOP_WORDS = stopwords.words(\"english\")\n\ndef preprocess(x):\n    x = str(x).lower()\n    x = x.replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"\u2032\", \"'\").replace(\"\u2019\", \"'\")\\\n                           .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n                           .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n                           .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n                           .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\\\n                           .replace(\"%\", \" percent \").replace(\"\u20b9\", \" rupee \").replace(\"$\", \" dollar \")\\\n                           .replace(\"\u20ac\", \" euro \").replace(\"'ll\", \" will\")\n    x = re.sub(r\"([0-9]+)000000\", r\"\\1m\", x)\n    x = re.sub(r\"([0-9]+)000\", r\"\\1k\", x)\n\n\n    porter = PorterStemmer()\n    pattern = re.compile('\\W')\n\n    if type(x) == type(''):\n        x = re.sub(pattern, ' ', x)\n\n\n    if type(x) == type(''):\n        x = porter.stem(x)\n        example1 = BeautifulSoup(x)\n        x = example1.get_text()\n\n\n    return x\n</code></pre> <pre><code>def get_token_features(q1, q2):\n    token_features = [0.0]*10\n\n    # Converting the Sentence into Tokens: \n    q1_tokens = q1.split()\n    q2_tokens = q2.split()\n\n    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n        return token_features\n    # Get the non-stopwords in Questions\n    q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])\n    q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])\n\n    #Get the stopwords in Questions\n    q1_stops = set([word for word in q1_tokens if word in STOP_WORDS])\n    q2_stops = set([word for word in q2_tokens if word in STOP_WORDS])\n\n    # Get the common non-stopwords from Question pair\n    common_word_count = len(q1_words.intersection(q2_words))\n\n    # Get the common stopwords from Question pair\n    common_stop_count = len(q1_stops.intersection(q2_stops))\n\n    # Get the common Tokens from Question pair\n    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n\n\n    token_features[0] = common_word_count / (min(len(q1_words), len(q2_words)) + SAFE_DIV)\n    token_features[1] = common_word_count / (max(len(q1_words), len(q2_words)) + SAFE_DIV)\n    token_features[2] = common_stop_count / (min(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n    token_features[3] = common_stop_count / (max(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n    token_features[4] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n    token_features[5] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n\n    # Last word of both question is same or not\n    token_features[6] = int(q1_tokens[-1] == q2_tokens[-1])\n\n    # First word of both question is same or not\n    token_features[7] = int(q1_tokens[0] == q2_tokens[0])\n\n    token_features[8] = abs(len(q1_tokens) - len(q2_tokens))\n\n    #Average Token Length of both Questions\n    token_features[9] = (len(q1_tokens) + len(q2_tokens))/2\n    return token_features\n\n# get the Longest Common sub string\n\ndef get_longest_substr_ratio(a, b):\n    strs = list(distance.lcsubstrings(a, b))\n    if len(strs) == 0:\n        return 0\n    else:\n        return len(strs[0]) / (min(len(a), len(b)) + 1)\n\ndef extract_features(df):\n    # preprocessing each question\n    df[\"question1\"] = df[\"question1\"].fillna(\"\").apply(preprocess)\n    df[\"question2\"] = df[\"question2\"].fillna(\"\").apply(preprocess)\n\n    print(\"token features...\")\n\n    # Merging Features with dataset\n\n    token_features = df.apply(lambda x: get_token_features(x[\"question1\"], x[\"question2\"]), axis=1)\n\n    df[\"cwc_min\"]       = list(map(lambda x: x[0], token_features))\n    df[\"cwc_max\"]       = list(map(lambda x: x[1], token_features))\n    df[\"csc_min\"]       = list(map(lambda x: x[2], token_features))\n    df[\"csc_max\"]       = list(map(lambda x: x[3], token_features))\n    df[\"ctc_min\"]       = list(map(lambda x: x[4], token_features))\n    df[\"ctc_max\"]       = list(map(lambda x: x[5], token_features))\n    df[\"last_word_eq\"]  = list(map(lambda x: x[6], token_features))\n    df[\"first_word_eq\"] = list(map(lambda x: x[7], token_features))\n    df[\"abs_len_diff\"]  = list(map(lambda x: x[8], token_features))\n    df[\"mean_len\"]      = list(map(lambda x: x[9], token_features))\n\n    #Computing Fuzzy Features and Merging with Dataset\n\n    # do read this blog: http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n    # https://stackoverflow.com/questions/31806695/when-to-use-which-fuzz-function-to-compare-2-strings\n    # https://github.com/seatgeek/fuzzywuzzy\n    print(\"fuzzy features..\")\n\n    df[\"token_set_ratio\"]       = df.apply(lambda x: fuzz.token_set_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    # The token sort approach involves tokenizing the string in question, sorting the tokens alphabetically, and \n    # then joining them back into a string We then compare the transformed strings with a simple ratio().\n    df[\"token_sort_ratio\"]      = df.apply(lambda x: fuzz.token_sort_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    df[\"fuzz_ratio\"]            = df.apply(lambda x: fuzz.QRatio(x[\"question1\"], x[\"question2\"]), axis=1)\n    df[\"fuzz_partial_ratio\"]    = df.apply(lambda x: fuzz.partial_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    df[\"longest_substr_ratio\"]  = df.apply(lambda x: get_longest_substr_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    return df\n</code></pre> <pre><code>if os.path.isfile(nlp_features_train):\n    df = pd.read_csv(nlp_features_train,encoding='latin-1')\n    df.fillna('')\nelse:\n    print(\"Extracting features for train:\")\n    df = pd.read_csv(raw_data_file_path)\n    df = extract_features(df)\n    df.to_csv(nlp_features_train, index=False)\ndf.head(2)\n</code></pre> id qid1 qid2 question1 question2 is_duplicate cwc_min cwc_max csc_min csc_max ... ctc_max last_word_eq first_word_eq abs_len_diff mean_len token_set_ratio token_sort_ratio fuzz_ratio fuzz_partial_ratio longest_substr_ratio 0 0 1 2 what is the step by step guide to invest in sh... what is the step by step guide to invest in sh... 0 0.999980 0.833319 0.999983 0.999983 ... 0.785709 0.0 1.0 2.0 13.0 100 93 93 100 0.982759 1 1 3 4 what is the story of kohinoor  koh i noor  dia... what would happen if the indian government sto... 0 0.799984 0.399996 0.749981 0.599988 ... 0.466664 0.0 1.0 5.0 12.5 86 63 66 75 0.596154 <p>2 rows \u00d7 21 columns</p> <pre><code>df_duplicate = df[df['is_duplicate'] == 1]\ndfp_nonduplicate = df[df['is_duplicate'] == 0]\n\n# Converting 2d array of q1 and q2 and flatten the array: like {{1,2},{3,4}} to {1,2,3,4}\np = np.dstack([df_duplicate[\"question1\"], df_duplicate[\"question2\"]]).flatten()\nn = np.dstack([dfp_nonduplicate[\"question1\"], dfp_nonduplicate[\"question2\"]]).flatten()\n\nprint (\"Number of data points in class 1 (duplicate pairs) :\",len(p))\nprint (\"Number of data points in class 0 (non duplicate pairs) :\",len(n))\n\npositive_text_file_path = 'data/Quora Question Pairs/04_feature/train_p.txt'\nnegative_text_file_path = 'data/Quora Question Pairs/04_feature/train_n.txt'\n\n#Saving the np array into a text file\nnp.savetxt(positive_text_file_path, p, delimiter=' ', fmt='%s')\nnp.savetxt(negative_text_file_path, n, delimiter=' ', fmt='%s')\n</code></pre> <pre>\n<code>Number of data points in class 1 (duplicate pairs) : 298526\nNumber of data points in class 0 (non duplicate pairs) : 510054\n</code>\n</pre> <pre><code># reading the text files and removing the Stop Words:\nd = path.dirname('.')\n\ntextp_w = open(path.join(d, positive_text_file_path)).read()\ntextn_w = open(path.join(d, negative_text_file_path)).read()\nstopwords = set(STOPWORDS)\nstopwords.add(\"said\")\nstopwords.add(\"br\")\nstopwords.add(\" \")\nstopwords.remove(\"not\")\n\nstopwords.remove(\"no\")\n#stopwords.remove(\"good\")\n#stopwords.remove(\"love\")\nstopwords.remove(\"like\")\n#stopwords.remove(\"best\")\n#stopwords.remove(\"!\")\nprint (\"Total number of words in duplicate pair questions :\",len(textp_w))\nprint (\"Total number of words in non duplicate pair questions :\",len(textn_w))\n</code></pre> <pre>\n<code>Total number of words in duplicate pair questions : 16109886\nTotal number of words in non duplicate pair questions : 33193130\n</code>\n</pre> <p>Word Clouds generated from  duplicate pair question's text</p> <pre><code>wc = WordCloud(background_color=\"white\", max_words=len(textp_w), stopwords=stopwords)\nwc.generate(textp_w)\nprint (\"Word Cloud for Duplicate Question pairs\")\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()\n</code></pre> <pre>\n<code>Word Cloud for Duplicate Question pairs\n</code>\n</pre> <p>Word Clouds generated from non duplicate pair question's text</p> <pre><code>wc = WordCloud(background_color=\"white\", max_words=len(textn_w),stopwords=stopwords)\n# generate word cloud\nwc.generate(textn_w)\nprint (\"Word Cloud for non-Duplicate Question pairs:\")\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()\n</code></pre> <pre>\n<code>Word Cloud for non-Duplicate Question pairs:\n</code>\n</pre> <pre><code>n = df.shape[0]\nsns.pairplot(df[['ctc_min', 'cwc_min', 'csc_min', 'token_sort_ratio', 'is_duplicate']][0:n], hue='is_duplicate', vars=['ctc_min', 'cwc_min', 'csc_min', 'token_sort_ratio'])\nplt.show()\n</code></pre> <pre><code># Distribution of the token_sort_ratio\nplt.figure(figsize=(10, 8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'token_sort_ratio', data = df[0:] , )\n\nplt.subplot(1,2,2)\nsns.distplot(df[df['is_duplicate'] == 1.0]['token_sort_ratio'][0:] , label = \"1\", color = 'red')\nsns.distplot(df[df['is_duplicate'] == 0.0]['token_sort_ratio'][0:] , label = \"0\" , color = 'blue' )\nplt.show()\n</code></pre> <pre><code>plt.figure(figsize=(10, 8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'fuzz_ratio', data = df[0:] , )\n\nplt.subplot(1,2,2)\nsns.distplot(df[df['is_duplicate'] == 1.0]['fuzz_ratio'][0:] , label = \"1\", color = 'red')\nsns.distplot(df[df['is_duplicate'] == 0.0]['fuzz_ratio'][0:] , label = \"0\" , color = 'blue' )\nplt.show()\n</code></pre> <pre><code># Using TSNE for Dimentionality reduction for 15 Features(Generated after cleaning the data) to 3 dimention\nfrom sklearn.preprocessing import MinMaxScaler\n\ndfp_subsampled = df[0:5000]\nX = MinMaxScaler().fit_transform(dfp_subsampled[['cwc_min', 'cwc_max', 'csc_min', 'csc_max' , 'ctc_min' , 'ctc_max' , 'last_word_eq', 'first_word_eq' , 'abs_len_diff' , 'mean_len' , 'token_set_ratio' , 'token_sort_ratio' ,  'fuzz_ratio' , 'fuzz_partial_ratio' , 'longest_substr_ratio']])\ny = dfp_subsampled['is_duplicate'].values\n</code></pre> <pre><code>tsne2d = TSNE(\n    n_components=2,\n    init='random', # pca\n    random_state=101,\n    method='barnes_hut',\n    n_iter=1000,\n    verbose=2,\n    angle=0.5\n).fit_transform(X)\n</code></pre> <pre>\n<code>[t-SNE] Computing 91 nearest neighbors...\n[t-SNE] Indexed 5000 samples in 0.005s...\n[t-SNE] Computed neighbors for 5000 samples in 0.289s...\n[t-SNE] Computed conditional probabilities for sample 1000 / 5000\n[t-SNE] Computed conditional probabilities for sample 2000 / 5000\n[t-SNE] Computed conditional probabilities for sample 3000 / 5000\n[t-SNE] Computed conditional probabilities for sample 4000 / 5000\n[t-SNE] Computed conditional probabilities for sample 5000 / 5000\n[t-SNE] Mean sigma: 0.116557\n[t-SNE] Computed conditional probabilities in 0.130s\n[t-SNE] Iteration 50: error = 85.3983765, gradient norm = 0.0928323 (50 iterations in 0.650s)\n[t-SNE] Iteration 100: error = 70.7441177, gradient norm = 0.0130134 (50 iterations in 0.498s)\n[t-SNE] Iteration 150: error = 68.7045746, gradient norm = 0.0077004 (50 iterations in 0.486s)\n[t-SNE] Iteration 200: error = 67.7494354, gradient norm = 0.0055012 (50 iterations in 0.465s)\n[t-SNE] Iteration 250: error = 67.2252350, gradient norm = 0.0041935 (50 iterations in 0.465s)\n[t-SNE] KL divergence after 250 iterations with early exaggeration: 67.225235\n[t-SNE] Iteration 300: error = 1.9825330, gradient norm = 0.0156138 (50 iterations in 0.454s)\n[t-SNE] Iteration 350: error = 1.5351998, gradient norm = 0.0144627 (50 iterations in 0.445s)\n[t-SNE] Iteration 400: error = 1.3278431, gradient norm = 0.0128098 (50 iterations in 0.440s)\n[t-SNE] Iteration 450: error = 1.2114807, gradient norm = 0.0114405 (50 iterations in 0.449s)\n[t-SNE] Iteration 500: error = 1.1379409, gradient norm = 0.0103406 (50 iterations in 0.448s)\n[t-SNE] Iteration 550: error = 1.0874577, gradient norm = 0.0096972 (50 iterations in 0.444s)\n[t-SNE] Iteration 600: error = 1.0505884, gradient norm = 0.0089717 (50 iterations in 0.448s)\n[t-SNE] Iteration 650: error = 1.0232079, gradient norm = 0.0082849 (50 iterations in 0.448s)\n[t-SNE] Iteration 700: error = 1.0022360, gradient norm = 0.0076192 (50 iterations in 0.450s)\n[t-SNE] Iteration 750: error = 0.9860880, gradient norm = 0.0069277 (50 iterations in 0.447s)\n[t-SNE] Iteration 800: error = 0.9734683, gradient norm = 0.0063558 (50 iterations in 0.453s)\n[t-SNE] Iteration 850: error = 0.9637550, gradient norm = 0.0053249 (50 iterations in 0.455s)\n[t-SNE] Iteration 900: error = 0.9566751, gradient norm = 0.0043437 (50 iterations in 0.457s)\n[t-SNE] Iteration 950: error = 0.9512537, gradient norm = 0.0037357 (50 iterations in 0.458s)\n[t-SNE] Iteration 1000: error = 0.9468948, gradient norm = 0.0032665 (50 iterations in 0.466s)\n[t-SNE] KL divergence after 1000 iterations: 0.946895\n</code>\n</pre> <pre><code>df = pd.DataFrame({'x':tsne2d[:,0], 'y':tsne2d[:,1] ,'label':y})\n\n# draw the plot in appropriate place in the grid\nsns.lmplot(data=df, x='x', y='y', hue='label', fit_reg=False, size=8,palette=\"Set1\",markers=['s','o'])\nplt.title(\"perplexity : {} and max_iter : {}\".format(30, 1000))\nplt.show()\n</code></pre> <pre><code>from sklearn.manifold import TSNE\ntsne3d = TSNE(\n    n_components=3,\n    init='random', # pca\n    random_state=101,\n    method='barnes_hut',\n    n_iter=1000,\n    verbose=2,\n    angle=0.5\n).fit_transform(X)\n</code></pre> <pre>\n<code>[t-SNE] Computing 91 nearest neighbors...\n[t-SNE] Indexed 5000 samples in 0.003s...\n[t-SNE] Computed neighbors for 5000 samples in 0.262s...\n[t-SNE] Computed conditional probabilities for sample 1000 / 5000\n[t-SNE] Computed conditional probabilities for sample 2000 / 5000\n[t-SNE] Computed conditional probabilities for sample 3000 / 5000\n[t-SNE] Computed conditional probabilities for sample 4000 / 5000\n[t-SNE] Computed conditional probabilities for sample 5000 / 5000\n[t-SNE] Mean sigma: 0.116557\n[t-SNE] Computed conditional probabilities in 0.114s\n[t-SNE] Iteration 50: error = 86.3213348, gradient norm = 0.0852814 (50 iterations in 1.047s)\n[t-SNE] Iteration 100: error = 69.4754181, gradient norm = 0.0074687 (50 iterations in 0.825s)\n[t-SNE] Iteration 150: error = 67.7562256, gradient norm = 0.0038234 (50 iterations in 0.756s)\n[t-SNE] Iteration 200: error = 67.1388702, gradient norm = 0.0025480 (50 iterations in 0.775s)\n[t-SNE] Iteration 250: error = 66.7948914, gradient norm = 0.0019368 (50 iterations in 0.793s)\n[t-SNE] KL divergence after 250 iterations with early exaggeration: 66.794891\n[t-SNE] Iteration 300: error = 1.7247280, gradient norm = 0.0087369 (50 iterations in 0.845s)\n[t-SNE] Iteration 350: error = 1.3098295, gradient norm = 0.0055429 (50 iterations in 1.030s)\n[t-SNE] Iteration 400: error = 1.1212687, gradient norm = 0.0042167 (50 iterations in 1.072s)\n[t-SNE] Iteration 450: error = 1.0208713, gradient norm = 0.0033299 (50 iterations in 1.073s)\n[t-SNE] Iteration 500: error = 0.9596957, gradient norm = 0.0027486 (50 iterations in 1.055s)\n[t-SNE] Iteration 550: error = 0.9220737, gradient norm = 0.0021657 (50 iterations in 1.031s)\n[t-SNE] Iteration 600: error = 0.8991240, gradient norm = 0.0016105 (50 iterations in 1.014s)\n[t-SNE] Iteration 650: error = 0.8837481, gradient norm = 0.0012414 (50 iterations in 1.142s)\n[t-SNE] Iteration 700: error = 0.8728597, gradient norm = 0.0010010 (50 iterations in 1.102s)\n[t-SNE] Iteration 750: error = 0.8647388, gradient norm = 0.0008529 (50 iterations in 1.025s)\n[t-SNE] Iteration 800: error = 0.8587740, gradient norm = 0.0006868 (50 iterations in 1.113s)\n[t-SNE] Iteration 850: error = 0.8542212, gradient norm = 0.0005571 (50 iterations in 1.122s)\n[t-SNE] Iteration 900: error = 0.8502351, gradient norm = 0.0004771 (50 iterations in 1.111s)\n[t-SNE] Iteration 950: error = 0.8468751, gradient norm = 0.0004645 (50 iterations in 1.120s)\n[t-SNE] Iteration 1000: error = 0.8434035, gradient norm = 0.0004388 (50 iterations in 1.143s)\n[t-SNE] KL divergence after 1000 iterations: 0.843403\n</code>\n</pre> <pre><code>trace1 = go.Scatter3d(\n    x=tsne3d[:,0],\n    y=tsne3d[:,1],\n    z=tsne3d[:,2],\n    mode='markers',\n    marker=dict(\n        sizemode='diameter',\n        color = y,\n        colorscale = 'Portland',\n        colorbar = dict(title = 'duplicate'),\n        line=dict(color='rgb(255, 255, 255)'),\n        opacity=0.75\n    )\n)\n\ndata=[trace1]\nlayout=dict(height=800, width=800, title='3d embedding with engineered features')\nfig=dict(data=data, layout=layout)\npy.iplot(fig, filename='3DBubble')\n</code></pre> <pre><code># avoid decoding problems\ndf = pd.read_csv(raw_data_file_path)\n# encode questions to unicode https://stackoverflow.com/a/6812069\ndf['question1'] = df['question1'].apply(lambda x: str(x))\ndf['question2'] = df['question2'].apply(lambda x: str(x))\n</code></pre> <pre><code>df.head()\n</code></pre> id qid1 qid2 question1 question2 is_duplicate 0 0 1 2 What is the step by step guide to invest in sh... What is the step by step guide to invest in sh... 0 1 1 3 4 What is the story of Kohinoor (Koh-i-Noor) Dia... What would happen if the Indian government sto... 0 2 2 5 6 How can I increase the speed of my internet co... How can Internet speed be increased by hacking... 0 3 3 7 8 Why am I mentally very lonely? How can I solve... Find the remainder when [math]23^{24}[/math] i... 0 4 4 9 10 Which one dissolve in water quikly sugar, salt... Which fish would survive in salt water? 0 <pre><code>from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n# merge texts\nquestions = list(df['question1']) + list(df['question2'])\n\ntfidf = TfidfVectorizer(lowercase=False, )\ntfidf.fit_transform(questions)\n\n# dict key:word and value:tf-idf score\nword2tfidf = dict(zip(tfidf.get_feature_names_out(), tfidf.idf_))\n</code></pre> <ul> <li>After we find TF-IDF scores, we convert each question to a weighted average of word2vec vectors by these scores.</li> <li>here we use a pre-trained GLOVE model which comes free with \"Spacy\".  https://spacy.io/usage/vectors-similarity</li> <li>It is trained on Wikipedia and therefore, it is stronger in terms of word semantics. </li> </ul> <pre><code>!python -m spacy download en_core_web_lg\n!python -m spacy download en_core_web_sm\n!python -m spacy download en_core_web_md\n!python -m spacy download en\n</code></pre> <pre>\n<code>Collecting en-core-web-lg==3.4.1\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.4.1/en_core_web_lg-3.4.1-py3-none-any.whl (587.7 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 587.7/587.7 MB 1.5 MB/s eta 0:00:0000:0100:04\nRequirement already satisfied: spacy&lt;3.5.0,&gt;=3.4.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from en-core-web-lg==3.4.1) (3.4.4)\nRequirement already satisfied: tqdm&lt;5.0.0,&gt;=4.38.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-lg==3.4.1) (4.61.0)\nRequirement already satisfied: pathy&gt;=0.3.5 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-lg==3.4.1) (0.10.1)\nRequirement already satisfied: requests&lt;3.0.0,&gt;=2.13.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-lg==3.4.1) (2.26.0)\nRequirement already satisfied: setuptools in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-lg==3.4.1) (65.5.0)\nRequirement already satisfied: wasabi&lt;1.1.0,&gt;=0.9.1 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-lg==3.4.1) (0.10.1)\nRequirement already satisfied: numpy&gt;=1.15.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-lg==3.4.1) (1.21.6)\nRequirement already satisfied: spacy-legacy&lt;3.1.0,&gt;=3.0.10 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-lg==3.4.1) (3.0.11)\nRequirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-lg==3.4.1) (2.0.7)\nRequirement already satisfied: packaging&gt;=20.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-lg==3.4.1) (20.9)\nRequirement already satisfied: typer&lt;0.8.0,&gt;=0.3.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-lg==3.4.1) (0.6.1)\nRequirement already satisfied: spacy-loggers&lt;2.0.0,&gt;=1.0.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-lg==3.4.1) (1.0.4)\nRequirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-lg==3.4.1) (3.0.8)\nRequirement already satisfied: catalogue&lt;2.1.0,&gt;=2.0.6 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-lg==3.4.1) (2.0.8)\nRequirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-lg==3.4.1) (1.0.9)\nRequirement already satisfied: langcodes&lt;4.0.0,&gt;=3.2.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-lg==3.4.1) (3.3.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,&lt;1.11.0,&gt;=1.7.4 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-lg==3.4.1) (1.10.2)\nRequirement already satisfied: srsly&lt;3.0.0,&gt;=2.4.3 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-lg==3.4.1) (2.4.5)\nRequirement already satisfied: smart-open&lt;7.0.0,&gt;=5.2.1 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-lg==3.4.1) (6.3.0)\nRequirement already satisfied: thinc&lt;8.2.0,&gt;=8.1.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-lg==3.4.1) (8.1.6)\nRequirement already satisfied: jinja2 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-lg==3.4.1) (3.0.3)\nRequirement already satisfied: pyparsing&gt;=2.0.2 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from packaging&gt;=20.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-lg==3.4.1) (3.0.9)\nRequirement already satisfied: typing-extensions&gt;=4.1.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,&lt;1.11.0,&gt;=1.7.4-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-lg==3.4.1) (4.3.0)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-lg==3.4.1) (2022.9.14)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-lg==3.4.1) (3.4)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-lg==3.4.1) (2.0.12)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-lg==3.4.1) (1.26.12)\nRequirement already satisfied: blis&lt;0.8.0,&gt;=0.7.8 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from thinc&lt;8.2.0,&gt;=8.1.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-lg==3.4.1) (0.7.9)\nRequirement already satisfied: confection&lt;1.0.0,&gt;=0.0.1 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from thinc&lt;8.2.0,&gt;=8.1.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-lg==3.4.1) (0.0.3)\nRequirement already satisfied: click&lt;9.0.0,&gt;=7.1.1 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from typer&lt;0.8.0,&gt;=0.3.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-lg==3.4.1) (8.1.3)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from jinja2-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-lg==3.4.1) (2.1.1)\nInstalling collected packages: en-core-web-lg\nSuccessfully installed en-core-web-lg-3.4.1\n\u2714 Download and installation successful\nYou can now load the package via spacy.load('en_core_web_lg')\nCollecting en-core-web-sm==3.4.1\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 12.8/12.8 MB 3.3 MB/s eta 0:00:0000:0100:01m\nRequirement already satisfied: spacy&lt;3.5.0,&gt;=3.4.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from en-core-web-sm==3.4.1) (3.4.4)\nRequirement already satisfied: setuptools in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (65.5.0)\nRequirement already satisfied: spacy-loggers&lt;2.0.0,&gt;=1.0.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (1.0.4)\nRequirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (3.0.8)\nRequirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (2.0.7)\nRequirement already satisfied: packaging&gt;=20.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (20.9)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,&lt;1.11.0,&gt;=1.7.4 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (1.10.2)\nRequirement already satisfied: typer&lt;0.8.0,&gt;=0.3.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (0.6.1)\nRequirement already satisfied: smart-open&lt;7.0.0,&gt;=5.2.1 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (6.3.0)\nRequirement already satisfied: spacy-legacy&lt;3.1.0,&gt;=3.0.10 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (3.0.11)\nRequirement already satisfied: requests&lt;3.0.0,&gt;=2.13.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (2.26.0)\nRequirement already satisfied: srsly&lt;3.0.0,&gt;=2.4.3 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (2.4.5)\nRequirement already satisfied: thinc&lt;8.2.0,&gt;=8.1.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (8.1.6)\nRequirement already satisfied: tqdm&lt;5.0.0,&gt;=4.38.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (4.61.0)\nRequirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (1.0.9)\nRequirement already satisfied: pathy&gt;=0.3.5 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (0.10.1)\nRequirement already satisfied: catalogue&lt;2.1.0,&gt;=2.0.6 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (2.0.8)\nRequirement already satisfied: numpy&gt;=1.15.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (1.21.6)\nRequirement already satisfied: langcodes&lt;4.0.0,&gt;=3.2.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (3.3.0)\nRequirement already satisfied: wasabi&lt;1.1.0,&gt;=0.9.1 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (0.10.1)\nRequirement already satisfied: jinja2 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (3.0.3)\nRequirement already satisfied: pyparsing&gt;=2.0.2 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from packaging&gt;=20.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (3.0.9)\n</code>\n</pre> <pre>\n<code>Requirement already satisfied: typing-extensions&gt;=4.1.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,&lt;1.11.0,&gt;=1.7.4-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (4.3.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (3.4)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (1.26.12)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (2022.9.14)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (2.0.12)\nRequirement already satisfied: confection&lt;1.0.0,&gt;=0.0.1 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from thinc&lt;8.2.0,&gt;=8.1.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (0.0.3)\nRequirement already satisfied: blis&lt;0.8.0,&gt;=0.7.8 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from thinc&lt;8.2.0,&gt;=8.1.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (0.7.9)\nRequirement already satisfied: click&lt;9.0.0,&gt;=7.1.1 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from typer&lt;0.8.0,&gt;=0.3.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (8.1.3)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from jinja2-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (2.1.1)\nInstalling collected packages: en-core-web-sm\nSuccessfully installed en-core-web-sm-3.4.1\n\u2714 Download and installation successful\nYou can now load the package via spacy.load('en_core_web_sm')\nCollecting en-core-web-md==3.4.1\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.4.1/en_core_web_md-3.4.1-py3-none-any.whl (42.8 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 42.8/42.8 MB 4.1 MB/s eta 0:00:0000:0100:01m\nRequirement already satisfied: spacy&lt;3.5.0,&gt;=3.4.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from en-core-web-md==3.4.1) (3.4.4)\nRequirement already satisfied: srsly&lt;3.0.0,&gt;=2.4.3 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-md==3.4.1) (2.4.5)\nRequirement already satisfied: numpy&gt;=1.15.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-md==3.4.1) (1.21.6)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,&lt;1.11.0,&gt;=1.7.4 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-md==3.4.1) (1.10.2)\nRequirement already satisfied: spacy-legacy&lt;3.1.0,&gt;=3.0.10 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-md==3.4.1) (3.0.11)\nRequirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-md==3.4.1) (1.0.9)\nRequirement already satisfied: tqdm&lt;5.0.0,&gt;=4.38.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-md==3.4.1) (4.61.0)\nRequirement already satisfied: pathy&gt;=0.3.5 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-md==3.4.1) (0.10.1)\nRequirement already satisfied: jinja2 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-md==3.4.1) (3.0.3)\nRequirement already satisfied: catalogue&lt;2.1.0,&gt;=2.0.6 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-md==3.4.1) (2.0.8)\nRequirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-md==3.4.1) (2.0.7)\nRequirement already satisfied: smart-open&lt;7.0.0,&gt;=5.2.1 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-md==3.4.1) (6.3.0)\nRequirement already satisfied: typer&lt;0.8.0,&gt;=0.3.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-md==3.4.1) (0.6.1)\nRequirement already satisfied: langcodes&lt;4.0.0,&gt;=3.2.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-md==3.4.1) (3.3.0)\nRequirement already satisfied: spacy-loggers&lt;2.0.0,&gt;=1.0.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-md==3.4.1) (1.0.4)\nRequirement already satisfied: packaging&gt;=20.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-md==3.4.1) (20.9)\nRequirement already satisfied: requests&lt;3.0.0,&gt;=2.13.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-md==3.4.1) (2.26.0)\nRequirement already satisfied: thinc&lt;8.2.0,&gt;=8.1.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-md==3.4.1) (8.1.6)\nRequirement already satisfied: setuptools in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-md==3.4.1) (65.5.0)\nRequirement already satisfied: wasabi&lt;1.1.0,&gt;=0.9.1 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-md==3.4.1) (0.10.1)\nRequirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-md==3.4.1) (3.0.8)\nRequirement already satisfied: pyparsing&gt;=2.0.2 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from packaging&gt;=20.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-md==3.4.1) (3.0.9)\nRequirement already satisfied: typing-extensions&gt;=4.1.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,&lt;1.11.0,&gt;=1.7.4-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-md==3.4.1) (4.3.0)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-md==3.4.1) (2022.9.14)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-md==3.4.1) (1.26.12)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-md==3.4.1) (3.4)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-md==3.4.1) (2.0.12)\nRequirement already satisfied: confection&lt;1.0.0,&gt;=0.0.1 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from thinc&lt;8.2.0,&gt;=8.1.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-md==3.4.1) (0.0.3)\nRequirement already satisfied: blis&lt;0.8.0,&gt;=0.7.8 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from thinc&lt;8.2.0,&gt;=8.1.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-md==3.4.1) (0.7.9)\nRequirement already satisfied: click&lt;9.0.0,&gt;=7.1.1 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from typer&lt;0.8.0,&gt;=0.3.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-md==3.4.1) (8.1.3)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from jinja2-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-md==3.4.1) (2.1.1)\n</code>\n</pre> <pre>\n<code>Installing collected packages: en-core-web-md\nSuccessfully installed en-core-web-md-3.4.1\n\u2714 Download and installation successful\nYou can now load the package via spacy.load('en_core_web_md')\n\u26a0 As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\nfull pipeline package name 'en_core_web_sm' instead.\nCollecting en-core-web-sm==3.4.1\n  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\nRequirement already satisfied: spacy&lt;3.5.0,&gt;=3.4.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from en-core-web-sm==3.4.1) (3.4.4)\nRequirement already satisfied: langcodes&lt;4.0.0,&gt;=3.2.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (3.3.0)\nRequirement already satisfied: catalogue&lt;2.1.0,&gt;=2.0.6 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (2.0.8)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,&lt;1.11.0,&gt;=1.7.4 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (1.10.2)\nRequirement already satisfied: srsly&lt;3.0.0,&gt;=2.4.3 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (2.4.5)\nRequirement already satisfied: thinc&lt;8.2.0,&gt;=8.1.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (8.1.6)\nRequirement already satisfied: wasabi&lt;1.1.0,&gt;=0.9.1 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (0.10.1)\nRequirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (2.0.7)\nRequirement already satisfied: smart-open&lt;7.0.0,&gt;=5.2.1 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (6.3.0)\nRequirement already satisfied: jinja2 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (3.0.3)\nRequirement already satisfied: tqdm&lt;5.0.0,&gt;=4.38.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (4.61.0)\nRequirement already satisfied: requests&lt;3.0.0,&gt;=2.13.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (2.26.0)\nRequirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (3.0.8)\nRequirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (1.0.9)\nRequirement already satisfied: packaging&gt;=20.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (20.9)\nRequirement already satisfied: spacy-loggers&lt;2.0.0,&gt;=1.0.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (1.0.4)\nRequirement already satisfied: pathy&gt;=0.3.5 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (0.10.1)\nRequirement already satisfied: spacy-legacy&lt;3.1.0,&gt;=3.0.10 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (3.0.11)\nRequirement already satisfied: setuptools in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (65.5.0)\nRequirement already satisfied: numpy&gt;=1.15.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (1.21.6)\nRequirement already satisfied: typer&lt;0.8.0,&gt;=0.3.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (0.6.1)\nRequirement already satisfied: pyparsing&gt;=2.0.2 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from packaging&gt;=20.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (3.0.9)\nRequirement already satisfied: typing-extensions&gt;=4.1.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,&lt;1.11.0,&gt;=1.7.4-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (4.3.0)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (2.0.12)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (1.26.12)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (3.4)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (2022.9.14)\nRequirement already satisfied: confection&lt;1.0.0,&gt;=0.0.1 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from thinc&lt;8.2.0,&gt;=8.1.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (0.0.3)\nRequirement already satisfied: blis&lt;0.8.0,&gt;=0.7.8 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from thinc&lt;8.2.0,&gt;=8.1.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (0.7.9)\nRequirement already satisfied: click&lt;9.0.0,&gt;=7.1.1 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from typer&lt;0.8.0,&gt;=0.3.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (8.1.3)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /Users/harshmaheshwari/miniconda3/envs/jupyter/lib/python3.8/site-packages (from jinja2-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (2.1.1)\n\u2714 Download and installation successful\nYou can now load the package via spacy.load('en_core_web_sm')\n</code>\n</pre> <pre><code># en_vectors_web_lg, which includes over 1 million unique vectors.\nnlp = spacy.load('en_core_web_lg')\n# vecs1 = []\n# # https://github.com/noamraph/tqdm\n# # tqdm is used to print the progrss bar\n# for qu1 in tqdm(list(df['question1'])):\n#     doc1 = nlp(qu1) \n#     # 384 is the number of dimensions of vectors \n#     mean_vec1 = np.zeros([len(doc1), len(doc1[0].vector)])\n#     for word1 in doc1:\n#         # word2vec\n#         vec1 = word1.vector\n#         # fetch df score\n#         try:\n#             idf = word2tfidf[str(word1)]\n#         except:\n#             idf = 0\n#         # compute final vec\n#         mean_vec1 += vec1 * idf\n#     mean_vec1 = mean_vec1.mean(axis=0)\n#     vecs1.append(mean_vec1)\n# df['q1_feats_m'] = list(vecs1)\nx=nlp('man')\nlen(x.vector)\n</code></pre> <pre>\n<code>300</code>\n</pre> <pre><code>vecs2 = []\nfor qu2 in tqdm(list(df['question2'])):\n    doc2 = nlp(qu2) \n    mean_vec1 = np.zeros([len(doc1), len(doc2[0].vector)])\n    for word2 in doc2:\n        # word2vec\n        vec2 = word2.vector\n        # fetch df score\n        try:\n            idf = word2tfidf[str(word2)]\n        except:\n            #print word\n            idf = 0\n        # compute final vec\n        mean_vec2 += vec2 * idf\n    mean_vec2 = mean_vec2.mean(axis=0)\n    vecs2.append(mean_vec2)\ndf['q2_feats_m'] = list(vecs2)\n</code></pre> <pre>\n<code>  0%|                                                                                                                                                                                                                              | 0/404290 [00:00&lt;?, ?it/s]\n</code>\n</pre> <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n/var/folders/tz/6dwwl4_92xv0hspfrgj59nch0000gp/T/ipykernel_2083/1614740358.py in &lt;cell line: 2&gt;()\n      2 for qu2 in tqdm(list(df['question2'])):\n      3     doc2 = nlp(qu2)\n----&gt; 4     mean_vec1 = np.zeros([len(doc1), len(doc2[0].vector)])\n      5     for word2 in doc2:\n      6         # word2vec\n\nNameError: name 'doc1' is not defined</pre> <pre><code>#prepro_features_train.csv (Simple Preprocessing Feartures)\n#nlp_features_train.csv (NLP Features)\nif os.path.isfile('nlp_features_train.csv'):\n    dfnlp = pd.read_csv(\"nlp_features_train.csv\",encoding='latin-1')\nelse:\n    print(\"download nlp_features_train.csv from drive or run previous notebook\")\n\nif os.path.isfile('df_fe_without_preprocessing_train.csv'):\n    dfppro = pd.read_csv(\"df_fe_without_preprocessing_train.csv\",encoding='latin-1')\nelse:\n    print(\"download df_fe_without_preprocessing_train.csv from drive or run previous notebook\")\n</code></pre> <pre><code>df1 = dfnlp.drop(['qid1','qid2','question1','question2'],axis=1)\ndf2 = dfppro.drop(['qid1','qid2','question1','question2','is_duplicate'],axis=1)\ndf3 = df.drop(['qid1','qid2','question1','question2','is_duplicate'],axis=1)\ndf3_q1 = pd.DataFrame(df3.q1_feats_m.values.tolist(), index= df3.index)\ndf3_q2 = pd.DataFrame(df3.q2_feats_m.values.tolist(), index= df3.index)\n</code></pre> <pre><code># dataframe of nlp features\ndf1.head()\n</code></pre> <pre><code># data before preprocessing \ndf2.head()\n</code></pre> <pre><code># Questions 1 tfidf weighted word2vec\ndf3_q1.head()\n</code></pre> <pre><code># Questions 2 tfidf weighted word2vec\ndf3_q2.head()\n</code></pre> <pre><code>print(\"Number of features in nlp dataframe :\", df1.shape[1])\nprint(\"Number of features in preprocessed dataframe :\", df2.shape[1])\nprint(\"Number of features in question1 w2v  dataframe :\", df3_q1.shape[1])\nprint(\"Number of features in question2 w2v  dataframe :\", df3_q2.shape[1])\nprint(\"Number of features in final dataframe  :\", df1.shape[1]+df2.shape[1]+df3_q1.shape[1]+df3_q2.shape[1])\n</code></pre> <pre><code># storing the final features to csv file\nif not os.path.isfile('final_features.csv'):\n    df3_q1['id']=df1['id']\n    df3_q2['id']=df1['id']\n    df1  = df1.merge(df2, on='id',how='left')\n    df2  = df3_q1.merge(df3_q2, on='id',how='left')\n    result  = df1.merge(df2, on='id',how='left')\n    result.to_csv('final_features.csv')\n</code></pre> <pre><code>#Creating db file from csv\nif not os.path.isfile('train.db'):\n    disk_engine = create_engine('sqlite:///train.db')\n    start = dt.datetime.now()\n    chunksize = 180000\n    j = 0\n    index_start = 1\n    for df in pd.read_csv('final_features.csv', names=['Unnamed: 0','id','is_duplicate','cwc_min','cwc_max','csc_min','csc_max','ctc_min','ctc_max','last_word_eq','first_word_eq','abs_len_diff','mean_len','token_set_ratio','token_sort_ratio','fuzz_ratio','fuzz_partial_ratio','longest_substr_ratio','freq_qid1','freq_qid2','q1len','q2len','q1_n_words','q2_n_words','word_Common','word_Total','word_share','freq_q1+q2','freq_q1-q2','0_x','1_x','2_x','3_x','4_x','5_x','6_x','7_x','8_x','9_x','10_x','11_x','12_x','13_x','14_x','15_x','16_x','17_x','18_x','19_x','20_x','21_x','22_x','23_x','24_x','25_x','26_x','27_x','28_x','29_x','30_x','31_x','32_x','33_x','34_x','35_x','36_x','37_x','38_x','39_x','40_x','41_x','42_x','43_x','44_x','45_x','46_x','47_x','48_x','49_x','50_x','51_x','52_x','53_x','54_x','55_x','56_x','57_x','58_x','59_x','60_x','61_x','62_x','63_x','64_x','65_x','66_x','67_x','68_x','69_x','70_x','71_x','72_x','73_x','74_x','75_x','76_x','77_x','78_x','79_x','80_x','81_x','82_x','83_x','84_x','85_x','86_x','87_x','88_x','89_x','90_x','91_x','92_x','93_x','94_x','95_x','96_x','97_x','98_x','99_x','100_x','101_x','102_x','103_x','104_x','105_x','106_x','107_x','108_x','109_x','110_x','111_x','112_x','113_x','114_x','115_x','116_x','117_x','118_x','119_x','120_x','121_x','122_x','123_x','124_x','125_x','126_x','127_x','128_x','129_x','130_x','131_x','132_x','133_x','134_x','135_x','136_x','137_x','138_x','139_x','140_x','141_x','142_x','143_x','144_x','145_x','146_x','147_x','148_x','149_x','150_x','151_x','152_x','153_x','154_x','155_x','156_x','157_x','158_x','159_x','160_x','161_x','162_x','163_x','164_x','165_x','166_x','167_x','168_x','169_x','170_x','171_x','172_x','173_x','174_x','175_x','176_x','177_x','178_x','179_x','180_x','181_x','182_x','183_x','184_x','185_x','186_x','187_x','188_x','189_x','190_x','191_x','192_x','193_x','194_x','195_x','196_x','197_x','198_x','199_x','200_x','201_x','202_x','203_x','204_x','205_x','206_x','207_x','208_x','209_x','210_x','211_x','212_x','213_x','214_x','215_x','216_x','217_x','218_x','219_x','220_x','221_x','222_x','223_x','224_x','225_x','226_x','227_x','228_x','229_x','230_x','231_x','232_x','233_x','234_x','235_x','236_x','237_x','238_x','239_x','240_x','241_x','242_x','243_x','244_x','245_x','246_x','247_x','248_x','249_x','250_x','251_x','252_x','253_x','254_x','255_x','256_x','257_x','258_x','259_x','260_x','261_x','262_x','263_x','264_x','265_x','266_x','267_x','268_x','269_x','270_x','271_x','272_x','273_x','274_x','275_x','276_x','277_x','278_x','279_x','280_x','281_x','282_x','283_x','284_x','285_x','286_x','287_x','288_x','289_x','290_x','291_x','292_x','293_x','294_x','295_x','296_x','297_x','298_x','299_x','300_x','301_x','302_x','303_x','304_x','305_x','306_x','307_x','308_x','309_x','310_x','311_x','312_x','313_x','314_x','315_x','316_x','317_x','318_x','319_x','320_x','321_x','322_x','323_x','324_x','325_x','326_x','327_x','328_x','329_x','330_x','331_x','332_x','333_x','334_x','335_x','336_x','337_x','338_x','339_x','340_x','341_x','342_x','343_x','344_x','345_x','346_x','347_x','348_x','349_x','350_x','351_x','352_x','353_x','354_x','355_x','356_x','357_x','358_x','359_x','360_x','361_x','362_x','363_x','364_x','365_x','366_x','367_x','368_x','369_x','370_x','371_x','372_x','373_x','374_x','375_x','376_x','377_x','378_x','379_x','380_x','381_x','382_x','383_x','0_y','1_y','2_y','3_y','4_y','5_y','6_y','7_y','8_y','9_y','10_y','11_y','12_y','13_y','14_y','15_y','16_y','17_y','18_y','19_y','20_y','21_y','22_y','23_y','24_y','25_y','26_y','27_y','28_y','29_y','30_y','31_y','32_y','33_y','34_y','35_y','36_y','37_y','38_y','39_y','40_y','41_y','42_y','43_y','44_y','45_y','46_y','47_y','48_y','49_y','50_y','51_y','52_y','53_y','54_y','55_y','56_y','57_y','58_y','59_y','60_y','61_y','62_y','63_y','64_y','65_y','66_y','67_y','68_y','69_y','70_y','71_y','72_y','73_y','74_y','75_y','76_y','77_y','78_y','79_y','80_y','81_y','82_y','83_y','84_y','85_y','86_y','87_y','88_y','89_y','90_y','91_y','92_y','93_y','94_y','95_y','96_y','97_y','98_y','99_y','100_y','101_y','102_y','103_y','104_y','105_y','106_y','107_y','108_y','109_y','110_y','111_y','112_y','113_y','114_y','115_y','116_y','117_y','118_y','119_y','120_y','121_y','122_y','123_y','124_y','125_y','126_y','127_y','128_y','129_y','130_y','131_y','132_y','133_y','134_y','135_y','136_y','137_y','138_y','139_y','140_y','141_y','142_y','143_y','144_y','145_y','146_y','147_y','148_y','149_y','150_y','151_y','152_y','153_y','154_y','155_y','156_y','157_y','158_y','159_y','160_y','161_y','162_y','163_y','164_y','165_y','166_y','167_y','168_y','169_y','170_y','171_y','172_y','173_y','174_y','175_y','176_y','177_y','178_y','179_y','180_y','181_y','182_y','183_y','184_y','185_y','186_y','187_y','188_y','189_y','190_y','191_y','192_y','193_y','194_y','195_y','196_y','197_y','198_y','199_y','200_y','201_y','202_y','203_y','204_y','205_y','206_y','207_y','208_y','209_y','210_y','211_y','212_y','213_y','214_y','215_y','216_y','217_y','218_y','219_y','220_y','221_y','222_y','223_y','224_y','225_y','226_y','227_y','228_y','229_y','230_y','231_y','232_y','233_y','234_y','235_y','236_y','237_y','238_y','239_y','240_y','241_y','242_y','243_y','244_y','245_y','246_y','247_y','248_y','249_y','250_y','251_y','252_y','253_y','254_y','255_y','256_y','257_y','258_y','259_y','260_y','261_y','262_y','263_y','264_y','265_y','266_y','267_y','268_y','269_y','270_y','271_y','272_y','273_y','274_y','275_y','276_y','277_y','278_y','279_y','280_y','281_y','282_y','283_y','284_y','285_y','286_y','287_y','288_y','289_y','290_y','291_y','292_y','293_y','294_y','295_y','296_y','297_y','298_y','299_y','300_y','301_y','302_y','303_y','304_y','305_y','306_y','307_y','308_y','309_y','310_y','311_y','312_y','313_y','314_y','315_y','316_y','317_y','318_y','319_y','320_y','321_y','322_y','323_y','324_y','325_y','326_y','327_y','328_y','329_y','330_y','331_y','332_y','333_y','334_y','335_y','336_y','337_y','338_y','339_y','340_y','341_y','342_y','343_y','344_y','345_y','346_y','347_y','348_y','349_y','350_y','351_y','352_y','353_y','354_y','355_y','356_y','357_y','358_y','359_y','360_y','361_y','362_y','363_y','364_y','365_y','366_y','367_y','368_y','369_y','370_y','371_y','372_y','373_y','374_y','375_y','376_y','377_y','378_y','379_y','380_y','381_y','382_y','383_y'], chunksize=chunksize, iterator=True, encoding='utf-8', ):\n        df.index += index_start\n        j+=1\n        print('{} rows'.format(j*chunksize))\n        df.to_sql('data', disk_engine, if_exists='append')\n        index_start = df.index[-1] + 1\n</code></pre> <pre><code>#http://www.sqlitetutorial.net/sqlite-python/create-tables/\ndef create_connection(db_file):\n    \"\"\" create a database connection to the SQLite database\n        specified by db_file\n    :param db_file: database file\n    :return: Connection object or None\n    \"\"\"\n    try:\n        conn = sqlite3.connect(db_file)\n        return conn\n    except Error as e:\n        print(e)\n\n    return None\n\n\ndef checkTableExists(dbcon):\n    cursr = dbcon.cursor()\n    str = \"select name from sqlite_master where type='table'\"\n    table_names = cursr.execute(str)\n    print(\"Tables in the databse:\")\n    tables =table_names.fetchall() \n    print(tables[0][0])\n    return(len(tables))\n</code></pre> <pre><code>read_db = 'train.db'\nconn_r = create_connection(read_db)\ncheckTableExists(conn_r)\nconn_r.close()\n</code></pre> <pre><code># try to sample data according to the computing power you have\nif os.path.isfile(read_db):\n    conn_r = create_connection(read_db)\n    if conn_r is not None:\n        # for selecting first 1M rows\n        # data = pd.read_sql_query(\"\"\"SELECT * FROM data LIMIT 100001;\"\"\", conn_r)\n\n        # for selecting random points\n        data = pd.read_sql_query(\"SELECT * From data ORDER BY RANDOM() LIMIT 100001;\", conn_r)\n        conn_r.commit()\n        conn_r.close()\n</code></pre> <pre><code># remove the first row \ndata.drop(data.index[0], inplace=True)\ny_true = data['is_duplicate']\ndata.drop(['Unnamed: 0', 'id','index','is_duplicate'], axis=1, inplace=True)\n</code></pre> <pre><code>data.head()\n</code></pre> <pre><code># after we read from sql table each entry was read it as a string\n# we convert all the features into numaric before we apply any model\ncols = list(data.columns)\nfor i in cols:\n    data[i] = data[i].apply(pd.to_numeric)\n    print(i)\n</code></pre> <pre><code># https://stackoverflow.com/questions/7368789/convert-all-strings-in-a-list-to-int\ny_true = list(map(int, y_true.values))\n</code></pre> <pre><code>X_train,X_test, y_train, y_test = train_test_split(data, y_true, stratify=y_true, test_size=0.3)\n</code></pre> <pre><code>print(\"Number of data points in train data :\",X_train.shape)\nprint(\"Number of data points in test data :\",X_test.shape)\n</code></pre> <pre><code>print(\"-\"*10, \"Distribution of output variable in train data\", \"-\"*10)\ntrain_distr = Counter(y_train)\ntrain_len = len(y_train)\nprint(\"Class 0: \",int(train_distr[0])/train_len,\"Class 1: \", int(train_distr[1])/train_len)\nprint(\"-\"*10, \"Distribution of output variable in train data\", \"-\"*10)\ntest_distr = Counter(y_test)\ntest_len = len(y_test)\nprint(\"Class 0: \",int(test_distr[1])/test_len, \"Class 1: \",int(test_distr[1])/test_len)\n</code></pre> <pre><code># This function plots the confusion matrices given y_i, y_i_hat.\ndef plot_confusion_matrix(test_y, predict_y):\n    C = confusion_matrix(test_y, predict_y)\n    # C = 9,9 matrix, each cell (i,j) represents number of points of class i are predicted class j\n\n    A =(((C.T)/(C.sum(axis=1))).T)\n    #divid each element of the confusion matrix with the sum of elements in that column\n\n    # C = [[1, 2],\n    #     [3, 4]]\n    # C.T = [[1, 3],\n    #        [2, 4]]\n    # C.sum(axis = 1)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array\n    # C.sum(axix =1) = [[3, 7]]\n    # ((C.T)/(C.sum(axis=1))) = [[1/3, 3/7]\n    #                           [2/3, 4/7]]\n\n    # ((C.T)/(C.sum(axis=1))).T = [[1/3, 2/3]\n    #                           [3/7, 4/7]]\n    # sum of row elements = 1\n\n    B =(C/C.sum(axis=0))\n    #divid each element of the confusion matrix with the sum of elements in that row\n    # C = [[1, 2],\n    #     [3, 4]]\n    # C.sum(axis = 0)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array\n    # C.sum(axix =0) = [[4, 6]]\n    # (C/C.sum(axis=0)) = [[1/4, 2/6],\n    #                      [3/4, 4/6]] \n    plt.figure(figsize=(20,4))\n\n    labels = [1,2]\n    # representing A in heatmap format\n    cmap=sns.light_palette(\"blue\")\n    plt.subplot(1, 3, 1)\n    sns.heatmap(C, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Confusion matrix\")\n\n    plt.subplot(1, 3, 2)\n    sns.heatmap(B, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Precision matrix\")\n\n    plt.subplot(1, 3, 3)\n    # representing B in heatmap format\n    sns.heatmap(A, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Recall matrix\")\n\n    plt.show()\n</code></pre> <pre><code># we need to generate 9 numbers and the sum of numbers should be 1\n# one solution is to genarate 9 numbers and divide each of the numbers by their sum\n# ref: https://stackoverflow.com/a/18662466/4084039\n# we create a output array that has exactly same size as the CV data\npredicted_y = np.zeros((test_len,2))\nfor i in range(test_len):\n    rand_probs = np.random.rand(1,2)\n    predicted_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])\nprint(\"Log loss on Test Data using Random Model\",log_loss(y_test, predicted_y, eps=1e-15))\n\npredicted_y =np.argmax(predicted_y, axis=1)\nplot_confusion_matrix(y_test, predicted_y)\n</code></pre> <pre><code>alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\n\n# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n# ------------------------------\n# default parameters\n# SGDClassifier(loss=\u2019hinge\u2019, penalty=\u2019l2\u2019, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=\u2019optimal\u2019, eta0=0.0, power_t=0.5, \n# class_weight=None, warm_start=False, average=False, n_iter=None)\n\n# some of methods\n# fit(X, y[, coef_init, intercept_init, \u2026]) Fit linear model with Stochastic Gradient Descent.\n# predict(X)    Predict class labels for samples in X.\n\n#-------------------------------\n# video link: \n#------------------------------\n\n\nlog_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(X_train, y_train)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(X_train, y_train)\n    predict_y = sig_clf.predict_proba(X_test)\n    log_error_array.append(log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n\nfig, ax = plt.subplots()\nax.plot(alpha, log_error_array,c='g')\nfor i, txt in enumerate(np.round(log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(X_train, y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(X_train, y_train)\n\npredict_y = sig_clf.predict_proba(X_train)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_test)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\npredicted_y =np.argmax(predict_y,axis=1)\nprint(\"Total number of data points :\", len(predicted_y))\nplot_confusion_matrix(y_test, predicted_y)\n</code></pre> <pre><code>alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\n\n# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n# ------------------------------\n# default parameters\n# SGDClassifier(loss=\u2019hinge\u2019, penalty=\u2019l2\u2019, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=\u2019optimal\u2019, eta0=0.0, power_t=0.5, \n# class_weight=None, warm_start=False, average=False, n_iter=None)\n\n# some of methods\n# fit(X, y[, coef_init, intercept_init, \u2026]) Fit linear model with Stochastic Gradient Descent.\n# predict(X)    Predict class labels for samples in X.\n\n#-------------------------------\n# video link: \n#------------------------------\n\n\nlog_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l1', loss='hinge', random_state=42)\n    clf.fit(X_train, y_train)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(X_train, y_train)\n    predict_y = sig_clf.predict_proba(X_test)\n    log_error_array.append(log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n\nfig, ax = plt.subplots()\nax.plot(alpha, log_error_array,c='g')\nfor i, txt in enumerate(np.round(log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l1', loss='hinge', random_state=42)\nclf.fit(X_train, y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(X_train, y_train)\n\npredict_y = sig_clf.predict_proba(X_train)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_test)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\npredicted_y =np.argmax(predict_y,axis=1)\nprint(\"Total number of data points :\", len(predicted_y))\nplot_confusion_matrix(y_test, predicted_y)\n</code></pre> <pre><code>import xgboost as xgb\nparams = {}\nparams['objective'] = 'binary:logistic'\nparams['eval_metric'] = 'logloss'\nparams['eta'] = 0.02\nparams['max_depth'] = 4\n\nd_train = xgb.DMatrix(X_train, label=y_train)\nd_test = xgb.DMatrix(X_test, label=y_test)\n\nwatchlist = [(d_train, 'train'), (d_test, 'valid')]\n\nbst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=20, verbose_eval=10)\n\nxgdmat = xgb.DMatrix(X_train,y_train)\npredict_y = bst.predict(d_test)\nprint(\"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n</code></pre> <pre><code>predicted_y =np.array(predict_y&amp;gt;0.5,dtype=int)\nprint(\"Total number of data points :\", len(predicted_y))\nplot_confusion_matrix(y_test, predicted_y)\n</code></pre> <p>References - Kaggle Winning Solution: https://www.dropbox.com/sh/93968nfnrzh8bp5/AACZdtsApc1QSTQc7X0H3QZ5a?dl=0 - Blog 1 : https://engineering.quora.com/Semantic-Question-Matching-with-Deep-Learning - Blog 2 : https://towardsdatascience.com/identifying-duplicate-questions-on-quora-top-12-on-kaggle-4c1cf93f1c30</p> <pre><code>\n</code></pre>"},{"location":"Applied%20Solutions/Quora%20Question%20Pairs/#quora-question-pairs","title":"Quora Question Pairs","text":""},{"location":"Applied%20Solutions/Quora%20Question%20Pairs/#business-understanding","title":"Business Understanding","text":"<p>Quora is a place to gain and share knowledge\u2014about anything. It\u2019s a platform to ask questions and connect with people who contribute unique insights and quality answers. This empowers people to learn from each other and to better understand the world.</p> <p></p> <p>Over 100 million people visit Quora every month, so it's no surprise that many people ask similarly worded questions. Multiple questions with the same intent can cause seekers to spend more time finding the best answer to their question, and make writers feel they need to answer multiple versions of the same question. Quora values canonical questions because they provide a better experience to active seekers and writers, and offer more value to both of these groups in the long term.</p> <p></p> <p>Data Credit: Kaggle - Quora </p>"},{"location":"Applied%20Solutions/Quora%20Question%20Pairs/#business-objectives-and-constraints","title":"Business Objectives and Constraints","text":"<ul> <li>Identify which questions asked on Quora are duplicates of questions that have already been asked.</li> <li>This should be useful to provide answers to questions that have already been answered.</li> <li>Output a probability of a pair of questions to be duplicates, so that any threshold can be chosen.</li> <li>The cost of a mis-classification is very high.</li> <li>Interpretability is partially important.</li> <li>No strict latency concerns.</li> </ul>"},{"location":"Applied%20Solutions/Quora%20Question%20Pairs/#machine-learinng-problem-statement","title":"Machine Learinng Problem Statement","text":"<ul> <li> <p>Type of Machine Leaning Problem : It is a binary classification problem, for a given pair of questions we need to predict if they are duplicate or not.</p> </li> <li> <p>Performance Metric :</p> <ul> <li>Cross Entropy or log-loss</li> <li>Confusion Matrix</li> </ul> </li> </ul>"},{"location":"Applied%20Solutions/Quora%20Question%20Pairs/#data-overview","title":"Data Overview","text":""},{"location":"Applied%20Solutions/Quora%20Question%20Pairs/#exploratory-data-analysis","title":"Exploratory Data Analysis","text":""},{"location":"Applied%20Solutions/Quora%20Question%20Pairs/#number-of-unique-questions","title":"Number of unique questions","text":""},{"location":"Applied%20Solutions/Quora%20Question%20Pairs/#checking-for-duplicates","title":"Checking for Duplicates","text":""},{"location":"Applied%20Solutions/Quora%20Question%20Pairs/#number-of-occurrences-of-each-question","title":"Number of occurrences of each question","text":""},{"location":"Applied%20Solutions/Quora%20Question%20Pairs/#checking-for-null-values","title":"Checking for NULL values","text":""},{"location":"Applied%20Solutions/Quora%20Question%20Pairs/#feature-engineering","title":"Feature Engineering","text":""},{"location":"Applied%20Solutions/Quora%20Question%20Pairs/#word_share","title":"word_share","text":""},{"location":"Applied%20Solutions/Quora%20Question%20Pairs/#word_common","title":"word_Common","text":""},{"location":"Applied%20Solutions/Quora%20Question%20Pairs/#text-preprocessing","title":"Text Preprocessing","text":""},{"location":"Applied%20Solutions/Quora%20Question%20Pairs/#fuzzy-features","title":"Fuzzy Features","text":"<p>Definition: - Token: You get a token by splitting sentence a space - Stop_Word : stop words as per NLTK. - Word : A token that is not a stop_word</p> <p>Features: - cwc_min :  Ratio of common_word_count to min lenghth of word count of Q1 and Q2 cwc_min = common_word_count / (min(len(q1_words), len(q2_words))</p> <ul> <li> <p>cwc_max :  Ratio of common_word_count to max lenghth of word count of Q1 and Q2 cwc_max = common_word_count / (max(len(q1_words), len(q2_words))</p> </li> <li> <p>csc_min :  Ratio of common_stop_count to min lenghth of stop count of Q1 and Q2  csc_min = common_stop_count / (min(len(q1_stops), len(q2_stops))</p> </li> <li> <p>csc_max :  Ratio of common_stop_count to max lenghth of stop count of Q1 and Q2csc_max = common_stop_count / (max(len(q1_stops), len(q2_stops))</p> </li> <li> <p>ctc_min :  Ratio of common_token_count to min lenghth of token count of Q1 and Q2ctc_min = common_token_count / (min(len(q1_tokens), len(q2_tokens))</p> </li> <li> <p>ctc_max :  Ratio of common_token_count to max lenghth of token count of Q1 and Q2ctc_max = common_token_count / (max(len(q1_tokens), len(q2_tokens))</p> </li> <li> <p>last_word_eq :  Check if Last word of both questions is equal or notlast_word_eq = int(q1_tokens[-1] == q2_tokens[-1])</p> </li> <li> <p>first_word_eq :  Check if First word of both questions is equal or notfirst_word_eq = int(q1_tokens[0] == q2_tokens[0])</p> </li> <li> <p>abs_len_diff :  Abs. length differenceabs_len_diff = abs(len(q1_tokens) - len(q2_tokens))</p> </li> <li> <p>mean_len :  Average Token Length of both Questionsmean_len = (len(q1_tokens) + len(q2_tokens))/2</p> </li> <li> <p>fuzz_ratio :  https://github.com/seatgeek/fuzzywuzzy#usage http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/</p> </li> <li> <p>fuzz_partial_ratio :  https://github.com/seatgeek/fuzzywuzzy#usage http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/</p> </li> <li> <p>token_sort_ratio : https://github.com/seatgeek/fuzzywuzzy#usage http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/</p> </li> <li> <p>token_set_ratio : https://github.com/seatgeek/fuzzywuzzy#usage http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/</p> </li> <li> <p>longest_substr_ratio :  Ratio of length longest common substring to min lenghth of token count of Q1 and Q2longest_substr_ratio = len(longest common substring) / (min(len(q1_tokens), len(q2_tokens))</p> </li> </ul>"},{"location":"Applied%20Solutions/Quora%20Question%20Pairs/#word-clouds","title":"Word clouds","text":"<ul> <li>Creating Word Cloud of Duplicates and Non-Duplicates Question pairs</li> <li>We can observe the most frequent occuring words</li> </ul>"},{"location":"Applied%20Solutions/Quora%20Question%20Pairs/#pair-plots","title":"Pair plots","text":""},{"location":"Applied%20Solutions/Quora%20Question%20Pairs/#tsne","title":"TSNE","text":""},{"location":"Applied%20Solutions/Quora%20Question%20Pairs/#tfidf-weighted-word-vectors","title":"tfidf weighted word-vectors","text":""},{"location":"Applied%20Solutions/Quora%20Question%20Pairs/#machine-learning-models","title":"Machine Learning Models","text":""},{"location":"Applied%20Solutions/Quora%20Question%20Pairs/#reading-data-from-file-and-storing-into-sql-table","title":"Reading data from file and storing into sql table","text":""},{"location":"Applied%20Solutions/Quora%20Question%20Pairs/#42-converting-strings-to-numerics","title":"4.2 Converting strings to numerics","text":""},{"location":"Applied%20Solutions/Quora%20Question%20Pairs/#43-random-train-test-split-7030","title":"4.3 Random train test split( 70:30)","text":""},{"location":"Applied%20Solutions/Quora%20Question%20Pairs/#44-building-a-random-model-finding-worst-case-log-loss","title":"4.4 Building a random model (Finding worst-case log-loss)","text":""},{"location":"Applied%20Solutions/Quora%20Question%20Pairs/#44-logistic-regression-with-hyperparameter-tuning","title":"4.4 Logistic Regression with hyperparameter tuning","text":""},{"location":"Applied%20Solutions/Quora%20Question%20Pairs/#45-linear-svm-with-hyperparameter-tuning","title":"4.5 Linear SVM with hyperparameter tuning","text":""},{"location":"Applied%20Solutions/Quora%20Question%20Pairs/#46-xgboost","title":"4.6 XGBoost","text":""},{"location":"Applied%20Solutions/Quora%20Question%20Pairs/#hyperparameter-tuning","title":"hyperparameter tuning","text":"<p>RandomsearchCV with vectorizer as TF-IDF W2V  to reduce the log-loss.</p>"},{"location":"Applied%20Solutions/Analysis%20and%20Guides/Differentiation%20in%20TensorFlow/","title":"Differentiation in TensorFlow","text":"<p>Automatic differentiation is useful for implementing machine learning algorithms such as backpropagation for training neural networks. In this guide, we will explore ways to compute gradients with TensorFlow in eager execution.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n</code></pre> <pre><code>w = tf.Variable(tf.random.normal((3, 2)), name='w')\nb = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\nx = [[1., 2., 3.]]\n</code></pre> <pre>\n<code>2022-12-28 18:53:47.361164: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n</code>\n</pre> <pre><code>w\n</code></pre> <pre>\n<code>&lt;tf.Variable 'w:0' shape=(3, 2) dtype=float32, numpy=\narray([[ 0.5979703 , -0.42594454],\n       [ 0.44298685,  0.67796963],\n       [-0.00147911, -1.0101731 ]], dtype=float32)&gt;</code>\n</pre> <pre><code>b\n</code></pre> <pre>\n<code>&lt;tf.Variable 'b:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)&gt;</code>\n</pre> <p>TensorFlow provides the tf.GradientTape API for automatic differentiation; that is, computing the gradient of a computation with respect to some inputs, usually tf.Variables. TensorFlow \"records\" relevant operations executed inside the context of a tf.GradientTape onto a \"tape\". TensorFlow then uses that tape to compute the gradients of a \"recorded\" computation using reverse mode differentiation.</p> <pre><code>with tf.GradientTape(persistent=True) as tape:\n    y = x @ w + b\n    loss = tf.reduce_mean(y**2)\n\n# Using lists\ngrad = tape.gradient(loss, [w, b])\nprint(f'w : {grad[0]} \\n\\nb : {grad[1]}\\n\\n')\n\n# Using dictionaries\ngrad = tape.gradient(loss, {'w': w, 'b': b})\nprint(f'w : {grad[\"w\"]} \\n\\nb : {grad[\"b\"]}')\n</code></pre> <pre>\n<code>w : [[ 1.4795066 -2.1005244]\n [ 2.9590132 -4.201049 ]\n [ 4.43852   -6.3015733]] \n\nb : [ 1.4795066 -2.1005244]\n\n\nw : [[ 1.4795066 -2.1005244]\n [ 2.9590132 -4.201049 ]\n [ 4.43852   -6.3015733]] \n\nb : [ 1.4795066 -2.1005244]\n</code>\n</pre> <pre><code># A trainable variable\nx0 = tf.Variable(3.0, name='x0')\n\n# Not trainable\nx1 = tf.Variable(3.0, name='x1', trainable=False)\n\n# Not a Variable: A variable + tensor returns a tensor.\nx2 = tf.Variable(2.0, name='x2') + 1.0\n\n# Not a variable\nx3 = tf.constant(3.0, name='x3')\n\nwith tf.GradientTape() as tape:\n    y = (x0**2) + (x1**2) + (x2**2)\n\ngrad = tape.gradient(y, [x0, x1, x2, x3])\n\nfor g in grad:\n    print(g)\n</code></pre> <pre>\n<code>tf.Tensor(6.0, shape=(), dtype=float32)\nNone\nNone\nNone\n</code>\n</pre> <p>tape.watched_variables is used to get the list of all variables which tensorflow is watching</p> <pre><code>[var.name for var in tape.watched_variables()]\n</code></pre> <pre>\n<code>['x0:0']</code>\n</pre> <p>To disable the default behavior of watching all tf.Variables, set watch_accessed_variables=False when creating the gradient tape</p> <pre><code>x0 = tf.Variable(0.0)\nx1 = tf.Variable(10.0)\n\nwith tf.GradientTape(watch_accessed_variables=False) as tape:\n    # set only x1 to be watched not x0\n    tape.watch(x1)\n    y0 = tf.math.sin(x0)\n    y1 = tf.nn.softplus(x1)\n    y = y0 + y1\n    ys = tf.reduce_sum(y)\n\ngrad = tape.gradient(ys, {'x0': x0, 'x1': x1})\n\nprint('dy/dx0:', grad['x0'])\nprint('dy/dx1:', grad['x1'].numpy())\n</code></pre> <pre>\n<code>dy/dx0: None\ndy/dx1: 0.9999546\n</code>\n</pre> <p>By default, the resources held by a GradientTape are released as soon as the GradientTape.gradient method is called. To compute multiple gradients over the same computation, create a gradient tape with persistent=True. This allows multiple calls to the gradient method as resources are released when the tape object is garbage collected. </p> <pre><code>x = tf.constant([1, 3.0])\nwith tf.GradientTape(persistent=True) as tape:\n    tape.watch(x)\n    y = x * x * x * x \n    z = y * y * y * y\n\nprint(tape.gradient(z, x).numpy()) \nprint(tape.gradient(y, x).numpy())\n</code></pre> <pre>\n<code>[1.6000000e+01 2.2958251e+08]\n[  4. 108.]\n</code>\n</pre> <pre><code>x = tf.constant(1.0)\n\nv0 = tf.Variable(2.0)\nv1 = tf.Variable(2.0)\n\nwith tf.GradientTape(persistent=True) as tape:\n    tape.watch(x)\n    if x &amp;gt; 0.0:\n        result = v0\n    else:\n        result = v1**2 \n\ndv0, dv1 = tape.gradient(result, [v0, v1])\n\nprint(dv0)\nprint(dv1)\n\ndx = tape.gradient(result, x)\nprint(dx)\n</code></pre> <pre>\n<code>tf.Tensor(1.0, shape=(), dtype=float32)\nNone\nNone\n</code>\n</pre> <p>Control statements themselves are not differentiable, so they are invisible to gradient-based optimizers. Depending on the value of x in the above example, the tape either records result = v0 or result = v1**2. The gradient with respect to x is always None.</p>"},{"location":"Applied%20Solutions/Analysis%20and%20Guides/Differentiation%20in%20TensorFlow/#gradients-and-automatic-differentiation","title":"Gradients and Automatic Differentiation","text":""},{"location":"Applied%20Solutions/Analysis%20and%20Guides/Differentiation%20in%20TensorFlow/#notes-on-performance","title":"Notes on performance","text":"<ul> <li> <p>There is a tiny overhead associated with doing operations inside a gradient tape context. For most eager execution this will not be a noticeable cost, but you should still use tape context around the areas only where it is required.</p> </li> <li> <p>Gradient tapes use memory to store intermediate results, including inputs and outputs, for use during the backwards pass.</p> </li> <li> <p>For efficiency, some ops (like ReLU) don't need to keep their intermediate results and they are pruned during the forward pass. However, if you use persistent=True on your tape, nothing is discarded and your peak memory usage will be higher.</p> </li> </ul>"},{"location":"Applied%20Solutions/Analysis%20and%20Guides/Differentiation%20in%20TensorFlow/#control-flow","title":"Control Flow","text":"<p>Here a different variable is used on each branch of an if. The gradient only connects to the variable that was used.</p>"},{"location":"Applied%20Solutions/Analysis%20and%20Guides/Dimensionality%20Reduction/","title":"Dimensionality Reduction","text":"<pre><code>from IPython.core.display import HTML\ndef css_styling():\n    styles = open(\"Assets/css/custom.css\", \"r\").read()\n    return HTML(styles)\ncss_styling()\n</code></pre> <ul> <li>Kaggle Competition : https://www.kaggle.com/c/digit-recognizer/data</li> <li>raw data file : https://raw.githubusercontent.com/wehrley/Kaggle-Digit-Recognizer/master/train.csv</li> </ul> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sn\nimport warnings\nwarnings.filterwarnings('ignore')\n</code></pre> <pre><code>train_file_path = 'https://raw.githubusercontent.com/wehrley/Kaggle-Digit-Recognizer/master/train.csv'\ndf = pd.read_csv(train_file_path)\nlabels = df['label']\ndata = df.drop(\"label\",axis=1)\nprint(\"the shape of data = \", data.shape)\ndf.head(5)\n</code></pre> <pre>\n<code>the shape of data =  (42000, 784)\n</code>\n</pre> label pixel0 pixel1 pixel2 pixel3 pixel4 pixel5 pixel6 pixel7 pixel8 ... pixel774 pixel775 pixel776 pixel777 pixel778 pixel779 pixel780 pixel781 pixel782 pixel783 0 1 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 2 1 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 3 4 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 <p>5 rows \u00d7 785 columns</p> <pre><code>idx = 22\nplt.figure(figsize=(4,4))\ngrid_data = data.iloc[idx].to_numpy().reshape(28,28)  # reshape from 1d to 2d pixel array\nplt.imshow(grid_data, interpolation = \"none\", cmap = \"gray\")\nplt.show()\nprint(f' label corresponding to the image is {labels[idx]}')\n</code></pre> <pre>\n<code> label corresponding to the image is 2\n</code>\n</pre> <pre><code>from sklearn.preprocessing import StandardScaler\nstandardized_data = StandardScaler().fit_transform(data)\nprint(standardized_data.shape)\n</code></pre> <pre>\n<code>(42000, 784)\n</code>\n</pre> <pre><code>#find the co-variance matrix which is : A^T * A\nsample_data = standardized_data\n\n# matrix multiplication using numpy\ncovar_matrix = np.matmul(sample_data.T , sample_data)\n\nprint ( \"The shape of variance matrix = \", covar_matrix.shape)\n</code></pre> <pre>\n<code>The shape of variance matrix =  (784, 784)\n</code>\n</pre> <pre><code># finding the top two eigen-values and corresponding eigen-vectors \n# for projecting onto a 2-Dim space.\n\nfrom scipy.linalg import eigh \n\n# the parameter 'eigvals' is defined (low value to heigh value) \n# eigh function will return the eigen values in asending order\n# this code generates only the top 2 (782 and 783) eigenvalues.\nvalues, vectors = eigh(covar_matrix, eigvals=(782,783))\n\nprint(\"Shape of eigen vectors = \",vectors.shape)\nprint(values)\n\n#vectors[:,0] represents the eigen vector corresponding to the 2nd eigen value.(First column in the vectors matrix)\n#vectors[:,1] represents the eigen vector correspondign to the 1st eigen value.(Second column in the vectors matrix)\n\n#Note : Eigen values are arranged in ascending order so the Eigen vectors too.\n\n\n# converting the eigen vectors into (2,d) shape for ease of computation which we do it later.\nvector = vectors.T\n\nprint(\"Updated shape of eigen vectors = \",vector.shape)\n# Here, vectors[0] represent the eigen vector corresponding to the 2nd eigen value.\n# Here, vectors[1] represent the eigen vector corresponding to the 1st eigen value.\n\n\n#For sanity check.\nprint((vector[0] == vectors[:,0]).all())\nprint((vector[1] == vectors[:,1]).all())\n\n#Now, we need to swap the rows of the vector matrix such that the first row corresponds to the eigen vector with the largest eigen value and the second row corresponds to the eigen vector with the second largest eigen value.\n\nvector[[0,1]]=vector[[1,0]]\n</code></pre> <pre>\n<code>Shape of eigen vectors =  (784, 2)\n[1222652.44613786 1709211.41082575]\nUpdated shape of eigen vectors =  (2, 784)\nTrue\nTrue\n</code>\n</pre> <pre><code># projecting the original data onto the eigen basis.\n# Basically, we form a matrix with the eigen vectors in row order. Then, we do a matrix-vector multiplication between the matrix we formed and all the data vectors.\n\nnew_coordinates = np.matmul(vector, sample_data.T)\n\nprint (\" resultant new data points' shape \", vector.shape, \"X\", sample_data.T.shape,\" = \", new_coordinates.shape)\n</code></pre> <pre>\n<code> resultant new data points' shape  (2, 784) X (784, 42000)  =  (2, 42000)\n</code>\n</pre> <pre><code># appending label to the 2d projected data\nnew_coordinates = np.vstack((new_coordinates, labels)).T\n\n# creating a new data frame for ploting the labeled points.\ndataframe = pd.DataFrame(data=new_coordinates, columns=(\"1st_principal\", \"2nd_principal\", \"label\"))\ndataframe.head()\n</code></pre> 1st_principal 2nd_principal label 0 -5.140478 -5.226445 1.0 1 19.292332 6.032996 0.0 2 -7.644503 -1.705813 1.0 3 -0.474207 5.836139 4.0 4 26.559574 6.024818 0.0 <pre><code># ploting the 2d data points with seaborn\nsn.FacetGrid(dataframe, hue=\"label\", size=6).map(plt.scatter, '1st_principal', '2nd_principal').add_legend()\n</code></pre> <pre>\n<code>&lt;seaborn.axisgrid.FacetGrid at 0x7f79560d5d00&gt;</code>\n</pre> <pre><code>sn.scatterplot(x=\"1st_principal\",y=\"2nd_principal\",legend=\"full\",hue=\"label\",data=dataframe)\n</code></pre> <pre>\n<code>&lt;AxesSubplot:xlabel='1st_principal', ylabel='2nd_principal'&gt;</code>\n</pre> <pre><code>from sklearn import decomposition\npca = decomposition.PCA()\n\n# the number of components = 2\npca.n_components = 2\npca_data = pca.fit_transform(sample_data)\n\n# pca_reduced will contain the 2-d projects of simple data\nprint(\"shape of pca_reduced.shape = \", pca_data.shape)\n\n# attaching the label for each 2-d data point \npca_data = np.vstack((pca_data.T, labels)).T\n\n# creating a new data fram which help us in ploting the result data\npca_df = pd.DataFrame(data=pca_data, columns=(\"1st_principal\", \"2nd_principal\", \"label\"))\nsn.FacetGrid(pca_df, hue=\"label\", size=6).map(plt.scatter, '1st_principal', '2nd_principal').add_legend()\nplt.show()\n</code></pre> <pre>\n<code>shape of pca_reduced.shape =  (42000, 2)\n</code>\n</pre> <pre><code># PCA for dimensionality redcution (non-visualization)\n\npca.n_components = 784\npca_data = pca.fit_transform(sample_data)\n\npercentage_var_explained = pca.explained_variance_ / np.sum(pca.explained_variance_);\n\ncum_var_explained = np.cumsum(percentage_var_explained)\n\n# Plot the PCA spectrum\nplt.figure(1, figsize=(6, 4))\n\nplt.clf()\nplt.plot(cum_var_explained, linewidth=2)\nplt.axis('tight')\nplt.grid()\nplt.xlabel('n_components')\nplt.ylabel('Cumulative_explained_variance')\nplt.show()\n\n# If we take 200-dimensions, approx. 90% of variance is expalined.\n</code></pre> <pre><code># TSNE\n\nfrom sklearn.manifold import TSNE\n\ndata_1000 = standardized_data.copy() #[0:1000,:]\nlabels_1000 = labels.copy() #[0:1000]\n\nmodel = TSNE(n_components=2, random_state=0)\n# configuring the parameteres\n# the number of components = 2\n# default perplexity = 30\n# default learning rate = 200\n# default Maximum number of iterations for the optimization = 1000\n\ntsne_data = model.fit_transform(data_1000)\n\n\n# creating a new data frame which help us in ploting the result data\ntsne_data = np.vstack((tsne_data.T, labels_1000)).T\ntsne_df = pd.DataFrame(data=tsne_data, columns=(\"Dim_1\", \"Dim_2\", \"label\"))\n\n# Ploting the result of tsne\nsn.FacetGrid(tsne_df, hue=\"label\", size=6).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\nplt.show()\n</code></pre> <pre><code>model = TSNE(n_components=2, random_state=0, perplexity=50)\ntsne_data = model.fit_transform(data_1000) \n\n# creating a new data fram which help us in ploting the result data\ntsne_data = np.vstack((tsne_data.T, labels_1000)).T\ntsne_df = pd.DataFrame(data=tsne_data, columns=(\"Dim_1\", \"Dim_2\", \"label\"))\n\n# Ploting the result of tsne\nsn.FacetGrid(tsne_df, hue=\"label\", size=6).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\nplt.title('With perplexity = 50')\nplt.show()\n</code></pre> <pre><code>model = TSNE(n_components=2, random_state=0, perplexity=50,  n_iter=5000)\ntsne_data = model.fit_transform(data_1000) \n\n# creating a new data fram which help us in ploting the result data\ntsne_data = np.vstack((tsne_data.T, labels_1000)).T\ntsne_df = pd.DataFrame(data=tsne_data, columns=(\"Dim_1\", \"Dim_2\", \"label\"))\n\n# Ploting the result of tsne\nsn.FacetGrid(tsne_df, hue=\"label\", size=6).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\nplt.title('With perplexity = 50, n_iter=5000')\nplt.show()\n</code></pre> <pre><code>model = TSNE(n_components=2, random_state=0, perplexity=2)\ntsne_data = model.fit_transform(data_1000) \n\n# creating a new data fram which help us in ploting the result data\ntsne_data = np.vstack((tsne_data.T, labels_1000)).T\ntsne_df = pd.DataFrame(data=tsne_data, columns=(\"Dim_1\", \"Dim_2\", \"label\"))\n\n# Ploting the result of tsne\nsn.FacetGrid(tsne_df, hue=\"label\", size=6).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\nplt.title('With perplexity = 2')\nplt.show()\n</code></pre>"},{"location":"Applied%20Solutions/Analysis%20and%20Guides/Dimensionality%20Reduction/#import-libraries","title":"Import Libraries","text":""},{"location":"Applied%20Solutions/Analysis%20and%20Guides/Dimensionality%20Reduction/#load-data","title":"Load Data","text":""},{"location":"Applied%20Solutions/Analysis%20and%20Guides/Dimensionality%20Reduction/#2d-visualization-using-pca","title":"2D Visualization Using PCA","text":""},{"location":"Applied%20Solutions/Analysis%20and%20Guides/Dimensionality%20Reduction/#data-preprocessing-standardizing-the-data","title":"Data-preprocessing: Standardizing the data","text":""},{"location":"Applied%20Solutions/Analysis%20and%20Guides/Dimensionality%20Reduction/#pca-using-scikit-learn","title":"PCA using Scikit-Learn","text":""},{"location":"Applied%20Solutions/Analysis%20and%20Guides/Dimensionality%20Reduction/#pca-for-dimensionality-redcution-not-for-visualization","title":"PCA for dimensionality redcution (not for visualization)","text":""},{"location":"Applied%20Solutions/Analysis%20and%20Guides/Dimensionality%20Reduction/#t-sne-using-scikit-learn","title":"t-SNE using Scikit-Learn","text":""},{"location":"Applied%20Solutions/Analysis%20and%20Guides/Scaling%20in%20Machine%20Learning/","title":"Scaling in Machine Learning","text":"<pre><code># Importing libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n% matplotlib inline\nmatplotlib.style.use('fivethirtyeight')\n</code></pre> <pre>\n<code>UsageError: Line magic function `%` not found.\n</code>\n</pre> <pre><code>def plot_comparison(x):\n    scaler = preprocessing.RobustScaler()\n    robust_df = scaler.fit_transform(x)\n    robust_df = pd.DataFrame(robust_df, columns =['x1', 'x2'])\n\n    scaler = preprocessing.StandardScaler()\n    standard_df = scaler.fit_transform(x)\n    standard_df = pd.DataFrame(standard_df, columns =['x1', 'x2'])\n\n    scaler = preprocessing.MinMaxScaler()\n    minmax_df = scaler.fit_transform(x)\n    minmax_df = pd.DataFrame(minmax_df, columns =['x1', 'x2'])\n\n    fig, (ax1, ax2, ax3, ax4) = plt.subplots(ncols = 4, figsize =(20, 5))\n    ax1.set_title('Before Scaling')\n\n    sns.kdeplot(x['x1'], ax = ax1, color ='r')\n    sns.kdeplot(x['x2'], ax = ax1, color ='b')\n    ax2.set_title('After Robust Scaling')\n\n    sns.kdeplot(robust_df['x1'], ax = ax2, color ='red')\n    sns.kdeplot(robust_df['x2'], ax = ax2, color ='blue')\n    ax3.set_title('After Standard Scaling')\n\n    sns.kdeplot(standard_df['x1'], ax = ax3, color ='black')\n    sns.kdeplot(standard_df['x2'], ax = ax3, color ='g')\n    ax4.set_title('After Min-Max Scaling')\n\n    sns.kdeplot(minmax_df['x1'], ax = ax4, color ='black')\n    sns.kdeplot(minmax_df['x2'], ax = ax4, color ='g')\n    plt.show()\n</code></pre> <pre><code>x = pd.DataFrame({\n    # Distribution with higher outliers\n    'x1': np.concatenate([np.random.normal(10, 2, 1000), np.random.normal(50, 2, 25)]),\n    # Distribution with lower outliers\n    'x2': np.concatenate([np.random.normal(100, 2, 1000), np.random.normal(60, 2, 25)]),\n})\n\nplot_comparison(x)\n</code></pre> <pre><code>x = pd.DataFrame({\n    # Distribution with lower outliers\n    'x1': np.concatenate([np.random.normal(20, 2, 1000), np.random.normal(1, 2, 25)]),\n    # Distribution with higher outliers\n    'x2': np.concatenate([np.random.normal(30, 2, 1000), np.random.normal(50, 2, 25)]),\n})\n\nplot_comparison(x)\n</code></pre> <pre><code>x = pd.DataFrame({\n    # Distribution with lower outliers\n    'x1': np.concatenate([np.random.normal(10, 2, 1000), np.random.normal(0, 2, 25)]),\n    # Distribution with higher outliers\n    'x2': np.concatenate([np.random.normal(100, 5, 1000), np.random.normal(110, 5, 25)]),\n})\n\nplot_comparison(x)\n</code></pre> <pre><code>x = pd.DataFrame({\n    # Distribution with lower outliers\n    'x1': np.concatenate([np.random.normal(10, 2, 1000), np.random.normal(-20, 1, 25)]),\n    # Distribution with higher outliers\n    'x2': np.concatenate([np.random.normal(100, 5, 1000), np.random.normal(130, 1, 25)]),\n})\n\nplot_comparison(x)\n</code></pre> <pre><code>x = pd.DataFrame({\n    # Distribution with lower outliers\n    'x1': np.concatenate([np.random.normal(10, 2, 1000), np.random.normal(-20, 20, 25)]),\n    # Distribution with higher outliers\n    'x2': np.concatenate([np.random.normal(100, 5, 1000), np.random.normal(130, 50, 25)]),\n})\n\nplot_comparison(x)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/","title":"Quick Steps Pipeline","text":"<p> Quick Steps Pipeline as the name suggests is a pretty fast template which can be used on any data science problem to generate a basic pipeline and get some quick and dirty analysis with baseline results. The aim of this pipeline is not to get the best results, but to get a good understanding about the data as quickly as possible and with minimum effort. </p> <p>In a data science project data is supreme and hence we will structure our project as the data is structured</p> <p>According to Data Engineering Principles described in the kedro library, We have divided the complete process into 8 Layers which are given below. Next we will discuss a little bit about each layer and what is supposed to be done in that layer</p> <p>Data Layer Definitions</p> <pre><code>\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 01_raw            &lt;-- Raw immutable data\n\u2502   \u251c\u2500\u2500 02_intermediate   &lt;-- Typed data\n\u2502   \u251c\u2500\u2500 03_primary        &lt;-- Cleaned Domain model data\n\u2502   \u251c\u2500\u2500 04_feature        &lt;-- Model features\n\u2502   \u251c\u2500\u2500 05_model_input    &lt;-- Often called 'master tables'\n\u2502   \u251c\u2500\u2500 06_models         &lt;-- Serialised models\n\u2502   \u251c\u2500\u2500 07_model_output   &lt;-- Data generated by model runs\n\u2502   \u251c\u2500\u2500 08_reporting      &lt;-- Ad hoc descriptive cuts\n</code></pre>"},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#raw-layer","title":"Raw Layer","text":"<p>This is the initial start of the pipeline, contains the sourced data model(s) that should never be changed, it forms your single source of truth to work from. These data models can be un-typed in most cases e.g. csv, but this will vary from case to case. Given the relative cost of storage today, painful experience suggests it's safer to never work with the original data directly!</p>"},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#understand-the-business-requirement","title":"Understand the Business Requirement.","text":"<ol> <li>Define the Problem Statement </li> <li>Define the customer and other stakeholders</li> <li>Define regression / classification / other</li> <li>Define assessment metrics</li> </ol>"},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#questions-hypotheses","title":"Questions &amp; Hypotheses","text":"<p>No matter what first generate questions &amp; hypotheses about your data and the problem at hand.</p> <ul> <li>Build as many hypotheses as you can about the data !</li> <li>What type of variation occurs within my variables?</li> <li>What type of co-variation occurs between my variables?</li> <li>Which values are the most common? Why?</li> <li>Which values are rare? Why? Does that match your expectations?</li> <li>Are there any unusual patterns? What might explain them?</li> <li>How are the observations within each cluster similar to each other?</li> <li>How are the observations in separate clusters different from each other?</li> <li>How can you explain or describe the clusters?</li> <li>Can the appearance of clusters be misleading? if Yes How could this happen?</li> <li>Are there any missing Values or outliers and should you remove these outliers.</li> <li>Could a pattern be due to coincidence (i.e. random chance)?</li> <li>How can you describe the relationship implied by the pattern?</li> <li>How strong is the relationship implied by the pattern?</li> <li>What other variables might affect a relationship?</li> <li>Does the relationship change if you look at individual subgroups of the data?</li> </ul>"},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#intermediate-layer","title":"Intermediate  Layer","text":"<p>This stage is optional if your data is already typed. Typed representation of the raw layer e.g. converting string based values into their current typed representation as numbers, dates etc. Our recommended approach is to mirror the raw layer in a typed format like Apache Parquet. Avoid transforming the structure of the data, but simple operations like cleaning up field names or 'unioning' multi-part CSVs is okay.</p>"},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#basic-cleaning","title":"Basic Cleaning","text":"<ol> <li>Parsing dates &amp; times and type the Data </li> <li>Typos in the name of features (UPPER_CASE) , the same attribute with a different name, mislabelled classes, i.e. separate classes that should really be the same, or inconsistent capitalisation.</li> <li>Treat data as immutable</li> </ol>"},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#create-schema","title":"Create Schema","text":"<p>We'll look at each variable and do a philosophical analysis about their meaning and importance for this problem.</p> <p>This is an template example of how a schema yaml file should look</p> <pre><code>schema:\n  raw_data1:\n    features:\n      col1:\n        name: col1\n        dtype: str / datetime64 / int16 / int32 / int64 / float16 / float32 / float64 \n        value_type: numerical_continuous / numerical_discrete / categorical_unordered / categorical_ordered / id / datetime / long_text\n        feature_type: target / feature\n        attribute_domain: events / location / aggregate / business / time\n        influence_expectation: High / Medium / Low\n        comments: general comments\n      col2:\n        name: col2\n        dtype: str / datetime64 / int16 / int32 / int64 / float16 / float32 / float64 \n        value_type: numerical_continuous / numerical_discrete / categorical_unordered / categorical_ordered / id / datetime / long_text\n        feature_type: target / feature\n        attribute_domain: events / location / aggregate / business / time\n        influence_expectation: High / Medium / Low\n        comments: general comments\n    primary_keys:\n      - col1\n      - col2\n</code></pre>"},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#exploratory-analysis","title":"Exploratory Analysis","text":"<ol> <li>EDA Block</li> <li>Slice and Dice </li> <li>Generate Data Drift for all features or at least the target</li> <li>Count, Quantiles, Mean, Std, Median, Mode, Skewness, Kurtosis / Peakedness, Min, Max </li> <li>QQ-Plots, Deviation from normal distribution</li> <li>Pandas Profiling / DataPrep Report</li> <li>Pandera</li> </ol>"},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#primary-layer","title":"Primary  Layer","text":"<p>Domain specific data model(s) containing cleansed, transformed and wrangled data from either raw or intermediate, which forms your layer that can be treated as the workspace for any feature engineering down the line. This holds the data transformed into a model that fits the problem domain in question. If you are working with data which is already formatted for the problem domain, it is reasonable to skip to this point.</p>"},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#missing-data-imputation","title":"Missing Data Imputation","text":"<ol> <li>How prevalent is the missing data?</li> <li>Is missing data random or does it have a pattern?</li> </ol> <pre><code>MeanMedianImputer: replaces missing data in numerical variables by the mean or median\nArbitraryNumberImputer: replaces missing data in numerical variables by an arbitrary number\nEndTailImputer: replaces missing data in numerical variables by numbers at the distribution tails\nCategoricalImputer: replaces missing data with an arbitrary string or by the most frequent category\nRandomSampleImputer: replaces missing data by random sampling observations from the variable\nAddMissingIndicator: adds a binary missing indicator to flag observations with missing data\nDropMissingData: removes observations (rows) containing missing values from dataframe\n</code></pre>"},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#outlier-analysis","title":"Outlier Analysis","text":"<ol> <li>Univariate and Multivariate Analysis </li> <li>PyOd , Alibi Detect and EDA Block</li> <li>Use Domain Knowledge to decide if you need to remove the outliers</li> <li>Treating for negative values, if any present depending on the data</li> </ol> <pre><code>ArbitraryOutlierCapper: caps maximum and minimum values at user defined values\nWinsorizer: caps maximum or minimum values using statistical parameters\nOutlierTrimmer: removes outliers from the dataset\n</code></pre>"},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#feature-layer","title":"Feature  Layer","text":"<p>Analytics specific data model(s) containing a set of features defined against the primary data, which are grouped by feature area of analysis and stored against a common dimension. In practice this covers the independent variables and target variable which will form the basis for ML exploration and application.  'Feature Stores' provide a versioned, centralised storage location with low-latency serving. </p>"},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#tips-on-creating-features","title":"Tips on Creating Features","text":"<p>It's good to keep in mind your model's own strengths and weaknesses when creating features. Here are some guidelines:</p> <ol> <li>Linear models learn sums and differences naturally, but can't learn anything more complex.</li> <li>Ratios seem to be difficult for most models to learn. Ratio combinations often lead to some easy performance gains.</li> <li>Linear models and neural nets generally do better with normalized features. Neural nets especially need features scaled to values not too far from 0. Tree-based models (like random forests and XGBoost) can sometimes benefit from normalization, but usually much less so.</li> <li>Tree models can learn to approximate almost any combination of features, but when a combination is especially important they can still benefit from having it explicitly created, especially when data is limited.</li> <li>Counts are especially helpful for tree models, since these models don't have a natural way of aggregating information across many features at once.</li> </ol>"},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#derived-feature-creation","title":"Derived Feature Creation","text":""},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#math-features","title":"Math Features","text":"<pre><code>MathFeatures: creates new variables by combining features with mathematical operations\nRelativeFeatures: combines variables with reference features\nCyclicalFeatures: creates variables using sine and cosine, suitable for cyclical features\n</code></pre>"},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#time-series-features","title":"Time series Features","text":"<pre><code>DatetimeFeatures: extract features from datetime variables\nLagFeatures: extract lag features\nWindowFeatures: create window features\nExpandingWindowFeatures: create expanding window features\n</code></pre>"},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#events-features","title":"Events Features","text":"<pre><code>Holidays\nDomain Seasonal Days/Weeks/Months\nDomain Peak Events\n</code></pre>"},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#location-distance-features","title":"Location - Distance Features","text":"<pre><code>Euclidean Distance, Manhattan Distance, Minkowski Distance\nLatitude Longitude\n</code></pre>"},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#feature-store-feast","title":"Feature Store : Feast","text":"<p>Register all the Features to a Feature Store like Feast</p>"},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#features-analysis","title":"Features Analysis","text":"<ol> <li>EDA Block</li> <li>DataPrep Report</li> <li>Spearman's rank correlation coefficient </li> <li>Pearson Product-Moment Coefficient </li> </ol>"},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#features-assumptions-test","title":"Features Assumptions Test :","text":"<p>According to Hair et al. (2013), four assumptions should be tested:</p> <ol> <li> <p>Check Normality:</p> <ol> <li>Ks Test : Find a distribution for the feature</li> <li>QQ Plot : Conform the distribution for the feature</li> <li>Box Cox Transforms / Yeo Johnson Transformations (Power Law Transforms)</li> <li>When we talk about normality what we mean is that the data should look like a normal distribution. This is important because several statistic tests rely on this (e.g. t-statistics). In this exercise we'll just check univariate normality for Target feature (which is a limited approach). Remember that univariate normality doesn't ensure multivariate normality (which is what we would like to have), but it helps. Another detail to take into account is that in big samples (&gt;200 observations) normality is not such an issue. However, if we solve normality, we avoid a lot of other problems (e.g. heteroscedacity) so that's the main reason why we are doing this analysis.</li> </ol> <p>Negative values and zero value don't allow log transformations so do one of the following</p> <pre><code>1. Create a binary feature with 0 for 0 values in original feature else 1 Then, proceed transformation on original feature with all the non-zero observations, ignoring those with value zero.\n2. Add a small value (1e-10) to all values allowing us to do the transform\n</code></pre> </li> <li> <p>Check Homoscedasticity:     &gt; Homoscedasticity refers to the 'assumption that dependent variable(s) exhibit equal levels of variance across the range of predictor variable(s)' (Hair et al., 2013). Homoscedasticity is desirable because we want the error term to be the same across all values of the independent variables.</p> </li> <li> <p>Check Linearity:     &gt; The most common way to assess linearity is to examine scatter plots and search for linear patterns. If patterns are not linear, it would be worthwhile to explore data transformations. However, we'll not get into this because most of the scatter plots we've seen appear to have linear relationships.</p> </li> <li> <p>Check Absence of correlated errors:     &gt; Correlated errors, like the definition suggests, happen when one error is correlated to another. For instance, if one positive error makes a negative error systematically, it means that there's a relationship between these variables. This occurs often in time series, where some patterns are time related. If you detect something, try to add a variable that can explain the effect you're getting. That's the most common solution for correlated errors.</p> </li> </ol>"},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#model-input-layer","title":"Model input  Layer","text":"<p>Analytics specific data model(s) containing all feature data against a common dimension and in the case of live projects against an analytics run date to ensure that you track the historical changes of the features over time. Many places call these the 'Master Table(s)', we believe this terminology is more precise and covers multi-models pipelines better.</p>"},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#feature-transformation","title":"Feature transformation","text":"<p>Here  we replace/transform the original feature with one or more features in order to make all features numerical or remove noise by clustering/ binning. Sometimes features are changed to introduce a Gaussian like feature. Or Create new dimension in feature by specific transforms like Fourier or by aggregation</p>"},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#categorical-encoding","title":"Categorical encoding","text":"<pre><code>OneHotEncoder: performs one hot encoding, optional: of popular categories\nCountFrequencyEncoder: replaces categories by the observation count or percentage\nOrdinalEncoder: replaces categories by numbers arbitrarily or ordered by target\nMeanEncoder: replaces categories by the target mean\nWoEEncoder: replaces categories by the weight of evidence\nPRatioEncoder: replaces categories by a ratio of probabilities\nDecisionTreeEncoder: replaces categories by predictions of a decision tree\nRareLabelEncoder: groups infrequent categories\n</code></pre>"},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#discretisation","title":"Discretisation","text":"<p>Cluster Labels as a Feature: Applied to a single real-valued feature, clustering acts like a traditional \"binning\" or \"discretization\" transform. Adding a feature of cluster labels can help machine learning models untangle complicated relationships of space or proximity. </p> <pre><code>ArbitraryDiscretiser: sorts variable into intervals defined by the user\nEqualFrequencyDiscretiser: sorts variable into equal frequency intervals\nEqualWidthDiscretiser: sorts variable into equal width intervals\nDecisionTreeDiscretiser: uses decision trees to create finite variables\n</code></pre>"},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#power-transforms","title":"Power Transforms","text":"<pre><code>LogTransformer: performs logarithmic transformation of numerical variables\nLogCpTransformer: performs logarithmic transformation after adding a constant value\nReciprocalTransformer: performs reciprocal transformation of numerical variables\nPowerTransformer: performs power transformation of numerical variables\nBoxCoxTransformer: performs Box-Cox transformation of numerical variables\nYeoJohnsonTransformer: performs Yeo-Johnson transformation of numerical variables\n</code></pre>"},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#fourier-features","title":"Fourier Features","text":""},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#aggregate-features","title":"Aggregate Features","text":""},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#generalization","title":"Generalization","text":""},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#preprocessing-transformations","title":"Preprocessing Transformations","text":"<ol> <li>Min-max normalization</li> <li>Z-Score normalization</li> <li>Decimal scaling normalization</li> </ol>"},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#target-encoding","title":"Target Encoding","text":"<ol> <li>A target encoding is any kind of encoding that replaces a feature's categories with some number derived from the target.</li> <li>High-cardinality features: A feature with a large number of categories can be troublesome to encode: a one-hot encoding would generate too many features and alternatives, like a label encoding, might not be appropriate for that feature. A target encoding derives numbers for the categories using the feature's most important property: its relationship with the target.</li> <li>Domain-motivated features: From prior experience, you might suspect that a categorical feature should be important even if it scored poorly with a feature metric. A target encoding can help reveal a feature's true informativeness.</li> </ol>"},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#feature-selection","title":"Feature selection","text":"<p>Feature Utility Metric, a function measuring associations between a feature and the target : Mutual information is a lot like correlation in that it measures a relationship between two quantities. The advantage of mutual information is that it can detect any kind of relationship, while correlation only detects linear relationships. Mutual information describes relationships in terms of uncertainty. The mutual information (MI) between two quantities is a measure of the extent to which knowledge of one quantity reduces uncertainty about the other. If you knew the value of a feature, how much more confident would you be about the target?</p> <pre><code>DropFeatures: drops an arbitrary subset of variables from a dataframe\nDropConstantFeatures: drops constant and quasi-constant variables from a dataframe\nDropDuplicateFeatures: drops duplicated variables from a dataframe\nDropCorrelatedFeatures: drops correlated variables from a dataframe\nSmartCorrelatedSelection: selects best features from correlated groups\nDropHighPSIFeatures: selects features based on the Population Stability Index (PSI)\nSelectByShuffling: selects features by evaluating model performance after feature shuffling\nSelectBySingleFeaturePerformance: selects features based on their performance on univariate estimators\nSelectByTargetMeanPerformance: selects features based on target mean encoding performance\nRecursiveFeatureElimination: selects features recursively, by evaluating model performance\nRecursiveFeatureAddition: selects features recursively, by evaluating model performance\n</code></pre>"},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#dimensionality-reduction","title":"Dimensionality Reduction","text":"<p>T-SNE  and PCA</p> <ol> <li>The first way is to use PCA as a descriptive technique. Since the components tell you about the variation, you could compute the MI scores for the components and see what kind of variation is most predictive of your target. That could give you ideas for kinds of features to create -- a product or a ratio of two features. You could even try clustering on one or more of the high-scoring components.</li> <li>The second way is to use the components themselves as features. Because the components expose the variational structure of the data directly, they can often be more informative than the original features.</li> <li>Dimensionality reduction: When your features are highly redundant (multicollinear, specifically), PCA will partition out the redundancy into one or more near-zero variance components, which you can then drop since they will contain little or no information.</li> <li>Anomaly detection: Unusual variation, not apparent from the original features, will often show up in the low-variance components. These components could be highly informative in an anomaly or outlier detection task.</li> <li>Noise reduction: A collection of sensor readings will often share some common background noise. PCA can sometimes collect the (informative) signal into a smaller number of features while leaving the noise alone, thus boosting the signal-to-noise ratio.</li> <li>Decorrelation: Some ML algorithms struggle with highly-correlated features. PCA transforms correlated features into uncorrelated components, which could be easier for your algorithm to work with.</li> </ol>"},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#models-layer","title":"Models  Layer","text":"<p>Stored, serialised pre-trained machine learning models. In the simplest case, these are stored as something like a pickle file on a filesystem. More mature implementations would leverage MLOps frameworks that provide model serving such as MLFlow.</p>"},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#model-architecture","title":"Model Architecture","text":""},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#model-output-layer","title":"Model output  Layer","text":"<p>Analytics specific data model(s) containing the results generated by the model based on the model input data.</p>"},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#reporting-layer","title":"Reporting  Layer","text":"<p>Reporting data model(s) that are used to combine a set of primary, feature, model input and model output data used to drive the dashboard and the views constructed. Used for outputting analyses or modelling results that are often Ad Hoc or simply descriptive reports. It encapsulates and removes the need to define any blending or joining of data, improve performance and replacement of presentation layer without having to redefine the data models.</p> <ul> <li>Analyse Error histograms for each sub section of the data</li> <li>To create subsection <ul> <li>Use low cardinality categorical features  as it is</li> <li>Convert high cardinality categorical features  to the mean/probability of predicted values (regression/classification)</li> <li>Bin the real valued features using decision trees</li> </ul> </li> </ul>"},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#appendix-resources","title":"Appendix : Resources","text":"<ol> <li>EDA</li> <li>Feature Engine Kaggle</li> <li>category-encoders</li> <li>practical-code-implementations of FE </li> <li>Python Libraries:<ol> <li>pandera</li> <li>kedro</li> <li>feature-engine</li> <li>mlxtend</li> <li>mlflow</li> </ol> </li> <li>Kedro Layering Principles</li> <li>Preprocessing</li> <li>EDA dataviz</li> <li>MLOPS</li> </ol>"},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#appendix-eda-block","title":"Appendix : EDA Block","text":""},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#univariate-study","title":"Univariate study","text":"<ol> <li> <p>Categorical Features</p> <ol> <li>catplot : Categorical estimate plots:<ol> <li>pointplot() (with kind=\"point\")</li> <li>barplot() (with kind=\"bar\") (metric=mean)</li> <li>countplot() (with kind=\"count\") (metric=count)</li> </ol> </li> </ol> </li> <li> <p>Continuous Features</p> <ol> <li>displot:<ol> <li>histplot() (with kind=\"hist\")</li> <li>kdeplot() (with kind=\"kde\")</li> <li>ecdfplot() (with kind=\"ecdf\")</li> </ol> </li> </ol> </li> </ol>"},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#multivariate-study","title":"Multivariate study","text":"<ol> <li> <p>Categorical Vs Continuous Features</p> <ol> <li>catplot : Categorical scatterplots:<ol> <li>stripplot() (with kind=\"strip\")</li> <li>swarmplot() (with kind=\"swarm\")</li> </ol> </li> <li>catplot : Categorical distribution plots:<ol> <li>boxplot() (with kind=\"box\")</li> <li>violinplot() (with kind=\"violin\")</li> <li>boxenplot() (with kind=\"boxen\")</li> </ol> </li> <li>catplot : Categorical estimate plots:<ol> <li>pointplot() (with kind=\"point\")</li> <li>barplot() (with kind=\"bar\") (metric=mean)</li> <li>countplot() (with kind=\"count\") (metric=count)</li> </ol> </li> </ol> </li> <li> <p>Continuous Vs Continuous Features</p> <ol> <li>relplot:<ol> <li>scatterplot() (with kind=\"scatter\"; the default)</li> <li>lineplot() (with kind=\"line\")</li> </ol> </li> <li>displot:<ol> <li>histplot() (with kind=\"hist\")</li> <li>kdeplot() (with kind=\"kde\")</li> </ol> </li> <li>Heatmap of Pivot Table</li> </ol> </li> <li> <p>Categorical Vs Categorical Features  </p> <ol> <li>catplot: Categorical estimate plots:<ol> <li>barplot() (with kind=\"bar\") (metric=mean)</li> <li>countplot() (with kind=\"count\") (metric=count)</li> </ol> </li> </ol> </li> </ol>"},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#unsupervised-study","title":"Unsupervised Study","text":"<ol> <li>K-means Clustering is a clustering method in unsupervised learning where data points are assigned into </li> <li>K groups, i.e. the number of clusters, based on the distance from each group\u2019s centroid.</li> </ol>"},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#refine-questions-hypotheses","title":"Refine questions &amp; hypotheses","text":"<p>Use what has been learned to refine questions &amp; hypotheses and generate new questions &amp; hypotheses </p>"},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#why-eda","title":"Why EDA","text":""},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#enabling-eda","title":"Enabling EDA","text":"<ul> <li>Suggest hypotheses about the causes of observed phenomena</li> <li>Assess assumptions on which statistical inference will be based</li> <li>Provide a basis for further data generation and collection through surveys, experiments and open sources</li> </ul>"},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#univariate-analysis-statistical-graphical","title":"Univariate Analysis : Statistical &amp; Graphical","text":"<p>Data is analyzed one variable at a time. Since it\u2019s a single variable, it doesn\u2019t deal with causes or  relationships. The main purpose of univariate analysis is to describe the data and find patterns that exist within it.</p> <ul> <li>Descriptive statistics are used to quantify the basic features of a sample distribution. They provide  simple summaries about the sample that can be used to make comparisons and draw preliminary conclusions. </li> <li>Stem-and-leaf plots show all data values and the shape of the distribution.</li> <li>Histograms, a bar plot in which each bar represents the frequency (count) or proportion (count/total count) of cases for a range of values.</li> <li>Box plots, which graphically depict the five-number summary of minimum, first quartile, median, third quartile, and maximum.</li> </ul>"},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#multivariate-analysis-statistical-graphical","title":"Multivariate Analysis : Statistical &amp; Graphical","text":"<p>Multivariate data arises from more than one variable and show the relationship between two or more  variables of the data through cross-tabulation, statistics and Graphics [display relationships]. </p> <ul> <li>Descriptive statistics are used to quantify the basic features of a sample distribution. They provide  simple summaries about the sample that can be used to make comparisons and draw preliminary conclusions. </li> <li>Grouped bar plot or bar chart with each group representing one level of one of the variables and each bar within a group representing the levels of the other variable.</li> <li>Scatter plot is used to plot data points on a horizontal and a vertical axis to show how much one variable is affected by another.</li> <li>Multivariate chart is a graphical representation of the relationships between factors and a response.</li> <li>Run chart is a line graph of data plotted over time.</li> <li>Bubble chart is a data visualization that displays multiple circles (bubbles) in a two-dimensional plot.</li> <li>Heat map is a graphical representation of data where values are depicted by color.</li> </ul>"},{"location":"Insights/Evolving%20Insights/2022-12-14%20Quick%20Steps%20Pipeline/#appendix-images","title":"Appendix : Images","text":""},{"location":"Insights/Evolving%20Insights/2022-12-15%20Training%20a%20Neural%20Network/","title":"Training a Neural Network","text":""},{"location":"Insights/Evolving%20Insights/2022-12-15%20Training%20a%20Neural%20Network/#general-advice","title":"General advice","text":"<p>General advice while training neural networks : </p> <ul> <li>Increase complexity one at a time and always provide a hypothesis on how should the model react to the said change in complexity. And verify if the hypothesis is True or False and why.</li> <li>Understand as many paterns as possible from the data using EDA and figure out which pattern is the model able to learn and why</li> <li>Before extensive experimentation get a complete pipeline executed for small data and small model.  The pipeline should be from getting raw data to finalising the techincal and business reports</li> <li>Set up a full training + evaluation + testing skeleton and gain trust in the pipeline execution via a series of experiments with explicit hypotheses, losses and metrics visualizations, model predictions. </li> </ul>"},{"location":"Insights/Evolving%20Insights/2022-12-15%20Training%20a%20Neural%20Network/#data-decisions","title":"Data Decisions","text":""},{"location":"Insights/Evolving%20Insights/2022-12-15%20Training%20a%20Neural%20Network/#feature-engineering","title":"Feature Engineering","text":"<ul> <li>Select features and remove any that may contain patterns that won't generalize beyond the training set and cause overfitting</li> <li>Scaling your features: For faster convergence all  features should have a similar scale before using them as inputs to the neural network.  </li> </ul>"},{"location":"Insights/Evolving%20Insights/2022-12-15%20Training%20a%20Neural%20Network/#network-architechture-decisions","title":"Network Architechture Decisions","text":""},{"location":"Insights/Evolving%20Insights/2022-12-15%20Training%20a%20Neural%20Network/#input-neurons","title":"Input neurons","text":"<ul> <li>For tabular dataset input vector is a combination of one input neuron per feature, So the shape of the input vector is the number of features selected for training</li> <li>For images dataset input vector is the dimensions of your image</li> <li>For text dataset input vector is decided by the text to vector embedding</li> </ul>"},{"location":"Insights/Evolving%20Insights/2022-12-15%20Training%20a%20Neural%20Network/#output-neurons","title":"Output neurons","text":"<p>Number of output neurons depends on the type and number of predictions - For single regression target this is a one value - For multi-variate regression, it is one neuron per predicted value, For bounding boxes we have 4 regression values one for each bounding box property : height, width, x-coordinate, y-coordinate - For binary classification we use one output neuron which represents the probability of the positive class.  - For multi class classification, we have one output neuron per class, and use the softmax activation function on the output layer to ensure the final probabilities sum to 1. </p>"},{"location":"Insights/Evolving%20Insights/2022-12-15%20Training%20a%20Neural%20Network/#hidden-layers","title":"Hidden Layers","text":"<ul> <li>No of hidden layers and no of nuerons per hidden layer: Decided by hit and trial, Start with either a too big or a too small network and adjust incrementally untill the model is neither overfitting nor underfitting.</li> <li>General Recommendation : Start with 1-5 layers and 1-100 neurons (same number of neurons for all hidden layers) and slowly adding more layers and neurons until you start overfitting. Usually we get more performance boost from adding more layers than adding more neurons in each layer.</li> <li>When chossing a smaller number of layers/neurons, the network will not be able to learn the underlying patterns in your data and thus be useless. An approach to counteract this is to start with a huge number of hidden layers and neurons and then use dropout and early stopping to let the neural network size itself down for you</li> <li>For many problems in image or speech domain there are pre-trained models (YOLO, ResNet, VGG) that allow you to use large parts of their networks, and train your model on top of these networks to learn only the higher order features. </li> <li>Manytimes having a large first layer and following it up with smaller layers will lead to better performance as the first layer can learn a lot of lower-level features that can feed into a few higher order features in the subsequent layers.</li> </ul>"},{"location":"Insights/Evolving%20Insights/2022-12-15%20Training%20a%20Neural%20Network/#skip-connections","title":"Skip Connections","text":""},{"location":"Insights/Evolving%20Insights/2022-12-15%20Training%20a%20Neural%20Network/#no-of-parameters","title":"No of Parameters","text":"<ul> <li>Calculate the number of parameters in the model and compare with the number of data points you have. (Just keep the comparision in mind).   </li> </ul>"},{"location":"Insights/Evolving%20Insights/2022-12-15%20Training%20a%20Neural%20Network/#activation-functions","title":"Activation Functions","text":"<ul> <li>Activation Functions: Comparison of Trends in Practice and Research for Deep Learning</li> <li>The performance from using different\u00a0hidden layer activations\u00a0improves in this order (from lowest\u2192highest performing): logistic \u2192 tanh \u2192 ReLU \u2192 Leaky ReLU \u2192 ELU \u2192 SELU. </li> <li>ReLU is the most popular activation function , But \u00a0ELU\u00a0or\u00a0GELU are on the rise</li> <li>To combat specific problems:<ul> <li>RReLU : To combat neural network overfitting</li> <li>PReLU: For massive training sets</li> <li>leaky ReLU: Reduce latency at runtime and fast inference times</li> <li>ELU: If your network doesn't self-normalize</li> <li>SELU: For an overall robust activation function</li> </ul> </li> <li>Regression output actiation <ul> <li>softplus : For positive prediction </li> <li>scale*tanh : For predictions in range  -scale and scale</li> <li>scale*sigmoid : For predictions in range  0 and scale</li> </ul> </li> </ul>"},{"location":"Insights/Evolving%20Insights/2022-12-15%20Training%20a%20Neural%20Network/#weight-initialization","title":"Weight Initialization","text":"<ul> <li>Weights should be small (not to small) (Normalisation)</li> <li>They should have good variance </li> </ul>"},{"location":"Insights/Evolving%20Insights/2022-12-15%20Training%20a%20Neural%20Network/#problem-of-symmetry","title":"Problem of Symmetry","text":"<ul> <li>If all weights are zero then for any given nueron is \\(w^Tx\\)  which would be 0, and all the weigths will have same gradient updates</li> <li>If all weights are one then for any given nueron is \\(w^Tx\\)  which would be \\(x\\), and all the weigths will have same gradient updates</li> </ul>"},{"location":"Insights/Evolving%20Insights/2022-12-15%20Training%20a%20Neural%20Network/#solution-is-asymmetry","title":"Solution is Asymmetry","text":"<ul> <li>With different initialisation different models lear different things about the data</li> <li>When using softmax, logistic, or tanh, use\u00a0Glorot initialization Mostly default used in Tensorflow </li> <li>When using ReLU or leaky RELU, use\u00a0He initialization</li> <li>When using SELU or ELU, use\u00a0LeCun initialization</li> </ul>"},{"location":"Insights/Evolving%20Insights/2022-12-15%20Training%20a%20Neural%20Network/#vanishing-and-exploding-gradients","title":"Vanishing and Exploding Gradients","text":"<ul> <li>Vanishing Gradients : when the backprop algorithm propagates the error gradient from the output layer to the first layers, the gradients get smaller and smaller until they're almost negligible when they reach the first layers. This means the weights of the first layers aren't updated significantly at each step.</li> <li>Exploding Gradients : when the gradients for certain layers get progressively larger, leading to massive weight updates for some layers as opposed to the others</li> </ul> <p>Question</p> <p>Where in the network would you first see the vanishing gradient or exploding gradients problem? Ans: Initial layers of the network</p>"},{"location":"Insights/Evolving%20Insights/2022-12-15%20Training%20a%20Neural%20Network/#norms","title":"Norms","text":"<ul> <li>A small change in input mini batch distribution can be noticed as a very large change in deeper layers (due to chain of change formulation). This phenomenon / problem is called internal covariance shift. BatchNorm solves the issue of internal covariance shift. </li> <li>BatchNorm simply learns the optimal means and scales of each layer's inputs. It does so by zero-centering and normalizing its input vectors, then scaling and shifting them. It also acts like a weak regularizer. Batch Norm is applied after calculating  \\(w^Tx\\) and before applying the activation</li> <li>Using BatchNorm lets us use larger learning rates (which result in faster convergence) and lead to huge improvements in most neural networks by reducing the vanishing gradients problem. The only downside is that it slightly increases training times because of the extra computations required at each layer.</li> <li>Reference </li> </ul>"},{"location":"Insights/Evolving%20Insights/2022-12-15%20Training%20a%20Neural%20Network/#early-stopping","title":"Early Stopping","text":"<ul> <li>Early Stopping lets you live it up by training a model with more hidden layers, hidden neurons and for more epochs than you need, and just stopping training when performance stops improving consecutively for n epochs. It also saves the best performing model for you.</li> </ul>"},{"location":"Insights/Evolving%20Insights/2022-12-15%20Training%20a%20Neural%20Network/#dropout","title":"Dropout","text":"<ul> <li> <p>Dropout gives you a massive performance boost (~2% for state-of-the-art models). All dropout does is randomly turn off a percentage of neurons at each layer, at each training step. This makes the network more robust because it can't rely on any particular set of input neurons for making predictions. The knowledge is distributed amongst the whole network. </p> </li> <li> <p>In Dropout around 2^n (where n is the number of neurons in the architecture) slightly-unique neural networks are generated during the  training process, and ensembled together to make predictions.</p> </li> <li> <p>A good Dropout Rate is between 0.1 to 0.5; 0.3 for RNNs, and 0.5 for CNNs. Use larger rates for bigger layers. Increasing the dropout rate decreases overfitting, and decreasing the rate is helpful to combat under-fitting.</p> </li> <li> <p>Read\u00a0Understanding the Disharmony between Dropout and Batch Normalization by Variance Shift\u00a0before using Dropout in conjunction with BatchNorm.</p> </li> </ul>"},{"location":"Insights/Evolving%20Insights/2022-12-15%20Training%20a%20Neural%20Network/#gradient-clipping","title":"Gradient Clipping","text":"<ul> <li>Gradient Clipping\u00a0is a great way to reduce gradients from exploding, specially when training RNNs. Simply clip them when they exceed a certain value. Use clip_by_global_norm  instead of clipnorm or clipvalue, which allows you to keep the direction of your gradient vector consistent.</li> <li> <p>tf.clip_by_value \u00a0clips each value inside one tensor, regardless of the other values in the tensor. For instance,</p> <pre><code># Only the values below 0 or above 3 are changed\ntf.clip_by_value([-1, 2, 10], 0, 3)  -&gt; [0, 2, 3]\n</code></pre> <p>Consequently, it can change the direction of the tensor, so it should be used if the values in the tensor are decorrelated one from another (which is not the case for gradient clipping), or to avoid zero / infinite values in a tensor that could lead to Nan / infinite values elsewhere (by clipping with a minimum of epsilon=1e-8 and a very big max value for instance).</p> </li> <li> <p>tf.clip_by_norm rescales one tensor if necessary, so that its L2 norm does not exceed a certain threshold. It's useful typically to avoid exploding gradient on one tensor, because you keep the gradient direction. For instance:   <pre><code># The original L2 norm is 7, which is &gt;5, so the final one is 5\ntf.clip_by_norm([-2, 3, 6], 5)  -&gt; [-2, 3, 6]*5/7  \n# The original L2 norm is 7, which is &lt;9, so it is left unchanged\ntf.clip_by_norm([-2, 3, 6], 9)  -&gt; [-2, 3, 6]  \n</code></pre></p> <p>However,\u00a0<code>clip_by_norm</code>\u00a0works on only one gradient, so if you use it on all your gradient tensors, you'll unbalance them (some will be rescaled, others not, and not all with the same scale).</p> </li> <li> <p>tf.clip_by_global_norm rescales a list of tensors so that the total norm of the vector of all their norms does not exceed a threshold. The goal is the same as\u00a0\u00a0<code>clip_by_norm</code>\u00a0(avoid exploding gradient, keep the gradient directions), but it works on all the gradients at once rather than on each one separately (that is, all of them are rescaled by the same factor if necessary, or none of them are rescaled). This is better, because the balance between the different gradients is maintained. For instance:\u00a0   <pre><code>tf.clip_by_global_norm([tf.constant([-2, 3, 6]),tf.constant([-4, 6, 12])] , 14.5)\n</code></pre></p> <p>will rescale both tensors by a factor\u00a0<code>14.5/sqrt(49 + 196)</code>, because the first tensor has a L2 norm of 7, the second one 14, and\u00a0<code>sqrt(7^2+ 14^2)&gt;14.5</code></p> <p>Choosing the max value is the hardest part. You should use the biggest value such that you don't have exploding gradient (whose effects can be\u00a0<code>Nan</code>s or\u00a0<code>infinite</code>\u00a0values appearing in your tensors, constant loss /accuracy after a few training steps). The value should be bigger for\u00a0<code>tf.clip_by_global_norm</code>\u00a0than for the others, since the global L2 norm will be mechanically bigger than the other ones due to the number of tensors implied.</p> </li> </ul> <p>Note</p> <p>Note that <code>tf.clip_by_value</code> and <code>tf.clip_by_norm</code>  work on only one tensor, while <code>tf.clip_by_global_norm</code> is used on a list of tensors.</p>"},{"location":"Insights/Evolving%20Insights/2022-12-15%20Training%20a%20Neural%20Network/#optimisation-and-training-decisions","title":"Optimisation and Training Decisions","text":"<ul> <li>Follow the advices in https://deci.ai/blog/tricks-training-neural-networks/</li> </ul>"},{"location":"Insights/Evolving%20Insights/2022-12-15%20Training%20a%20Neural%20Network/#optimizers","title":"Optimizers","text":"<ul> <li>Blog : SebastianRuder optimization for Deep Learning | Lecture Notes</li> <li>Adagrad's main benefits is that it eliminates the need to manually tune the learning rate </li> <li>Adaptive Moments Estimaion : Adam/Nadam are usually good starting points, and tend to be quite forgiving to a bad learning late and other non-optimal hyperparameters.</li> <li>Use Stochastic Gradient Descent if you care deeply about quality of convergence and if time is not of the essence.</li> <li>If you care about time-to-convergence and a point close to optimal convergence will suffice, experiment with Adam, Nadam, RMSProp, and Adamax optimizers</li> <li>Batch vs Mini-batch vs Stochastic Gradient Descent </li> <li>SGD can get stuck in a Saddle point,  Moment based indicators generally do better! Reference</li> <li></li> </ul> <p>Question</p> <ul> <li>Gradients of loss with respect to weights can be 0 for 3 points : Minima, Maxima and Saddle point! How do you know where you are? </li> <li>Why do sparse features need a different learning rate from dense features? And how Adagrad, Adadelta, RMSProp helps solve this? Reference</li> </ul>"},{"location":"Insights/Evolving%20Insights/2022-12-15%20Training%20a%20Neural%20Network/#batch-size","title":"Batch Size","text":"<ul> <li>Large batch sizes can be great because they can harness the power of GPUs to process more training instances per time.\u00a0OpenAI has found\u00a0larger batch size (of tens of thousands for image-classification and language modeling, and of millions in the case of RL agents) serve well for scaling and parallelizability.</li> <li>According to\u00a0this paper\u00a0by Masters and Luschi, the advantage gained from increased parallelism from running large batches is offset by the increased performance generalization and smaller memory footprint achieved by smaller batches. They show that increased batch sizes reduce the acceptable range of learning rates that provide stable convergence. Their takeaway is that smaller is, in-fact, better; and that the best performance is obtained by mini-batch sizes between 2 and 32.</li> <li>If you're not operating at massive scales, start with lower batch sizes and slowly increasing the size and monitoring performance to determine the best fit. </li> </ul>"},{"location":"Insights/Evolving%20Insights/2022-12-15%20Training%20a%20Neural%20Network/#learning-rate-and-momentum","title":"Learning Rate And Momentum","text":"<ul> <li>Use a constant medium learning rate until you've trained all other hyper-parameters and implement learning rate decay scheduling at the end.</li> <li>Start with a very low values (10^-6) and increase by a factor of 10 until it reaches a very high value (e.g. 10). Measure your model performance (vs the log of your learning rate) to determine which rate served you well for your problem. </li> <li>The best learning rate is usually half of the learning rate that causes the model to diverge.</li> <li>Learning Rate finder\u00a0method proposed by Leslie Smith. It an excellent way to find a good learning rate for most gradient optimizers (most variants of SGD) and works with most network architectures.</li> <li>With learning rate scheduling we can start with higher rates to move faster through gradient slopes, and slow it down when we reach a gradient valley in the hyper-parameter space which requires taking smaller steps.</li> <li>Gradient Descent takes tiny, consistent steps towards the local minima and when the gradients are tiny it can take a lot of time to converge. Momentum on the other hand takes into account the previous gradients, and accelerates convergence by pushing over valleys faster and avoiding local minima.</li> <li>In general you want your momentum value to be very close to one. 0.9 is a good place to start for smaller datasets, and you want to move progressively closer to one (0.999) the larger your dataset gets.</li> <li>Setting nesterov=True lets momentum take into account the gradient of the cost function a few steps ahead of the current point, which makes it slightly more accurate and faster.</li> </ul>"},{"location":"Insights/Evolving%20Insights/2022-12-15%20Training%20a%20Neural%20Network/#number-of-epochs","title":"Number of epochs","text":"<ul> <li>Start with a large number of epochs and use early stopping to halt training when performance stops improving.</li> </ul>"},{"location":"Insights/Evolving%20Insights/2022-12-15%20Training%20a%20Neural%20Network/#losses-and-metrics","title":"Losses and Metrics","text":"<p>Checkout the insight in Losses and Metrics</p>"},{"location":"Insights/Evolving%20Insights/2022-12-15%20Training%20a%20Neural%20Network/#references-and-good-articles","title":"References and Good Articles","text":"<ul> <li>Andrej Karpathy's guide </li> <li>EfficientNets</li> <li>A DISCIPLINED APPROACH TO NEURAL NETWORK HYPER-PARAMETERS </li> <li>Blog </li> <li>Stochastic Weight Averaging </li> <li>On the difficulty of training Recurrent Neural Networks</li> <li>kaggle</li> <li>Stack Overflow Discussion</li> <li>Images</li> <li>Gradient Descent | Denoise gradients using Exponential smoothing : Stochastic Gradient Descent with momentum</li> </ul>"},{"location":"Insights/Evolving%20Insights/2022-12-15%20Training%20a%20Neural%20Network/#transfer-learning","title":"Transfer Learning","text":"<ol> <li>If new data is very similar to previous data then : Don't fine tune anything : Use the pre trained model weights as feature engineering tool</li> <li>If new data is medium size and similar to previous data then : Fine tune the the a few last layers of the model with small learning </li> <li>If new data is large size and similar/dis-similar to previous data then : Fine tune the the complete model equivalently use the pre trained model weights as initialisation for the new model</li> <li>Transfer Learning</li> </ol>"},{"location":"Insights/Evolving%20Insights/2022-12-15%20Training%20a%20Neural%20Network/#convnets","title":"ConvNets","text":"<ol> <li>Padding : Zero Padding (popular) Vs Same value padding (Not used very much)</li> <li>Output dimension formula for Input Image of size \\(N \\times N\\), with padding \\(P\\) and stride \\(S\\)  is \\(floor(\\frac{N-2+2P}{S}) + 1\\)</li> <li>Depth-wise Convolution and Depth-wise Separable Convolution</li> <li>Data Augmentation : Improve types and quantity of Invariance (shift, rotation, zoom, shear, noise)  </li> <li>Max Pooling Subsampaling is used to solve these problems<ul> <li>Location Invariance</li> <li>Scale Invariance</li> <li>Rotation Invariance</li> </ul> </li> </ol> <p>Should the kerenel width and number of kernels increase or decrease as depth increases in CNNs ?</p> <ul> <li>In general keep the feature space wide (smaller size of filters <code>mxn</code>) and shallow (low number of filters) in the initial stages of the network,  and then make it narrower (bigger size of filters <code>mxn</code>) and deeper (high number of filters) towards the end.</li> <li>We want initial layers to learn simple features like edge detectors etc. There are a limited number of such features. Hence the low number of filters in the beginning.</li> <li>Each of the filters in the previous layers can be combined in exponential number of ways to detect complex patterns. For example, if previous layer has 4 filters, then next layer can learn to combine them in 2^n = 16 different ways.</li> <li>Not all of those ways will be exhaustively needed, so a rough rule of thumb is to double the number of filters as you increase layers because the number of complex patterns will be exponentially more than number of simple patterns to learn.</li> </ul>"},{"location":"Insights/Evolving%20Insights/2022-12-15%20Training%20a%20Neural%20Network/#refernces-and-questions","title":"Refernces and Questions","text":"<ul> <li>In CNNs, how we can be sure that all kernals are not learning same thing? (Ans: Random Initialisation)</li> <li>large_kernel</li> <li>Cnn ppt</li> <li>DL-Tutorial-NIPS2015</li> <li>cs231n Stanford CNN course</li> <li>ConvolutionalNeuralNetwork</li> <li>LeNet</li> <li>AlexNet paper,\u00a0AlexNet image</li> <li>ResNet : x -&gt; Conv -&gt; Relu -&gt; Conv -&gt; x1 || \u2190+ x || -&gt; Relu</li> <li>InceptionNet : Multple kernel output concatenations using 1x1 convolution hack</li> <li>LSTMs | LSTMs No of Parameters | GRU_1\u00a0 | GRU_2</li> <li>Why is padding done in LSTMs ? (To run the Optimisation on a batch and not on single example)</li> </ul>"},{"location":"Insights/Evolving%20Insights/2022-12-16%20Losses%20and%20Metrics%20In%20Machine%20Learning/","title":"Losses and Metrics In Machine Learning","text":""},{"location":"Insights/Evolving%20Insights/2022-12-16%20Losses%20and%20Metrics%20In%20Machine%20Learning/#reading-material","title":"Reading Material","text":"<ol> <li>Comprehensive Survey of Loss Functions in Machine Learning</li> <li>Classification<ol> <li>Study of deep learning loss functions for multi-label remote sensing image classification</li> <li>Recall Loss for Semantic Segmentation</li> <li>Focal Loss for Dense Object Detection</li> <li>Class Distance Weighted Cross-Entropy Loss</li> <li>Squared Earth Mover\u2019s Distance-based Loss for Training Deep Neural Networks</li> </ol> </li> <li>Regression<ol> <li>Regression Based Loss Functions for Time Series Forecasting</li> </ol> </li> </ol> <p>Note</p> <ul> <li>We can prove mathematically that linear regression, logistics regression &amp; all kinds of svms have a convex loss function hence only a single minima</li> <li>All deep learning models with 2 or more layers can have a non Convex loss function hence   more than one  minima</li> </ul>"},{"location":"Insights/Evolving%20Insights/2022-12-16%20Losses%20and%20Metrics%20In%20Machine%20Learning/#regression-losses","title":"Regression Losses","text":"Loss Function Use Case Advantages Disadvantages Mean Absolute Error (MAE) If your use case demands an error metric that treats all errors equally and returns a more interpretable value. MAE is computationally cheap because of its simplicity and provides an even measure of how well the model performs. MAE follows a linear scor- ing approach, which means that all errors are weighted equally when computing the mean. Because of the steepness of MAE, we may hop beyond the minima during backpropagation. MAE is less sensitive towards outliers MAE is not differentiable at zero, therefore it can be challenging to compute gradients. Mean Squared Error (MSE) If the outliers represent significant anomalies, your use case demands an error metric where outliers should be detected. MSE aids in the efficient convergence to minima for tiny mistakes as the gradient gradually decreases. Squaring the values accelerates the rate of training, but a higher loss value may result in a substantial leap during back propagation, which is undesirable. MSE values are expressed in quadratic equations, aids to penalizing model in case of outliers. MSE is especially sensitive to outliers, which means that significant outliers in data may influence our model performance. Mean Bias Error (MBE) The directionality of bias can be preserved with MBE. If you are simply concerned with a system\u2019s net or cumulative behavior, MBE can help you assess how well biases balance out. If you wish to identify and correct model bias, you should use MBE to determine the direction of the model (i.e., whether it is biased positively or negatively). MBE tends to err in one direction continuously, while attempting to anticipate traffic patterns. Given that the errors tend to cancel each other out, it is not a suitable loss function for numbers ranging from (\u2212\u221e, \u221e) Relative Absolute Error (RAE) RAE is a method to measure the performance of a predictive model. If you are simply concerned and want a metric that indicates and compares how well a model works. RAE can compare models where errors are measured in different units. If the reference forecast is equal to the ground truth, RAE can become undefinable, which is one of its key drawbacks. Relative Squared Error (RSE) RSE is not scale dependent. If your use case demands similarity between models, it can be used to compare models where errors are measured in different units. RSE is independent of scale. It can compare models when errors are measured in various units. RSE is not affected by the predictions\u2019 mean or size. Mean Absolute Percentage Error (MAPE) Since its error estimates are expressed in percentages. It is independent of the scale of the variables. MAPE can be used if your use case requires that all mistakes be normalized on a standard scale. MAPE Loss is computed by standardizing all errors on a single scale of hundred. As denominator of the MAPE equation is the predicted output, which can be zero leading to undefined value. As the error estimates are expressed in percentages, MAPE is independent of the scale of the variables. The issue of positive numbers canceling out negative ones is avoided since MAPE utilizes absolute percentage mistakes. Positive errors are penalized less by MAPE than negative ones. Therefore, it is biased when we compare the precision of prediction algorithms since it defaults to selecting one whose results are too low. Root Mean Squared Error (RMSE) We can utilize RMSE if your use case necessitates a computationally straightforward and easily differentiable loss( as many optimization techniques do). Also, it does not penalize errors as severely as MSE does. RMSE works as a training heuristic for models. Many optimization methods choose it because it is easily differ- entiable and computationally straightforward. As RMSE is still a linear scoring function, the gradient is abrupt around minima. Even with larger values, there are fewer extreme losses, and the square root causes RMSE to penalize errors less than MSE. The scale of data determines the RMSE, as the errors\u2019 magnitude grows, so does sensitivity to outliers. In order to converge the model the sensitivity must be reduced, leading to extra overhead to use RMSE. Mean Squared Logarithmic Error (MSLE) MSLE reduce the punishing effect of significant differences in large predicted values. When the model predicts unscaled quantities directly, it may be more appropriate as a loss measure. Treats small differences between small actual and predicted values the same as big differences between large actual and predicted values. MSLE penalizes underestimates more than overestimates. Root Mean Squared Logarithmic Error (RMSLE) RMSLE has a significant penalty for underestimation than overestimation. If your use case requires situations where we are not bothered by overestimation, but underestima- tion is not acceptable, we can use RMSL RMSLE is applicable at several scales and is not scaledependent. It is unaffected by significant outliers. Only the relative error between the actual value and the anticipated value is taken into account. Due to RMSLE\u2019s biased penalty, underestimating is penalized more severely than overestimation. Normalized Root Mean Squared Error (NRMSE) When comparing models with different dependent variables or when the dependent variables are changed, NRMSE is a valuable measure (log-transformed or standardized). It eliminates scale dependence and allows for easier comparison of models of different scales or even datasets NRMSE overcomes the scale dependency and eases comparison between models of different scales or datasets. NRMSE loses the units associated with the response variable. Relative Root Mean Squared Error (RRMSE) While the scale of the original measurements limits RMSE, RRMSE can be used if your use case necessitates comparing different measurement techniques. Also, RRMSE expresses the error relatively or in a percentage form. RRMSE can be used to compare different measurement techniques. RRMSE can hide inaccuracy in experiment results Huber Loss Huber Loss curves around the minima, which decreases the gradient and is more robust to outliers while optimally penalizing the incorrect values. Linearity above the delta guarantees that outliers are given appropriate weightage (Not as extreme as in MSE). Addition of hyper parameter delta (\u03b4) allows flexibility to adapt to any distribution Huber loss is computationally expensive due to the additional conditionals and comparisons, especially if your dataset is huge. The curved form below the delta guarantees that the steps are the correct length during backpropagation. To achieve the best outcomes must be optimized, which raises training requirements. LogCosh Loss Logcosh works similarly to mean squared error but is less affected by the occasional wildly incorrect prediction. It has all of the benefits of Huber loss; unlike Huber loss, It is twice differentiable everywhere. As Log-cosh calculates the log of hyperbolic cosine of the error. Therefore, it has a considerable advantage over Huber loss for its\u2019 property of continuity and differentiability. It is less adaptable than Huber since it operates on a fixed scale (no \u03b4). There are fewer computations required compared with Huber. The derivation is more challenging than Huber loss and necessitates more research. Quantile Loss Use Quantile Loss when predicting an interval instead of point estimates. Quantile Loss can also be used to calculate prediction intervals in neural nets and tree based models, and It is robust to outliers. It is robust to outliers. Quantile loss is computationally intensive. It is beneficial for making in- terval predictions as opposed to point estimates. This func- tion can also be used in neural networks and tree based models to determine prediction intervals. Quantile loss will be worse if we estimate the mean or use a squared loss to quantify efficiency <p>Issue with MAPE &amp; Metrics which solve it</p> <p>MAPE puts a heavier penalty on negative errors than on positive errors as stated in Accuracy measures: theoretical and practical concerns - REF As a consequence, when MAPE is used to compare the accuracy of prediction methods it is biased in that it will systematically select a method whose forecasts are too low. This issue can be overcome by using an accuracy measure based on the logarithm of the accuracy ratio (the ratio of the predicted to actual value). This leads to superior statistical properties and also leads to predictions which can be interpreted in terms of the geometric mean.</p> <ul> <li>Mean Absolute Scaled Error\u00a0(MASE) , </li> <li>Symmetric Mean Absolute Percentage Error\u00a0(sMAPE)</li> <li>Mean Directional Accuracy (MDA)</li> <li>Mean Arctangent Absolute Percentage Error (MAAPE): MAAPE can be considered a\u00a0slope as an angle, while MAPE is a\u00a0slope as a ratio</li> </ul>"},{"location":"Insights/Evolving%20Insights/2022-12-16%20Losses%20and%20Metrics%20In%20Machine%20Learning/#classification-losses","title":"Classification Losses","text":"<ul> <li>Multi Class Log-Loss  or Categorical Cross Entropy </li> </ul> \\[- \\frac{1}{n}\\sum_{i=1}^n \\sum_{j=1}^C y_{ij}' log(p_{ij})\\] <ul> <li>RecallCE Loss</li> </ul> \\[RecallCE \\ Loss = -\\sum_{c=1}^C(1-R_{c,t})*log(p_{n,t})  \\ = \\  -\\sum_{c=1}^C(1-\\frac{TP_{c,t}}{FN_{c,t}+TP_{c,t}})*N_clog(p^{c,t}) \\] <p>where \\(R_{c,t}\\) is the recall for class c at optimization step t.</p>"},{"location":"Insights/Evolving%20Insights/2022-12-16%20Losses%20and%20Metrics%20In%20Machine%20Learning/#classification-metrics","title":"Classification Metrics","text":""},{"location":"Insights/Evolving%20Insights/2022-12-16%20Losses%20and%20Metrics%20In%20Machine%20Learning/#accuracy-precision-recall-f1-score","title":"Accuracy, Precision, Recall, F1-score","text":"<ul> <li>Accuracy : What fraction of predictions were correct ?</li> <li>Precision : What fraction of positive identifications were actually correct ?</li> <li>Recall : What fraction of actual positives were identified correctly ?</li> </ul> <p>Real Understanding</p> <p>Precision and Recall are often in working against each other  that is, improving precision typically reduces recall and vice versa. Try imagining a logistic regression model for a binary classification task and calculate Precision &amp; Recall for different classification threshold. To understand this in more detail Schedule a Call</p> <ul> <li>F1 score : The\u00a0F1\u00a0score is the\u00a0harmonic mean\u00a0of the precision and recall.</li> </ul> <p>Real Understanding</p> <p>The highest possible value of an F-score is 1.0, indicating perfect precision and recall, and the lowest possible value is 0, if either precision or recall are zero. To understand this in more detail Schedule a Call</p> <ul> <li>F\u03b2\u00a0score : To consider recall \u03b2 times as important as precision a modified  </li> <li>that uses a positive real factor\u00a0\u03b2, where\u00a0\u03b2\u00a0is chosen such that recall is considered\u00a0\u03b2 times as important as precision, is:</li> </ul>"},{"location":"Insights/Evolving%20Insights/2022-12-16%20Losses%20and%20Metrics%20In%20Machine%20Learning/#roc","title":"ROC","text":""},{"location":"Insights/Evolving%20Insights/2022-12-16%20Losses%20and%20Metrics%20In%20Machine%20Learning/#auc","title":"AUC","text":""},{"location":"Insights/Evolving%20Insights/2022-12-16%20Losses%20and%20Metrics%20In%20Machine%20Learning/#ranking-metrics","title":"Ranking Metrics","text":""},{"location":"Insights/Evolving%20Insights/2022-12-16%20Losses%20and%20Metrics%20In%20Machine%20Learning/#mrr","title":"MRR","text":""},{"location":"Insights/Evolving%20Insights/2022-12-16%20Losses%20and%20Metrics%20In%20Machine%20Learning/#dcg","title":"DCG","text":""},{"location":"Insights/Evolving%20Insights/2022-12-16%20Losses%20and%20Metrics%20In%20Machine%20Learning/#ndcg","title":"NDCG","text":""},{"location":"Insights/Evolving%20Insights/2022-12-16%20Losses%20and%20Metrics%20In%20Machine%20Learning/#statistical-metrics","title":"Statistical Metrics","text":""},{"location":"Insights/Evolving%20Insights/2022-12-16%20Losses%20and%20Metrics%20In%20Machine%20Learning/#correlation","title":"Correlation","text":""},{"location":"Insights/Evolving%20Insights/2022-12-16%20Losses%20and%20Metrics%20In%20Machine%20Learning/#computer-vision-metrics","title":"Computer Vision Metrics","text":""},{"location":"Insights/Evolving%20Insights/2022-12-16%20Losses%20and%20Metrics%20In%20Machine%20Learning/#psnr","title":"PSNR","text":""},{"location":"Insights/Evolving%20Insights/2022-12-16%20Losses%20and%20Metrics%20In%20Machine%20Learning/#ssim","title":"SSIM","text":""},{"location":"Insights/Evolving%20Insights/2022-12-16%20Losses%20and%20Metrics%20In%20Machine%20Learning/#iou","title":"IoU","text":""},{"location":"Insights/Evolving%20Insights/2022-12-16%20Losses%20and%20Metrics%20In%20Machine%20Learning/#nlp-metrics","title":"NLP Metrics","text":""},{"location":"Insights/Evolving%20Insights/2022-12-16%20Losses%20and%20Metrics%20In%20Machine%20Learning/#perplexity","title":"Perplexity","text":""},{"location":"Insights/Evolving%20Insights/2022-12-16%20Losses%20and%20Metrics%20In%20Machine%20Learning/#bleu-score","title":"BLEU score","text":""},{"location":"Insights/Evolving%20Insights/2022-12-16%20Losses%20and%20Metrics%20In%20Machine%20Learning/#deep-learning-related-metrics","title":"Deep Learning Related Metrics","text":""},{"location":"Insights/Evolving%20Insights/2022-12-16%20Losses%20and%20Metrics%20In%20Machine%20Learning/#inception-score","title":"Inception score","text":""},{"location":"Insights/Evolving%20Insights/2022-12-16%20Losses%20and%20Metrics%20In%20Machine%20Learning/#frechet-inception-distance","title":"Frechet Inception distance","text":""},{"location":"Insights/Evolving%20Insights/2022-12-16%20Losses%20and%20Metrics%20In%20Machine%20Learning/#table-of-confusion","title":"Table of Confusion","text":""},{"location":"Insights/Evolving%20Insights/2022-12-16%20Losses%20and%20Metrics%20In%20Machine%20Learning/#literally-very-confusing","title":"Literally very confusing","text":"<ul> <li>Condition positive (P) : the number of real positive cases in the data</li> <li>Condition negative (N) : the number of real negative cases in the data</li> <li>True Positive (TP) : A test result that correctly indicates the presence of a condition</li> <li>True Negative (TN) : A test result that correctly indicates the absence of a condition</li> <li>False Positive (FP) : A test result which wrongly indicates that a particular condition is present</li> <li>False Negative (FN) : A test result which wrongly indicates that a particular condition is absent</li> </ul>"},{"location":"Insights/Evolving%20Insights/2022-12-16%20Losses%20and%20Metrics%20In%20Machine%20Learning/#some-basic-formula","title":"Some basic formula","text":"sensitivity,\u00a0recall,\u00a0hit rate, or\u00a0true positive rate\u00a0(TPR) specificity,\u00a0selectivity\u00a0or\u00a0true negative rate\u00a0(TNR) precision\u00a0or\u00a0positive predictive value\u00a0(PPV) negative predictive value\u00a0(NPV) miss rate or\u00a0false negative rate\u00a0(FNR) fall-out\u00a0or\u00a0false positive rate\u00a0(FPR) false discovery rate\u00a0(FDR) false omission rate\u00a0(FOR) Positive likelihood ratio\u00a0(LR+) Negative likelihood ratio\u00a0(LR-) prevalence threshold\u00a0(PT) threat score (TS) or critical success index (CSI) Prevalence accuracy\u00a0(ACC) balanced accuracy (BA) F1 score is the\u00a0harmonic mean\u00a0of\u00a0precision\u00a0and\u00a0sensitivity phi coefficient\u00a0(\u03c6 or r\u03c6) or\u00a0Matthews correlation coefficient\u00a0(MCC) Fowlkes\u2013Mallows index\u00a0(FM) informedness\u00a0or bookmaker informedness (BM) markedness\u00a0(MK) or deltaP (\u0394p) Diagnostic odds ratio\u00a0(DOR) Confusion Matrix Reference"},{"location":"Insights/Evolving%20Insights/2022-12-16%20Losses%20and%20Metrics%20In%20Machine%20Learning/#references","title":"References","text":"<ul> <li>Focal Loss</li> </ul>"},{"location":"Insights/Evolving%20Insights/2022-12-17%20Research%20Papers%2C%20Blogs%20%26%20Resources/","title":"Research Papers, Blogs & Resources","text":"","tags":["Data Science","Machine Learning","Deep Learning"]},{"location":"Insights/Evolving%20Insights/2022-12-17%20Research%20Papers%2C%20Blogs%20%26%20Resources/#research-papers-and-blogs","title":"Research Papers and Blogs","text":"","tags":["Data Science","Machine Learning","Deep Learning"]},{"location":"Insights/Evolving%20Insights/2022-12-17%20Research%20Papers%2C%20Blogs%20%26%20Resources/#graph-based-method","title":"Graph Based Method","text":"<ol> <li>DeepWalk: Online Learning of Social Representations</li> <li>HOPE : Asymmetric Transitivity Preserving Graph Embedding</li> <li>Feature Extraction for Graphs</li> <li>A overview of FE in Graphs</li> </ol>","tags":["Data Science","Machine Learning","Deep Learning"]},{"location":"Insights/Evolving%20Insights/2022-12-17%20Research%20Papers%2C%20Blogs%20%26%20Resources/#discipline-the-method","title":"Discipline the method","text":"<ol> <li>A DISCIPLINED APPROACH TO NEURAL NETWORK HYPER-PARAMETERS: PART 1 \u2013 LEARNING RATE, BATCH SIZE, MOMENTUM, AND WEIGHT DECAY</li> </ol>","tags":["Data Science","Machine Learning","Deep Learning"]},{"location":"Insights/Evolving%20Insights/2022-12-17%20Research%20Papers%2C%20Blogs%20%26%20Resources/#loss-functions","title":"Loss Functions","text":"<ol> <li>Comprehensive Survey of Loss Functions in Machine Learning</li> <li>Classification<ol> <li>Study of deep learning loss functions for multi-label remote sensing image classification</li> <li>Recall Loss for Semantic Segmentation</li> <li>Focal Loss for Dense Object Detection</li> <li>Class Distance Weighted Cross-Entropy Loss</li> <li>Squared Earth Mover\u2019s Distance-based Loss for Training Deep Neural Networks</li> </ol> </li> <li>Regression<ol> <li>Regression Based Loss Functions for Time Series Forecasting</li> </ol> </li> </ol>","tags":["Data Science","Machine Learning","Deep Learning"]},{"location":"Insights/Evolving%20Insights/2022-12-17%20Research%20Papers%2C%20Blogs%20%26%20Resources/#tech-blogs","title":"Tech Blogs","text":"<ol> <li>AirBnB Engineering</li> <li>Spotify Research</li> <li>Netflix Research</li> <li>DoorDash ML Blog</li> <li>Uber Engineering</li> <li>Lyft Engineering</li> <li>Shopify Engineering</li> <li>Meta Engineering</li> <li>LinkedIn Engineering </li> <li>Kaggle Competition Blog</li> </ol>","tags":["Data Science","Machine Learning","Deep Learning"]},{"location":"Insights/Evolving%20Insights/2022-12-17%20Research%20Papers%2C%20Blogs%20%26%20Resources/#knowledge-distillation","title":"Knowledge Distillation","text":"<ol> <li>https://arxiv.org/pdf/2006.05525.pdf </li> <li>https://arxiv.org/pdf/1503.02531.pdf </li> <li>https://arxiv.org/pdf/1910.01108.pdf</li> </ol>","tags":["Data Science","Machine Learning","Deep Learning"]},{"location":"Insights/Evolving%20Insights/2022-12-17%20Research%20Papers%2C%20Blogs%20%26%20Resources/#modalities-and-mixture-of-experts","title":"Modalities and Mixture of Experts","text":"<ul> <li>Language-Image Mixture of Experts</li> <li>Google AI blog limoe-learning-multiple-modalities-with</li> <li>Youtube</li> </ul>","tags":["Data Science","Machine Learning","Deep Learning"]},{"location":"Insights/Evolving%20Insights/2022-12-17%20Research%20Papers%2C%20Blogs%20%26%20Resources/#deep-learning-for-tabular-data","title":"Deep learning for Tabular data","text":"<ul> <li>Revisiting Deep Learning Models for Tabular Data</li> <li>wnadb.ai</li> </ul>","tags":["Data Science","Machine Learning","Deep Learning"]},{"location":"Insights/Evolving%20Insights/2022-12-17%20Research%20Papers%2C%20Blogs%20%26%20Resources/#embeddings","title":"Embeddings","text":"<ul> <li>Time2Vec: Learning a Vector Representation of Time\u00a0</li> </ul>","tags":["Data Science","Machine Learning","Deep Learning"]},{"location":"Insights/Evolving%20Insights/2022-12-17%20Research%20Papers%2C%20Blogs%20%26%20Resources/#gans","title":"GANs","text":"<ol> <li>GAN = Generative model + Adversarial model (This model judges the Generative model)</li> <li>GAN tricks and Hacks: https://github.com/soumith/ganhacks</li> <li>https://medium.com/@jonathan_hui/gan-some-cool-applications-of-gans-4c9ecca35900</li> <li>[MNIST GAN](https://medium.com/datadriveninvestor/generative-adversarial-network-gan-using-keras-ce1c05cfdfd3</li> <li>deepgenerativemodels research</li> <li>https://medium.com/@sanjay035/sketch-to-color-anime-translation-using-generative-adversarial-networks-gans-8f4f69594aeb</li> </ol>","tags":["Data Science","Machine Learning","Deep Learning"]},{"location":"Insights/Evolving%20Insights/2022-12-17%20Research%20Papers%2C%20Blogs%20%26%20Resources/#encoder-decoders","title":"Encoder Decoders","text":"<ol> <li>Sequence to Sequence Learning with Neural Networks</li> <li>Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation</li> <li>Deep Visual-Semantic Alignments for Generating Image Descriptions</li> <li>https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html</li> <li>https://ai.googleblog.com/2018/05/smart-compose-using-neural-networks-to.html</li> <li>https://medium.com/@martin.monperrus/sequence-to-sequence-learning-program-repair-e39dc5c0119b</li> <li>https://towardsdatascience.com/image-captioning-with-keras-teaching-computers-to-describe-pictures-c88a46a311b8</li> <li>http://www.manythings.org/anki/</li> <li>https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py</li> </ol>","tags":["Data Science","Machine Learning","Deep Learning"]},{"location":"Insights/Evolving%20Insights/2022-12-17%20Research%20Papers%2C%20Blogs%20%26%20Resources/#autoencoders","title":"AutoEncoders","text":"<ul> <li>http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/</li> <li>https://en.wikipedia.org/wiki/Autoencoder</li> </ul>","tags":["Data Science","Machine Learning","Deep Learning"]},{"location":"Insights/Evolving%20Insights/2022-12-17%20Research%20Papers%2C%20Blogs%20%26%20Resources/#attention","title":"Attention","text":"<ol> <li>NEURAL MACHINE TRANSLATION</li> <li>Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</li> <li>Attention in Deep Networks with Keras</li> <li>Tx is a hyperparam, explained in a 2015 paper 1508.04025\u00a0 and not in the original 2014 Attention Models paper 1409.0473.pdf. In the 2014 paper, Tx is the length of the whole input sentence.  </li> </ol>","tags":["Data Science","Machine Learning","Deep Learning"]},{"location":"Insights/Evolving%20Insights/2022-12-17%20Research%20Papers%2C%20Blogs%20%26%20Resources/#transformers","title":"Transformers","text":"<ol> <li>https://jalammar.github.io/illustrated-transformer/</li> <li>TabNet Transformer</li> <li>Pytorch TabNet Youtube</li> </ol>","tags":["Data Science","Machine Learning","Deep Learning"]},{"location":"Insights/Evolving%20Insights/2022-12-17%20Research%20Papers%2C%20Blogs%20%26%20Resources/#explainanle-ai","title":"Explainanle AI","text":"<ol> <li>Explaining the Predictions of Any Classifier</li> <li>Integrated Gradients</li> <li>Robustness of Interpretability Methods</li> <li>Interpretable Machine Learning Web Book</li> <li>LIME TDS 1 | LIME Blog | LIME Text Explain</li> </ol>","tags":["Data Science","Machine Learning","Deep Learning"]},{"location":"Insights/Evolving%20Insights/2022-12-17%20Research%20Papers%2C%20Blogs%20%26%20Resources/#siamese-networks","title":"Siamese Networks","text":"<ul> <li>Intro</li> </ul>","tags":["Data Science","Machine Learning","Deep Learning"]},{"location":"Insights/Evolving%20Insights/2022-12-17%20Research%20Papers%2C%20Blogs%20%26%20Resources/#python-libraries","title":"Python Libraries","text":"","tags":["Data Science","Machine Learning","Deep Learning"]},{"location":"Insights/Evolving%20Insights/2022-12-17%20Research%20Papers%2C%20Blogs%20%26%20Resources/#graph-analysis","title":"Graph Analysis","text":"<ul> <li>networkx</li> </ul>","tags":["Data Science","Machine Learning","Deep Learning"]},{"location":"Insights/Evolving%20Insights/2022-12-17%20Research%20Papers%2C%20Blogs%20%26%20Resources/#web-developement","title":"Web Developement","text":"<ul> <li>pyweb</li> <li>anvil.works</li> <li>pynecone</li> <li>streamlit</li> </ul>","tags":["Data Science","Machine Learning","Deep Learning"]},{"location":"Insights/Evolving%20Insights/2022-12-17%20Research%20Papers%2C%20Blogs%20%26%20Resources/#explainable-ai","title":"Explainable AI","text":"<ul> <li>explainerdashboard</li> <li>omnixai</li> <li>InterpretML</li> <li>ELI5</li> <li>Shapash</li> <li>LIME</li> <li>SHAP</li> </ul>","tags":["Data Science","Machine Learning","Deep Learning"]},{"location":"Insights/Evolving%20Insights/2022-12-17%20Research%20Papers%2C%20Blogs%20%26%20Resources/#trading","title":"Trading","text":"<ul> <li>quantstats</li> <li>alphalens-reloaded</li> <li>alphalens-reloaded</li> <li>tsfresh</li> <li>scalecast</li> </ul>","tags":["Data Science","Machine Learning","Deep Learning"]},{"location":"Insights/Evolving%20Insights/2022-12-17%20Research%20Papers%2C%20Blogs%20%26%20Resources/#github-pages","title":"GitHub pages","text":"<ul> <li>ml-tooling/best-of-python</li> <li>ml-tooling/best-of-ml-python</li> <li>ml-tooling/ml-workspace</li> </ul>","tags":["Data Science","Machine Learning","Deep Learning"]},{"location":"Insights/Evolving%20Insights/2022-12-17%20Research%20Papers%2C%20Blogs%20%26%20Resources/#other-resourses","title":"Other Resourses","text":"<ul> <li>https://mlops.toys/data-versioning</li> <li>https://madewithml.com/courses/mlops/labeling/</li> <li>https://mlflow.org/docs/latest/index.html</li> <li>https://umap-learn.readthedocs.io/en/latest/</li> <li>https://docs.feast.dev/</li> <li>https://www.timvink.nl/reproducible-reports-with-mkdocs/</li> <li>Kedro DVC setup <pre><code>kedro new\n</code></pre> <pre><code>kedro mlflow init\nkedro docker init\necho \"dvc\" &gt;&gt; src/requirements.txt\npip install -r src/requirements.txt\n</code></pre> <pre><code>dvc init\ndvc add data/01_raw\ngit rm data/01_raw/.gitkeep\ngit add -u\ngit commit -m \"spec the datasets\"\ndvc add data/01_raw\ngit add data/01_raw.dvc\ngit add data/.gitignore\ngit commit -m \"Added input data to DVC\"\n</code></pre></li> </ul>","tags":["Data Science","Machine Learning","Deep Learning"]},{"location":"Insights/Evolving%20Insights/2022-12-28%20Probablistic%20Data%20Modeling/","title":"Probablistic Data Modeling","text":""},{"location":"Insights/Evolving%20Insights/2022-12-28%20Probablistic%20Data%20Modeling/#count-data-models","title":"Count Data Models","text":"<ul> <li>Consumer demand : for example the number of products that a consumer buys on Amazon </li> <li>Recreational data : the number of trips taken per year </li> <li>Family economics : The number of children that a couple has</li> <li>Health demand : the number of doctor visits in a reasonably big geographical area</li> </ul> <p>Dependent variable is counts which is a non-negative integer so Y would be equal to 0 1 2 3 4 and so on.  These are indeed numbers but the sample is concentrated on a few very small discrete values like for example you can say well how many grains of rice do you consume that's also could theoretically be counted but it's just so much that you may not want to use how many grains of rice but instead how many pounds of rice or some like that and move more toward continuous variable models rather than the discrete variable models so make sure that your when you have data that is count data the numbers are relatively small you know most of the the sample say within 10 20 30 as far as as numbers and you have very few observations beyond that if you have numbers in the thousands and so on then perhaps this is not a very good model for the use case. </p> <p>The number of children that a couple has that is definitely a discrete variable however there may be situations in which that could be turned into a continuous variable.  For example \"how many children per couple\" are in each of the countries and in this way you know you have like a country with 2.3 children per couple in a given country.This again becomes a continuos distribution not good for count data models</p>"},{"location":"Insights/Evolving%20Insights/2022-12-28%20Probablistic%20Data%20Modeling/#count-data-dependent-variable-and-its-properties","title":"Count data dependent variable and its properties","text":"<ul> <li>The dependent variable is counts (a non-negative integer): y = 0, 1, 2, 3, 4, ...</li> <li>An event can occur any number of times during a time period but the sample is concentrated on a few small discrete values.</li> <li>Events occur independently. In other words, if an event occurs, it does not affect the probability of another event occurring in the same time period.</li> <li>We study the factors affecting the average number of the dependent variable</li> </ul>"},{"location":"Insights/Evolving%20Insights/2022-12-28%20Probablistic%20Data%20Modeling/#poisson-model","title":"Poisson model","text":"<p>The Poisson model predicts the number of occurrences of an event. The Poisson model states that the probability that the dependent variable Y will be equal to a certain number y is :</p> \\[p(Y=y)= {e^{-\\mu}\\mu^{y} \\over y!}\\] <p>where \\(\\mu\\) is the intensity or rate parameter</p>"},{"location":"Insights/Evolving%20Insights/2022-12-28%20Probablistic%20Data%20Modeling/#properties-of-the-poisson-distribution","title":"Properties of the Poisson distribution","text":"<ul> <li>Equidispersion property of the Poisson distribution i.e the equality of mean and variance. <ul> <li>\\(E(y|x)=var(y|x)=\\mu\\)</li> <li>Equidispersion property is a restrictive property and often fails to hold in real world examples, i.e., there is \u201coverdispersion\u201d in the data. In this case, use the negative binomial model.   </li> </ul> </li> <li> <p>Merging Independent Poisson Processes : </p> <ul> <li>Let \\(N_1(t), N_2(t), ... ,\u00a0N_m(t)\\)\u00a0 be\u00a0\\(m\\)\u00a0independent Poisson processes  </li> <li>Let the rates be\u00a0\\(\u03bb_1,\u00a0\u03bb_2,\u00a0..., \u00a0\u03bb_m\\) </li> <li>Let  \\(N(t) = N_1(t) + ... + N_m(t)\\), for all\u00a0\\(t\u2208[0,\u221e)\\) </li> </ul> <p>Then, \\(N(t)\\)\u00a0is a Poisson process with rate\u00a0\\(\u03bb_1+\u03bb_2+ ... +\u03bb_m\\)</p> </li> <li> <p>Skellam Poisson Relation : </p> <ul> <li>Let\u00a0\\(N_1(t), N_2(t)\\)\u00a0 be\u00a02\u00a0independent Poisson processes with rates\u00a0\\(\u03bb_1,\u00a0\u03bb_2\\)</li> <li>Let \\(S(t) = N_1(t) - N_2(t)\\) ,for all\u00a0\\(t\u2208[0,\u221e)\\) </li> </ul> <p>Then \\(S(t)\\) is a Skellam distribution with \\(rate1 =  \u03bb_1\\) and \\(rate2 =  \u03bb_2\\)</p> </li> <li> <p>Marginal effects for the Poisson model : One unit increase in x will increase/decrease the average number of the dependent variable events by the marginal effect. It is given by:</p> </li> </ul> \\[\\frac{\\partial E(y|x)}{\\partial x_j} =\\beta_jexp(\\mathbf{x}_i^{'}\\beta)\\] <ul> <li>Excess zeros problem of the Poisson distribution: there are usually more zeros in the data than a Poisson model predicts. In this case, use the zero-inflated Poisson model.</li> <li>The probability of an event occurring is proportional to the length of the time period. For example, it should be twice as likely for an event to occur in a 2 hour time period than it is for an event to occur in a 1 hour period.</li> </ul>"},{"location":"Insights/Evolving%20Insights/2022-12-28%20Probablistic%20Data%20Modeling/#negative-binomial-model","title":"Negative binomial model","text":"<p>The negative binomial model is used with count data instead of the Poisson model if there is overdispersion in the data. Unlike the Poisson model, the negative binomial model has a less restrictive property that the variance is not equal to the mean \\(\\mu\\) \\(\\(var(y|x) = \\mu + \\alpha\\mu^2\\)\\) Where  \\(\\alpha\\)  is the overdispersion parameter and \\(\\mu\\) is the intensity or rate parameter. Another functional form is \\(var(y|x) = \\mu + \\alpha\\mu\\),but this form is less used</p>"},{"location":"Insights/Evolving%20Insights/2022-12-28%20Probablistic%20Data%20Modeling/#test-for-overdispersion","title":"Test for overdispersion","text":"<p>We estimate the negative binomial model which includes the overdispersion parameter \\(\\alpha\\) and test if \\(\\alpha\\) is significantly different than zero.</p> <ul> <li> <p>We have three cases for \\(H_0: \\alpha = 0\\)  or  \\(H_a: \\alpha \\ne 0\\)</p> <ul> <li>When \\(\\alpha = 0\\), the Poisson model. </li> <li>When \\(\\alpha &gt; 0\\) , overdispersion (frequently holds with real data).</li> <li>When \\(\\alpha &lt; 0\\), underdispersion (not very common).</li> </ul> </li> </ul>"},{"location":"Insights/Evolving%20Insights/2022-12-28%20Probablistic%20Data%20Modeling/#incidence-rate-ratios-irr","title":"Incidence rate ratios (irr)","text":"<ul> <li>The incidence rate ratios report \\(exp(b)\\) rather than \\(b\\).</li> <li>Interpretation of the incidence rate ratios: irr=2 means that for each unit increase in x, the expected number of y will double.</li> </ul>"},{"location":"Insights/Evolving%20Insights/2022-12-28%20Probablistic%20Data%20Modeling/#hurdle-or-two-part-models","title":"Hurdle or two-part models","text":"<p>The two-part model relaxes the assumption that the zeros (whether or not there are events) and positives (how many events) come from the same data generating processes. Example: different factors may affect whether or not you practice a particular sport and how many times you practice your sport in a month. We can estimate two-part models similar to the truncated regression models. If the process generating the zeros is \\(f_1\\) and the process generating the positive responses is \\(f_2\\) then the two-part hurdle model is defined by the following probabilities:</p> \\[ g(y) = f_1(0) \\ \\ if \\ \\ y=0\\] \\[g(y) = \\frac{1-f_1(0)}{1-f_2(0)}f_2(y) \\ \\ if\\ \\ y\\ge1\\] <p>The model for the zero versus positive responses is a binary model with the specified distribution, but we can estimate it with the probit/logit model.</p>"},{"location":"Insights/Evolving%20Insights/2022-12-28%20Probablistic%20Data%20Modeling/#zero-inflated-models","title":"Zero inflated models","text":"<p>The zero-inflated model is used with count data when there is an excess zeros problem. Example: you either like hiking or you do not. If you like hiking, the number of hiking trips you can take is 0, 1, 2, 3, etc. So you may like hiking, but may not take a trip this year. We are able to generate more zeros in the data.  The zero-inflated model lets the zeros occur in two different ways:  </p> <ol> <li>As a realization of the binary process (z=0) </li> <li>As a realization of the count process when the binary variable z=1.</li> </ol> <p>If the process generating the zeros is \\(f_1\\) and the process generating the positive responses is \\(f_2\\) then the zero-inflated model is:</p> \\[g(y)=f_1(0)+(1-f_1(0))f_2(0) \\ \\ if \\ \\ y = 0\\] \\[g(y)=(1-f_1(0))f_2(y) \\ \\ if \\ \\ y \\ge 1\\]"},{"location":"Insights/Evolving%20Insights/2023-01-01%20%20Enhanced%20decision%20making%20using%20Probability/","title":"Enhanced decision making using Probability","text":""},{"location":"Insights/Evolving%20Insights/2023-01-01%20%20Enhanced%20decision%20making%20using%20Probability/#concepts-in-probability","title":"Concepts in Probability","text":"<p>Probability computes the likelihood of an event from a random variable. Let's consider\u00a0\\(x\\)\u00a0is a random variable then the probability of an event\u00a0\\(X\\)\u00a0can be expressed as\u00a0\\(P(x=X)\\). For example, flipping a coin can be represented as random variable \\(x\\) then the probability of an event getting head,\u00a0\\(P(x=head)\\) will be\u00a0\\(0.5\\)</p>"},{"location":"Insights/Evolving%20Insights/2023-01-01%20%20Enhanced%20decision%20making%20using%20Probability/#joint-and-conditional-probabilities","title":"Joint and Conditional Probabilities","text":"<p>Consider two random variables\u00a0\\(x\\)\u00a0and\u00a0\\(y\\)  and  pick two events \\(X\\) and \\(Y\\) from these random variables to define the following probabilities:</p> <ol> <li>Conditional probability\u00a0computes the probability of occurring of event \\(X\\) given that event \\(Y\\) has occurred and is denoted by\u00a0\\(P(X|Y)\\).</li> <li>Joint probability\u00a0is defined as the probability of both events \\((x=X \\ and \\  y=Y)\\) jointly and is calculated as \\(P(X \\cap Y) = P(X|Y)*P(Y)\\)</li> </ol> <p>Given \\(x\\) as a set of training data and \\(y\\) as corresponding label</p> <ul> <li>Discriminative Models learn the conditional distribution\u00a0\\(P(y|x): x \u2192 y\\)  and during inference,\u00a0\\(P(y|x)\\)\u00a0will compute the likelihood of that sample belonging from the given class.</li> <li>Generative models learn the joint distribution\u00a0\\(P(x,y)\\), which can be decomposed in conditionals using the definition of the joint probability\u00a0\\(P(x, y) = P(x|y) P(y)\\). Then these models use the marginal probability of classes and then learn\u00a0\\(P(x|y)\\).</li> </ul>"},{"location":"Insights/Evolving%20Insights/2023-01-01%20%20Enhanced%20decision%20making%20using%20Probability/#bayes-theorem","title":"Bayes Theorem","text":"<p>Bayes theorem allows manipulation between conditional probabilities. This theorem can be derived using the commutative property of joint probability, which means:</p> \\[P(X \\cap Y) = P(Y \\cap X)\\] \\[P(X|Y)P(Y) = P(Y|X)P(X)\\] \\[P(X|Y) = \\frac{P(Y|X)P(X)}{P(Y)}\\] <p>In generative models, evaluating\u00a0\\(P(x|y)P(y)\\)\u00a0will be easier compared to\u00a0\\(P(y|x)P(x)\\) given that \\(y\\) is sampled from a tractable density.</p>"},{"location":"Insights/Evolving%20Insights/2023-01-01%20%20Enhanced%20decision%20making%20using%20Probability/#entropy","title":"Entropy","text":"<p>Entropy is similar to the concept of\u00a0information, which is defined as the number of bits required to transmit an event. Low probability events have lesser predictability therefore they have higher information and vice versa. Entropy measures the uncertainty of a system and is defined as the number of bits required the transmit a random event from a probability distribution. Similar to information, entropy can be defined as:</p> \\[H(x) = - \\sum_{X \\ \\epsilon \\ x} P(X)*log(P(X))\\]"},{"location":"Insights/Evolving%20Insights/2023-01-01%20%20Enhanced%20decision%20making%20using%20Probability/#cross-entropy","title":"Cross entropy","text":"<p>Cross entropy is based on the concept of Entropy and is defined between two probability distributions. Let us consider two probability distributions\u00a0\\(p\\)\u00a0and\u00a0\\(q\\). Then the cross-entropy of\u00a0\\(q\\)\u00a0from\u00a0\\(p\\)\u00a0is the number of additional bits to represent an event using\u00a0\\(q\\)\u00a0instead of\u00a0\\(p\\)\u00a0and can be formulated as:</p> \\[CE(p,q) = - \\sum_{X \\ \\epsilon \\ x} p(X)*log(q(X))\\] <p>In DL, cross-entropy is the default loss function for both discriminative and generative models. Let's take an example of binary classification (cat and dog classification), where \\(p\\) is the given distribution for the labels with probabilities 0 or 1. We would like to approximate the distribution \\(q\\) such that an event from \\(q\\) can represent \\(p\\) with minimum number of bits. Since \\(X\\) can have two values, therefore:</p> \\[CE(p,q) = - p(X=0)*log(q(X=0)) - p(X=1)*log(q(X=1))\\] <p>if \\(p\\)\u00a0represents the labelled data \\(y_i\\)\u00a0and\u00a0\\(q\\)\u00a0represents the predicted output\u00a0\\(prob_i\\), then above equation can be modified to</p> \\[CE(p,q) = \\sum_{\\forall_i} -y_i*log(prob_i) -(1-y_i)*log(1-prob_i)\\] <p>In the above equation, if \\(q(X=0) =\u00a0prob_i\\) then\u00a0\\(q(X=1) = 1- prob_i\\) because of the binary classification.</p>"},{"location":"Insights/Evolving%20Insights/2023-01-01%20%20Enhanced%20decision%20making%20using%20Probability/#measure-distances-between-probabilities-distributions","title":"Measure distances between probabilities distributions","text":""},{"location":"Insights/Evolving%20Insights/2023-01-01%20%20Enhanced%20decision%20making%20using%20Probability/#kullback-leibler-divergence","title":"Kullback - Leibler divergence","text":"<p>Kullback - Leibler Divergence also called relative entropy calculates a distance metric between two PDFs or divergence between two probabilities distributions. KL-divergence between \\(p\\) and \\(q\\) (two probability distributions) computes the number of extra bits to represent an event using\u00a0\\(q\\)\u00a0instead of\u00a0\\(p\\). While cross-entropy computes an average number of total bits to represent an event from\u00a0\\(q\\)\u00a0instead of\u00a0\\(p\\). KL divergence is diffferentiable and can be written as:</p> \\[ D_{KL}(p||q) = - \\int_x p(x) log(\\frac{q(x)}{p(x)})\\] \\[ D_{KL}(P||Q) = - \\sum_x p(x) log(\\frac{q(x)}{p(x)})\\] <p>The above equation can be expanded and rewritten in the form of cross-entropy and entropy:</p> \\[CE(p,q) = D_{KL}(p||q) + H(p)\\] <ol> <li>When\u00a0\\(p\\) (target distribution)\u00a0and\u00a0\\(q\\) (predicted distribution)\u00a0are the\u00a0same then KL divergence will be zero, i.e. the lower value of KL divergence indicates the higher similarity between two distributions.</li> <li>KL divergence and cross-entropy both are not symmetrical (a con which is ignored many times)</li> <li>Cross-entropy is mostly used as the loss function and from an optimization point of view, KL divergence and cross-entropy will be the same because the entropy term \\(H(p)\\) is a constant and will be zero during the derivative calculations.</li> </ol>"},{"location":"Insights/Evolving%20Insights/2023-01-01%20%20Enhanced%20decision%20making%20using%20Probability/#jensen-shannon-js-divergence","title":"Jensen Shannon (JS ) divergence","text":"<p>Jensen Shannon divergence also quantifies the difference between two probability distributions and extends KL-divergence to calculate the symmetrical measure. JS-divergence between\u00a0\\(p\\)\u00a0and\u00a0\\(q\\)\u00a0is:</p> \\[D_{JS}(p||q) =  \\frac{1}{2}D_{KL}(p||\\frac{p+q}{2}) + \\frac{1}{2}D_{KL}(q||\\frac{p+q}{2})\\] <ol> <li>JS-divergence computes the symmetrical measure.</li> <li>If we use base-2 logarithm then the above measure is a normalized version of KL divergence, with scores between 0 (same) and 1 (completely different).</li> <li>JS-divergence is an improved version of KL-divergence with symmetric measures and normalized outcomes</li> <li>Normalization provides better stability during the loss function optimization and therefore JS-divergence is prefered for the complicated tasks (GAN) compared to KL-divergence</li> </ol>"},{"location":"Insights/Evolving%20Insights/2023-01-01%20%20Enhanced%20decision%20making%20using%20Probability/#wasserstein-metric","title":"Wasserstein metric","text":"<p>The Wasserstein metric calculates the distance between two probability distributions and is termed as Earth Mover\u2019s distance, which measures the minimum energy cost of moving a pile of dirt in the shape of one probability distribution to the shape of the other distribution. The Wasserstein metric between \\(p\\) and \\(q\\) is defined as</p> \\[W(p,q) = \\underset{\\gamma \\ \\sim \\ \\prod (p,q)}{\\inf} \\  \\mathbb{E}_{(x,y) \\ \\sim \\ \\gamma} * [||x-y||]\\] <p>In the above equation, the product term\u00a0\\(\\prod (p,q)\\)\u00a0represents\u00a0the set of joint probabilities. The term\u00a0\\(\\gamma(x, y)\\)\u00a0is a joint probability and measures the amount of dirt that should be moved from\u00a0\\(x\\)\u00a0to \\(y\\)\u00a0for\u00a0\\(x\\)\u00a0to follow the same distribution as\u00a0\\(y\\). Therefore, the total amount of moved dirt would be\u00a0\\(\\gamma(x, y)\\)\u00a0with the moving distance of\u00a0\\(||x-y||\\)\u00a0and further the cost will be\u00a0\\(\\gamma(x, y).||x-y||\\)\u00a0for a given set of\u00a0\\(x\\)\u00a0and\u00a0\\(y\\).\u00a0The term\u00a0\\(\\inf\\)\u00a0(infimum) indicates that the above equation will always provide the smallest cost. Note Marginals of \\(\\prod (p,q)\\) will be \\(p\\) and \\(q\\)</p> <ol> <li>Wasserstein metric is a metric, not a divergence which means it follows all three metric properties (positive, symmetric and triangle inequality) that the Wasserstein metric is more stable and smooth in the optimization problems.</li> <li>It is able to compute the distance between two distributions even if they are not in the same probability space, unlike KL-divergence</li> </ol>"},{"location":"Insights/Evolving%20Insights/2023-01-01%20%20Enhanced%20decision%20making%20using%20Probability/#kl-divergence-vs-js-divergence-vs-wasserstein-metric","title":"KL divergence vs JS divergence vs Wasserstein metric","text":"<p>Consider \\(p\\) , \\(q\\) two uniform distributions centred at\u00a0\\(0\\)\u00a0and\u00a0\\(\\theta\\). Basically both distributions \\(p\\) and \\(q\\) are not overlapping and have a distance of\u00a0\\(\\theta\\)\u00a0between them, which is a very simple example of a non-sharing probability space.  The values of all the metrics discussed is given below</p> \\[ D_{KL}(p||q) = \\infty\\] \\[ D_{KL}(q||p) = \\infty\\] \\[ D_{JS}(p||q) = log(2)\\] \\[ W(p,q) = |\\theta|\\] <ul> <li>KL-divergence between these two distributions will be \\(\\infty\\) because of non-overlapping and the denominator part will be zero</li> <li>JS-divergence will be constant\u00a0\\(log(2)\\)\u00a0irrespective of the value of\u00a0\\(\\theta\\)\u00a0as one of them will be zero at the given\u00a0\\(x\\)</li> <li>Wasserstein metric depends on the distance between these two distributions which is desirable and considering the horizontal shifts irrespective of overlaps. Therefore, the Wasserstein metric is more practical and produces better results compared to JS-divergence </li> </ul>"},{"location":"Insights/Evolving%20Insights/2023-01-01%20%20Enhanced%20decision%20making%20using%20Probability/#kolmogorovsmirnov-test","title":"Kolmogorov\u2013Smirnov test","text":"<p>Kolmogorov\u2013Smirnov test KS test is a\u00a0nonparametric test\u00a0of the equality of continuous (or discontinuous), one-dimensional\u00a0probability distributions\u00a0that can be used to compare a\u00a0sample\u00a0with a reference probability distribution (one-sample K\u2013S test), or to compare two samples (two-sample K\u2013S test). KS Statistic is not differentiable</p> \\[KS \\ Stat = Max (|p'(x)-q'(x)|)\\] <p>where \\(p'\\) = CDF of \\(P(x)\\)</p>"},{"location":"Insights/Evolving%20Insights/2023-01-01%20%20Enhanced%20decision%20making%20using%20Probability/#how-to-measure-if-two-distributions-are-similar-or-not","title":"How to measure if two distributions are similar or not ?","text":"<ul> <li>There are many tests and metrics like KS-test, JS-divergence, KL-divergence, Wasserstein metric to get an idea about similarity of two distributions. But all of these have some or the other limiltation. </li> <li>The most robust way to estimate similarity is to train a model with each component distribution given a class label, i.e. for the \\(i_{th}\\) distrubution \\(D_i\\)  we can assign a distinct label \\(i\\) in the target \\(y\\). Then train a fairly complex model, If the log loss of the traied model is low then the model is able to distinguish between the distributions. and hence the distributions are different. If the log loss is on the higher side than the model is not able to distinguish between the distributions and hence the distributions are similar.</li> </ul>"},{"location":"Insights/Evolving%20Insights/2023-01-01%20%20Enhanced%20decision%20making%20using%20Probability/#references","title":"References","text":"<ol> <li>basic probability</li> <li>comparing distributions</li> <li>Arjovsky, Martin, Soumith Chintala, and L\u00e9on Bottou. \u201cWasserstein generative adversarial networks.\u201d\u00a0International conference on machine learning</li> <li>Weng, Lilian. \u201cFrom gan to WGAN.\u201d\u00a0arXiv preprint arXiv:1904.08994\u00a0(2019).</li> </ol>"},{"location":"Insights/Evolving%20Insights/2023-01-11%20Learn%20One%20Thing%20At%20A%20Time/","title":"Learn One Thing At A Time","text":"<ul> <li>ML for Product Analytics by Ron Tidhar</li> <li>Intro to Opportunity Sizing and a rational conversation about impact</li> <li>Ken Jee Resourses</li> <li>Best in ML</li> <li>Embedding vs Dense Layers</li> </ul>"},{"location":"Insights/Evolving%20Insights/2023-01-13%20Explainable%20AI/","title":"Explainable AI","text":""},{"location":"Insights/Evolving%20Insights/2023-01-13%20Explainable%20AI/#research-papers-blogs-resources","title":"Research Papers, Blogs &amp; Resources","text":"<ol> <li>Explaining the Predictions of Any Classifier</li> <li>Integrated Gradients</li> <li>Robustness of Interpretability Methods</li> <li>Interpretable Machine Learning Web Book</li> <li>LIME TDS 1 | LIME Blog | LIME Text Explain</li> </ol>"},{"location":"Insights/Evolving%20Insights/2023-01-13%20Explainable%20AI/#python-libraries","title":"Python Libraries","text":"<ol> <li>explainerdashboard</li> <li>omnixai</li> <li>InterpretML</li> <li>ELI5</li> <li>Shapash</li> <li>LIME</li> <li>SHAP</li> </ol>"},{"location":"Insights/Evolving%20Insights/2023-01-13%20Explainable%20AI/#local-vs-global-interpretability","title":"Local vs Global Interpretability","text":"<ul> <li>Global feature importance comes from understanding how the model breaks down the global space/dimension and tries to explain how are all the points in data predicted</li> <li>Local feature importance comes from understanding how the model behaves around the locality of current query point</li> <li>Within the neighbourhood of the query point create a surrogate model which aproximates the original model in that neighbourhood. This surrogate model can be a basic linear model or a Decision Trees, which have easily interpretable feature importance</li> <li>Interpretable representation : Conversion of a feature of dimension d to a binary feature of dimension d'. Ex : for text use Bag of Word, for image use super pixel, for real values use real valued binning</li> <li>The Surrogate model will use these Interpretable representation as input rather than the original input</li> </ul>"},{"location":"Insights/Industry%20Insights/2022-01-01%20Banking%20Business/","title":"Banking Business","text":"","tags":["Banking"]},{"location":"Insights/Industry%20Insights/2022-01-01%20Banking%20Business/#understanding-banking-as-a-business","title":"Understanding Banking as a Business","text":"<p>The banking industry players deal in a variety of products from savings accounts to loans and mortgages, offer various services from check cashing to underwriting, cater to different types of customers from individuals to large corporates, serve diverse geographies from rural villages to cross-border operations. Thus the banking industry is made up of several types of banks, with their own objectives, roles, and functions</p>","tags":["Banking"]},{"location":"Insights/Industry%20Insights/2022-01-01%20Banking%20Business/#retail-sectors","title":"Retail Sectors","text":"<p>Retail banks are the banks that cater to the needs of individuals and the most common format of banking that we experience. They include deposit accepting institutions like saving banks loan associations, credit unions, thrifts, and other savings banks.</p> <p>Individuals are the targeted consumers for retail banking and banks offer a variety of products and services to these consumers including savings accounts, safe lockers, fixed and recurring deposits housing loans, consumer loans, personal loans, and unsecured and revolving loans, such as credit cards.</p> <p>Retail Banking includes exposures that fulfil the four criteria of orientation, product, granularity, and low value of individual exposures: These retail exposures are laid down in the Basel Committee on Banking Supervision document \"International Convergence of Capital Measurement and Capital Standards. Product criterion exposure means exposure on specified products specified as revolving credits, lines of credit, overdrafts, term loans, instalment loans, student and educational loans other leases, and small business facilities and commitments, It also includes housing loans.</p>","tags":["Banking"]},{"location":"Insights/Industry%20Insights/2022-01-01%20Banking%20Business/#commercial","title":"Commercial","text":"<p>This category represents corporate and business banking and includes commercial and foreign banks. Commercial banks offer similar kinds of products and services as retail banks however, as retail banks target individual consumers, commercial banks are focused on corporate, Institutions, and commercial businesses. Products and services include consumer and commercial deposits. business loans, mortgage and real estate loans, overseas operations, investment in high-grade securities, and industrial loans</p>","tags":["Banking"]},{"location":"Insights/Industry%20Insights/2022-01-01%20Banking%20Business/#investment","title":"Investment","text":"<p>The products and services of this category include managing portfolios of financial assets, trading in securities, fixed income, commodity and currency, corporate advisory services for mergers and acquisitions, corporate finance, and debt and equity underwriting. Trading activities include trading both on behalf of clients or on the bank's own account</p>","tags":["Banking"]},{"location":"Insights/Industry%20Insights/2022-01-01%20Banking%20Business/#central-banks","title":"Central Banks","text":"<p>Central Banks, are bankers and every country generally has one central bank that occupies a central position in the banking system and acts as the highest financial authority. The main function of this bank is to regulate and supervise the whole banking system in the country. It is a banker's bank and controller of credit in the country</p>","tags":["Banking"]},{"location":"Insights/Industry%20Insights/2022-01-01%20Banking%20Business/#banking-divisions","title":"Banking Divisions","text":"<ul> <li>Banking Operations: Personal &amp; Commercial Banking comprises of personal banking operations and transactional banking</li> <li>Clearing Functions: Internal and External Reconciliations &amp; Internal and External Clearing</li> <li>Asset Management: Provide asset management products and services</li> <li>Wealth Management: High net worth clients with a full suite of investment, trust, and other wealth management solutions</li> <li>Treasury &amp; Risk Management: Provide custodial, advisory, financing and other services for clients to safeguard assets, maximize liquidity &amp; manage risk</li> <li>Cards Issuance and Management: Issuance of credit cards and managing credit and operations for cards business</li> <li>Trading Intermediary: Acting as Depository Participant Registry, Exchanges, Trading, or Broker-Dealer</li> <li>Enabling Functions: Human Resources, Finance, Banking Technology, Surveillance, Legal and Compliance</li> </ul>","tags":["Banking"]},{"location":"Insights/Industry%20Insights/2022-01-02%20Supply%20Chain/","title":"Supply Chain","text":"","tags":["Supply Chain"]},{"location":"Insights/Industry%20Insights/2022-01-02%20Supply%20Chain/#supply-chain-management","title":"Supply Chain Management","text":"","tags":["Supply Chain"]},{"location":"Insights/Industry%20Insights/2022-01-02%20Supply%20Chain/#three-broad-levels","title":"Three broad levels","text":"<ol> <li>Strategic Level\u00a0SCM takes in the entire scope of a company's supply chain network and processes. It addresses the major long-term elements in a comprehensive strategy, including the type and number of facilities, technologies and suppliers.   </li> <li>Tactical\u00a0Level nails down the specific means of executing the strategy, be they production schedules, logistics processes, contracts or software applications. Time horizons are shorter than in strategic SCM. Here's where standards and best practices are identified for key deliverables such as customer service, efficiency and cost.</li> <li>Operational\u00a0Level encompasses the daily operations of SCM. Forecasting, production scheduling, shipping and invoicing are typical operational tasks.</li> </ol>","tags":["Supply Chain"]},{"location":"Insights/Industry%20Insights/2022-01-02%20Supply%20Chain/#data-science-in-supply-chain","title":"Data Science in Supply Chain","text":"","tags":["Supply Chain"]},{"location":"Insights/Industry%20Insights/2022-01-02%20Supply%20Chain/#predicting-supply-disruptions","title":"PREDICTING SUPPLY DISRUPTIONS","text":"<p>Supply chain data analytics gives companies the ability to predict supply disruptions and make adjustments before problems impact production. Companies can also look at trending data on weather events, political instability, or financial issues that may impact a supplier\u2019s ability to deliver on schedule. With this information, teams can make alternate plans as needed so they can maintain normal operations.</p>","tags":["Supply Chain"]},{"location":"Insights/Industry%20Insights/2022-01-02%20Supply%20Chain/#quality-assurance","title":"QUALITY ASSURANCE","text":"<p>Manufacturing companies use real-time data analysis for quality assurance. Using IoT-enabled cameras and measuring devices, a manufacturer can identify issues before a product is shipped to retailers or consumers.</p>","tags":["Supply Chain"]},{"location":"Insights/Industry%20Insights/2022-01-02%20Supply%20Chain/#warehouse-management","title":"WAREHOUSE MANAGEMENT","text":"<p>Information gathered on warehouse temperature, shelf weight, and load weight can be used to optimize warehouse operations and improve productivity. Analytics can inform receiving, tracking, and storing inventory, as well as workload planning, managing shipping, and monitoring the movement of items in the warehouse.</p>","tags":["Supply Chain"]},{"location":"Insights/Industry%20Insights/2022-01-02%20Supply%20Chain/#logistics","title":"LOGISTICS","text":"<p>Data\u00a0analytics is used in logistics\u00a0to plan more efficient delivery routes and reduce fuel consumption. Companies can use data to identify the ideal mode of transportation for their loads.</p>","tags":["Supply Chain"]},{"location":"Insights/Industry%20Insights/2022-01-02%20Supply%20Chain/#sales-inventory-and-operations-planning","title":"SALES, INVENTORY, AND OPERATIONS PLANNING","text":"<p>Retailers can analyze point-of-sale (POS), inventory, and production volume data to identify misalignment in supply and demand. As a result, they can determine when to place orders with suppliers, which products to put on sale when, and when to launch new product offerings.</p>","tags":["Supply Chain"]},{"location":"Insights/Industry%20Insights/2022-01-02%20Supply%20Chain/#demand-planning","title":"Demand planning","text":"<p>Demand planning is a process for gathering historical data, such as past sales, and applying analytics and statistical modeling to create a forecast or demand plan that the sales department and operational departments -- for example, manufacturing and marketing -- can agree on. The forecast determines the types and quantities of products to be manufactured. Getting the forecast right is critical for avoiding costly problems such as the\u00a0bullwhip effect, in which small fluctuations in retail demand are magnified further up the supply chain, leading to severe shortages or surpluses of inventory.</p>","tags":["Supply Chain"]},{"location":"Insights/Industry%20Insights/2022-01-02%20Supply%20Chain/#production-planning","title":"Production planning","text":"<p>Production planning,the company nails down the specifics of where and how the products called for in the demand plan will be manufactured. A more fine-tuned variation called advanced planning and scheduling &amp; optimization seeks to optimize the resources that go into production and make them more responsive to changes in demand.</p>","tags":["Supply Chain"]},{"location":"Insights/Industry%20Insights/2022-01-02%20Supply%20Chain/#material-requirements-planning","title":"Material requirements planning","text":"<p>MRP\u00a0is used to ensure sufficient materials and components, such as subassemblies, are available for use in the manufacturing process by taking inventory of what's on hand, identifying gaps, and buying or making the remaining items. The central document in both MRP and production planning is the bill of materials (BOM), a complete list of the items needed to make a product. MRP is sometimes done as part of manufacturing resource planning (MRP II), which broadens the MRP concept to other departments such as HR and finance. </p>","tags":["Supply Chain"]},{"location":"Insights/Industry%20Insights/2022-01-02%20Supply%20Chain/#inventory-management","title":"Inventory management","text":"<p>Inventory management\u00a0consists of various techniques and formulas for ensuring adequate supply -- from raw materials in a manufacturing plant, perhaps managed in an\u00a0MRP system, to packaged goods in a retail store -- for the least expenditure of time and resources.</p> <p>Manufacturers are faced with a variety of\u00a0inventory management issues, many of which involve coordinating demand planning with inventory at both ends of the production process. For example, sometimes MRP leads to\u00a0more\u00a0inventory -- especially when the system is first implemented -- and the manufacturer must work to synchronize MRP parameters with the inventory already on hand.</p>","tags":["Supply Chain"]},{"location":"Insights/Industry%20Insights/2022-01-02%20Supply%20Chain/#procurement","title":"Procurement","text":"<p>Sometimes called\u00a0sourcing,\u00a0procurement\u00a0is the process of finding suppliers for goods, managing those relationships and acquiring the goods economically -- along with all the communication, such as sending out requests for bids, and paperwork, including purchase orders and invoices.</p> <ol> <li> <p>Strategic sourcing\u00a0is an elevated and more sophisticated type of procurement that aims to optimize a company's sourcing process by taking advantage of its consolidated purchasing power and aligning it with business goals.</p> </li> <li> <p>Supplier relationship management (SRM), in contrast, addresses sourcing issues by focusing on the suppliers the company deems most critical to success and systematically strengthening relationships with them while fostering optimal performance. </p> </li> </ol>","tags":["Supply Chain"]},{"location":"Insights/Industry%20Insights/2022-01-02%20Supply%20Chain/#logistics_1","title":"Logistics","text":"<p>Logistics\u00a0covers the transporting and storing of goods, from the delivery of parts and raw materials to manufacturers or processors to the delivery of finished products to stores or direct to consumers, and even beyond for product servicing, return and recycling -- a process called\u00a0reverse logistics. Inventory management is threaded throughout the logistics process.</p>","tags":["Supply Chain"]},{"location":"Insights/Industry%20Insights/2022-01-02%20Supply%20Chain/#goals-of-scm","title":"Goals of SCM","text":"<ul> <li>cost savings from acquiring goods and services as cheaply as possible and minimizing expenses on capital goods, such as inventory, facilities and equipment;</li> <li>efficiency that comes from avoiding waste and duplication;</li> <li>revenue increases from meeting higher levels of demand with sufficient supply;</li> <li>profit from lower costs and higher revenue;</li> <li>customer satisfaction from balancing supply and demand, and delivering the products consumers want;</li> <li>quality improvements from sourcing better materials, avoiding production errors and gathering customer feedback; and</li> <li>stability from effective risk management, visibility and collaboration.</li> <li>better relationships with suppliers, distributors and retailers;</li> <li>improved brand image;</li> <li>environmental sustainability;</li> <li>improved cash flow;</li> <li>safer products and services;</li> <li>lower overhead;</li> <li>improved accountability and compliance; and</li> <li>more innovation.</li> </ul>","tags":["Supply Chain"]},{"location":"Insights/Industry%20Insights/2022-01-02%20Supply%20Chain/#softwares-and-tools-involved-in-scm","title":"Softwares and Tools involved in SCM","text":"<ul> <li>Basic Stuff : APS, demand management, inventory management, MRP, production planning, procurement and sourcing, S&amp;OP and SRM. </li> <li>Order management systems\u00a0designed to integrate the workflow of customer orders from receipt to fulfillment and payment.</li> <li>Transportation management system**\u00a0is essentially a repository of detailed information about shipping carriers that enables users to plan, execute and track shipments.</li> <li>****Warehouse management system**** is software for managing the many processes involved in moving goods through warehouses, such as inventory tracking, receiving and putaway.</li> </ul>","tags":["Supply Chain"]},{"location":"Insights/Industry%20Insights/2022-01-02%20Supply%20Chain/#sources-of-supply-chain-alternate-data","title":"Sources of Supply Chain Alternate Data","text":"<ol> <li>Trade (Shipping) Data:\u00a0     Shipping/Trade data is a collection of information on ships, cargo, owners, shipbuilders, movements, fixtures, causalities, ports, and companies. Shipping data is perhaps the most obvious and widely-used dataset category to consider when undertaking supply chain analysis. Trade data comes from government import/export data and bill of lading data at the port level. The coverage typically includes all products and commodities shipped globally. These products and commodities can frequently be tied back to individual companies or rolled up for sector insights.</li> <li>Satellite Data:\u00a0     Satellite data provides information on both commodities and manufacturing plants where products and materials are traveling from, but also offers information on vessel movements and port activity for shipping data. Satellite data can be harnessed for supply chain analysis to make better choices, anticipate issues, and re-route material flows to minimise disruption. For investors, satellite data is most valuable for tracking sea freight. The value of satellite data relative to import/export data provided by trade data is that the data is more timely \u2013 often real-time. It\u2019s possible to see in real-time if a vessel is stuck at a port like LA and how many ships are floating offshore</li> <li>Flight Data:\u00a0     Flight data provides end-to-end information on cargo and the movements of aircraft through alerts and tracking. Flight data, specifically as regards cargo traffic, can be valuable for analysing supply chain dynamics for higher value or time-sensitive goods such as pharmaceuticals, tech, and luxury goods, or items that need to be expedited. However, unlike sea freight discussed previously, it\u2019s not possible to see \u201cinside the container\u201d for air freight so it\u2019s not always clear what is being shipped.</li> <li>IoT &amp; Geolocation:      IoT devices allow companies to understand where goods are, how they are being stored, and when they can be expected at a specific location. For Ex: authenticating the location of goods in real-time, tracking speed of movement, monitoring of storage conditions, streamlining the movement and location of goods, and validation of arrival.</li> <li>Other Data for Supply Chain Tracking:\u00a0     Other datasets worth exploring in the context of supply chain analysis are credit datasets, ESG datasets, firmographic and reference data, NLP on public text data, and crawled data for analysis of stock-outs at online retailers.</li> </ol>","tags":["Supply Chain"]},{"location":"Insights/Industry%20Insights/2022-01-02%20Supply%20Chain/#supply-chain-planning","title":"Supply Chain Planning","text":"<ol> <li>Demand Planning</li> <li>Inventory Optimization</li> <li>Merchandise Financial Planning</li> <li>Network Design</li> <li>Production Planning</li> <li>Sales &amp; Operations Execution</li> <li>Sales &amp; Operations Planning</li> <li>Allocation &amp; Replenishment</li> <li>Supply Planning        </li> </ol>","tags":["Supply Chain"]},{"location":"Insights/Industry%20Insights/2022-01-02%20Supply%20Chain/#supply-chain-execution","title":"Supply Chain Execution","text":"<ol> <li>Transportation Management</li> <li>Logistics Network</li> <li>Modeling</li> <li>Warehouse Management</li> <li>Labor Management</li> <li>Robotics Hub</li> <li>Warehouse Tasking</li> </ol>","tags":["Supply Chain"]},{"location":"Insights/Industry%20Insights/2022-01-02%20Supply%20Chain/#omnichannel-commerce","title":"Omni\u2011Channel Commerce","text":"<ol> <li>Assortment Management</li> <li>Inventory &amp; Order</li> <li>Lifecycle Pricing</li> <li>Space &amp; Floor Planning</li> <li>Store Execution</li> <li>Workforce Management</li> <li>Merchandise Operations</li> </ol> <p>Problem Statement</p> <p>In the wake of a swiftly globalising market, Supply chain processes are facing a multitude of complications with respect to routing, resources and planning. Transportation vehicle schedules are becoming increasingly difficult to determine, and resources are often wasted when orders are under- or over-supplied. Inventory management is becoming inefficient and overly costly as customers increase their demand for on-time delivery and accurate tracking. Supply chain operations are experiencing significant issues due to a lack of automated tools integrated into their process to effectively manage planning, forecasting and optimisation of resources.</p> <p>Solution and Services</p> <p>The use of Machine Learning technology offers a viable solution to many of the problems plaguing supply chain operations. AI algorithms can be used to automatically detect inconsistencies, predict changes in customer orders, and ultimately increase operational efficiency. Machine Learning technologies can enable companies to improve their inventory management systems by accurately forecasting demand, optimising routing, and adjusting vehicle schedules. Automation technology also enables companies to develop new technologies to maintain better shipment tracking capabilities and accurate inventory forecasts.</p> <p>To properly and effectively employ Machine Learning technologies in the supply chain, companies must optimise the technologies through algorithms and models. They must use the data collected from their supply chain operations to generate sets of rules that can accurately predict and optimise the performance of their operations. </p> <p>In addition, Machine Learning technology is well-suited to integrate customer data with production data to provide better insight into customer preferences and needs. Companies can develop customer models that can accurately predict customer desires and behaviours, which can allow the company to adapt their product offering and operations to meet these customer needs. Machine Learning technologies can also provide companies with faster customer feedback, enabling them to monitor customer satisfaction and make more informed decisions about their operations.</p> <p>By leveraging Machine Learning technologies, companies can reduce cost and improve the efficiency of their supply chain operations. With the power of automation, companies can avoid unnecessary expenses, better coordinate resources, and ultimately increase their customer satisfaction and optimise return on investments.</p> <p>From predictive analytics to predicting customer demand and even planning shipping routes, machine learning can help make the supply chain smoother and more productive. Here are a few specific ways machine learning can be used to transform supply chain operations:</p> <ol> <li> <p>Demand forecasting: By leveraging the power of predictive analytics, machine learning algorithms can be used to accurately forecast customer demand and anticipate changes in the industry. This helps companies better plan for future supplies, adjust their supply chain strategies, and stay ahead of their competitors.</p> </li> <li> <p>Transportation planning: By leveraging real-time data, machine learning algorithms can be used to optimise transportation routes and reduce costs. With smarter routing, companies can save time and money by optimising the delivery of goods and services.</p> </li> <li> <p>Inventory optimisation: With machine learning, it\u2019s now possible to anticipate when to order inventory and which products to carry. This can help businesses reduce stock-outs and keep inventory costs low.</p> </li> <li> <p>Quality control: Machine learning algorithms can help companies improve product designs and bolster manufacturing processes. This ensures that the highest-quality parts are produced, ensuring customer satisfaction and reducing the need for product returns.</p> </li> </ol> <p>Conclusion: The efficient operation of a company\u2019s supply chain is essential for the company to thrive in today\u2019s global economy. By employing Machine Learning technologies, companies can improve processes and integrate analytics to reduce costs and increase the efficiency of their supply chain operations. Through automation technology, companies can reduce their inventory management efforts, enhance customer feedback processes, and ultimately increase the accuracy of their supply chain operations. With Machine Learning, companies can be better equipped to meet customer needs, save time and money, and increase their ROI.</p> a. Inventory planning b. Sales Forecasting c. Price Forecasting (Change: When &amp; How Much) d. Replenishment Forecasting e. Return Forecasting f. Stock Out Calculation g. Markdown Forecasting h. Estimated Delivery Date and Location i. Processing Time Estimation ii. Transit Time Estimation i. Availability j. Safety Stock","tags":["Supply Chain"]},{"location":"Insights/Industry%20Insights/2022-01-03%20Product%20Analytics/","title":"Product Analytics","text":""},{"location":"Insights/Industry%20Insights/2022-01-03%20Product%20Analytics/#why-product-analytics","title":"Why Product Analytics","text":"<p>Understanding user behavior in a variety of contexts can lead to better-targeted campaigns, increased revenue, and greater user satisfaction and engagement for any product.  Every company is tasked with understanding, altering, and predicting user behavior.  when you understand who your users are, what they are doing with your product, and what drives them to purchase and engage with your product, then you can try to modify their engagement and revenue behavior.</p>"},{"location":"Insights/Industry%20Insights/2022-01-03%20Product%20Analytics/#behavioral-data","title":"Behavioral Data","text":"<ol> <li>Click stream data is the path of clicks through a website or model product ordered by time. </li> <li>User sessions comprise a pattern of consistent use from the first to last interaction on a site. </li> <li>Churn is the number of users or rate of leaving the site over a particular period.</li> <li>Bounce rate is the proportion of users who leave the site after viewing only one page</li> </ol>"},{"location":"Insights/Industry%20Insights/2022-01-03%20Product%20Analytics/#common-pitfalls","title":"Common Pitfalls","text":"<ol> <li>When we focus on only a couple of descriptive facts, we do not get a holistic picture of what is happening in the product and eventually we just get lost in the details.</li> <li>Machine learning prediction is not as useful as causal inference in deriving insights or changing user behavior because it does not help us find variables that cause a user to behave in a certain way, such as deciding to purchase.</li> </ol>"},{"location":"Insights/Industry%20Insights/2022-01-03%20Product%20Analytics/#basic-principles","title":"Basic Principles","text":"<ol> <li>The very first step in understanding your users is building a conceptual model and collecting the right metrics or descriptive statistics to verify or falsify that model. Next, we use metrics, experimentation, and statistical inference to derive insights about users. Finally, we focus on user behavioral change in a web product based on understanding what causes behavior, or answering the why question. </li> <li>Determining what causes a behavior and understanding the size of its effect.</li> </ol>"},{"location":"Insights/Industry%20Insights/2022-01-03%20Product%20Analytics/#social-process-of-using-a-product","title":"Social Process of Using a Product","text":"<ol> <li>Social behavior is a process, not a problem to be solved. Users are coming to your product repeatedly and interacting with people and content. They are also creating content, building communities, reinforcing norms, and building culture, which in turn affects the environment that other users interact with. </li> <li>Social systems are open systems, meaning there are omitted or unmeasurable variables that can affect outcomes.  Variables reflecting events that happen rarely can greatly affect social processes, are very difficult to predict, and are often left out of models. Because many variables are present in social systems, and we have little understanding of their complete effect, some will always be left out.</li> <li>When exploring social behavior, there are often no clear and defined outcomes. In a social process, behavior is easier to understand if you can understand your users\u2019 incentives and the causal impact of different actions on user behavior. Causal effects and user incentives are key to understanding what is happening with your product, but are left out of predictive frameworks.</li> <li>Social systems have rampant problems of incomplete or one-sided information. It\u2019s easy to mistake quantity for quality of data. However, often data on the most important variables in describing user behavior is not easily collected. The primary variable that is left out of analyses but is vital to determining behavior is motivation or user goals. Users of our web product probably have a variety of motivations, and these motivations impact how they access and view the web product. Having an understanding of the individual goals and motivations can help us improve our product.</li> <li>Social systems consist of millions of potential behaviors. Omitted information like goals and expectations is very important, yet information we may never obtain. Instead, we could have thousands of much less useful variables that we need to sift through to find the right ones to describe the process.</li> <li>Inferring causation or why something happens is almost impossible. Creating an A/B test setting os very difficult in a product environment</li> </ol>"},{"location":"Insights/Industry%20Insights/2022-01-03%20Product%20Analytics/#techniques","title":"Techniques","text":"<ol> <li>A counterfactual is an alternative situation where everything is the same except the variable that influences our outcome. It allows us to determine how this one difference would change the result.</li> </ol>"},{"location":"Insights/Industry%20Insights/2022-01-03%20Product%20Analytics/#predictive-vs-causal-inference","title":"Predictive vs Causal Inference","text":"<ol> <li>Causal inference does not necessarily improve with more data. Causal inference relies on having a valid counterfactual or \u201cplacebo\u201d test. Conversely, massive amounts of data do improve predictive models.</li> <li>Prediction also allows for generalization and validation based on external or new data. Causal inference is difficult to generalize and validate outside of an experiment.</li> <li>Machine learning prediction is not as useful as causal inference in deriving insights or changing user behavior because it does not help us find variables that cause a user to behave in a certain way, such as deciding to purchase.</li> </ol>"},{"location":"Insights/Industry%20Insights/2022-01-04%20Export%20Import/","title":"Export Import","text":"","tags":["Export","Import","ExIm"]},{"location":"Insights/Industry%20Insights/2022-01-04%20Export%20Import/#important-topics-in-exim","title":"Important Topics in ExIm","text":"","tags":["Export","Import","ExIm"]},{"location":"Insights/Industry%20Insights/2022-01-04%20Export%20Import/#module-1-introduction-and-opportunity-in-export-import","title":"Module 1: Introduction and opportunity in Export import","text":"<ul> <li>Welcome and Introduction</li> <li>What is Export Import Business?</li> <li>Exporter Importer and Their Types</li> <li>What is FEMA?</li> <li>Foreign Exchange Management Act</li> <li>Role of FEMA in Export Import</li> <li>What Is Foreign Trade Policy</li> <li>Introduction of Foreign Trade Policy</li> <li>Roll of FTP</li> <li>Export Import Rules and Regulation</li> <li>Opportunity in Export Import</li> <li>Growth in international trade</li> <li>India\u2019s Share in international trade</li> <li>Basic Document in Export Import</li> <li>IER RCMC GST ISO</li> </ul>","tags":["Export","Import","ExIm"]},{"location":"Insights/Industry%20Insights/2022-01-04%20Export%20Import/#module-2-how-to-select-product-for-export","title":"Module 2 : How to Select Product for Export","text":"<ul> <li>Why Product Selection Is Important</li> <li>Find right product for Export</li> <li>Right guidance for Growth</li> <li>Good opportunity in future</li> <li>Future demand in product</li> <li>SWOT Analysis for Product Selection</li> <li>How to select product for Export</li> <li>Top Thing to keep mind while product selection</li> <li>Important parameter in product selection</li> <li>Investment in Export</li> <li>Manpower and other resources</li> <li>Sourcing of product</li> <li>Availability and feasibility of product</li> <li>Production and packaging</li> <li>Crates, Pallets, Jumbo bags, Food packaging, Drums and Bundles (As per the product)</li> <li>Best product you can export in small budget</li> <li>List of product you can Export in small budget</li> <li>Start Export in Small Budget</li> </ul>","tags":["Export","Import","ExIm"]},{"location":"Insights/Industry%20Insights/2022-01-04%20Export%20Import/#module-3-government-benefits","title":"Module 3 : Government Benefits","text":"<ul> <li>Merchandise Exports from India Scheme (MEIS)</li> <li>Duty Drawback Scheme (DBK)</li> <li>Advance Authorisation (AA)</li> <li>Duty-Free Import Authorisation</li> <li>Market Access Initiative (MAI) Scheme</li> <li>EPCG (Export Promotion Capital Goods) Scheme</li> <li>Duty-Free Import Authorisation</li> <li>RoDTEP (Remission of Duties or Taxes on Export Products) Scheme</li> <li>Export-Oriented Units (EOUs)</li> <li>Software Technology Parks (STPs)</li> <li>Electronics Hardware Technology Parks (EHTPs)</li> <li>Biotechnology Parks (BTPs)</li> <li>Risk Management</li> <li>Understanding Risk in International Market</li> <li>How to manage Payment risk</li> <li>How to Manage Currency Risk</li> <li>How to Manage Goods risk</li> <li>ECGC (Export credit Guarantee Corporation of India)</li> <li>Role of ECGC (Export credit guarantee corporation)*</li> <li>What is ECGC?</li> <li>Risk covered by ECGC</li> <li>Types of policy by ECGC</li> <li>TypesExport Insurance</li> <li>Goods insurance (Marine insurance)</li> <li>Process of claim and settlement</li> <li>Debt recovery agencies</li> </ul>","tags":["Export","Import","ExIm"]},{"location":"Insights/Industry%20Insights/2022-01-04%20Export%20Import/#module-4-how-to-select-target-market","title":"Module 4: How to select Target Market","text":"<ul> <li>Practical analysis of Country</li> <li>Market Research</li> <li>How to select Market for Export</li> <li>Market Selection</li> <li>HS code of product</li> <li>How to find HS code of Product</li> <li>Practical market analysis</li> <li>Selection of market</li> <li>Where to export?</li> <li>Select the country or Region</li> <li>Where to export?</li> <li>Identification of right country to export your product</li> <li>Identifying genuine buyers</li> </ul>","tags":["Export","Import","ExIm"]},{"location":"Insights/Industry%20Insights/2022-01-04%20Export%20Import/#module-5-how-to-do-international-marketing","title":"Module 5 : How to do international marketing","text":"<ul> <li>What is International Marketing?</li> <li>Exporter Importer Profile</li> <li>IEC RCMC GST LUT</li> <li>Name of Company</li> <li>Company Logo</li> <li>Company stationary</li> <li>Website and Email ID</li> <li>Social Media profile</li> <li>Become Export Ready</li> <li>Why Buyer will buy from you?</li> <li>Export Promotion council Benefits</li> <li>List of EPC with Product</li> <li>Find your EPC for your product</li> </ul>","tags":["Export","Import","ExIm"]},{"location":"Insights/Industry%20Insights/2022-01-04%20Export%20Import/#module-6-ways-of-finding-buyers","title":"Module 6 : Ways of Finding Buyers","text":"<ul> <li>Practical Video of Finding buyers</li> <li>How to find buyer Online</li> <li>Way of finding Buyer Online</li> <li>Offline Buyer finding</li> <li>B2B websites</li> <li>B2B Sites Registration and Free Listing</li> <li>B2B and B2C Concept</li> <li>Exhibition and trade fair</li> <li>Practical how to find buyer from exhibition and trade fair</li> <li>How to do Business set up out of India</li> <li>How to send sample for export</li> <li>Open office abroad</li> <li>Export Promotion council</li> <li>Indian Embassy</li> <li>List of Indian embassies abroad</li> <li>Trade Delegation</li> <li>Country wise Yellow Pages</li> <li>Country wise Trade Directories</li> <li>Personal Visit in Target market</li> <li>Trade Associations Directory</li> <li>How to find buyer from Agent</li> <li>Friends and relative</li> <li>How to use paid data to find buyer</li> <li>How to find suppliers for product</li> <li>How to check supplier is genuine</li> <li>One to one meeting with suppliers</li> </ul>","tags":["Export","Import","ExIm"]},{"location":"Insights/Industry%20Insights/2022-01-04%20Export%20Import/#module-7-digital-marketing","title":"Module 7 : Digital Marketing","text":"<ul> <li>Practical Digital Marketing</li> <li>Social media profile</li> <li>Google my business registration</li> <li>Facebook marketing</li> <li>linkedin marketing</li> <li>Instagram Marketing</li> <li>Paid marketing in Google and social media</li> <li>PPC Pay per click Paid marketing</li> <li>Keywords to find buyers</li> <li>Google map uses</li> <li>How to use Google</li> <li>Email marketing</li> <li>IMP websites</li> </ul>","tags":["Export","Import","ExIm"]},{"location":"Insights/Industry%20Insights/2022-01-04%20Export%20Import/#module-8-communication","title":"Module 8 : Communication","text":"<ul> <li>Inside of International Communication (Practical)</li> <li>How to do international communication</li> <li>Practical Email formats</li> <li>Practical Email to find buyer</li> <li>Approach Effectively to Overseas Buyer</li> <li>Communicate with Buyers professional way</li> <li>Reaching the Right Person</li> <li>Language Problem with Solution</li> <li>Ways of Communication</li> <li>How to contact buyers</li> <li>How to call buyers</li> <li>How to do social media communication</li> </ul>","tags":["Export","Import","ExIm"]},{"location":"Insights/Industry%20Insights/2022-01-04%20Export%20Import/#module-9-logistics","title":"Module 9 : Logistics","text":"<ul> <li>Practical logistic process</li> <li>Step by Step Logistic process</li> <li>Export pricing practically</li> <li>How to do by Sea Export</li> <li>How to do by Air Export</li> <li>How to Do by Road Export</li> <li>What is Logistics?</li> <li>Incoterms 2020 (Responsibility of Buyer and Seller during export-import business)</li> <li>Ex-works, FCA, FAS, FOB, CFR, CIF, CPT, CIP, DPU, DAP, DDP</li> <li>Types of container</li> <li>Standard Container</li> <li>Open Top Container</li> <li>Flat Rack Container</li> <li>Refrigerated and insulated Container</li> <li>Tank Container</li> <li>Types of ships and its working in export</li> <li>Types of Shipping Line</li> <li>How to do stuffing in Export</li> <li>Factory stuffing</li> <li>ICD Stuffing</li> <li>Port/Dock stuffing</li> <li>Packaging of Product</li> <li>Different type of packaging</li> <li>How to do export packing</li> <li>How to get Quotation from CHA</li> <li>How to calculate pricing</li> <li>Freight Calculation FCL / LCL</li> <li>What is bill of lading?</li> <li>What is Air Waybill?</li> <li>What is LR Lorry Receipt?</li> <li>How to give Quotation</li> <li>Procedure of Export Clearance</li> <li>CHA \u2013 Custom House Agent</li> <li>What is shipping bill in Export?</li> <li>Transshipments / Partial Shipments</li> <li>EDI Registration /AD code Registration</li> <li>Procedure of Export Clearance</li> <li>Shipping Bill Filing</li> <li>Custom Clearance Procedure</li> <li>Custom website www.icegate.gov.in</li> <li>Export Custom Process</li> <li>Custom house agent (CHA)</li> <li>Role of CHA</li> <li>Shipping bill</li> <li>Custom examination</li> <li>Indian Government Bodies</li> <li>Directorate general of foreign (DGFT)</li> <li>Customs</li> <li>RBI - Reserve Bank of India</li> <li>Ministry of commerce (MOC)</li> <li>Federation of Indian export organizations (FIEO)</li> <li>Export promotion councils (EPC\u2019s)</li> <li>APEDA, EEPC, AEPC Spice Boards</li> <li>Export Inspection council of India</li> <li>International bodies</li> <li>World trade organization (WTO)</li> <li>ICC - international chamber of commerce</li> <li>ITC - international trade council</li> </ul>","tags":["Export","Import","ExIm"]},{"location":"Insights/Industry%20Insights/2022-01-04%20Export%20Import/#module-10-documentation","title":"Module 10: Documentation","text":"<ul> <li>Document Practical</li> <li>How to prepare document practically</li> <li>Different type of documents in Export</li> <li>Company Registration</li> <li>Current Bank count</li> <li>IEC Import Export code</li> <li>RCMC - Registration cum Membership Certificate</li> <li>GST - Goods and Services Tax</li> <li>LUT - Letter of Undertaking</li> <li>AD Code Registration</li> <li>ISO Certificate</li> <li>FSSAI Certificate for Food</li> <li>ADC Certificate for Pharma</li> <li>FDA Certificate</li> <li>GMP Certificate</li> <li>HACCP Certificate</li> <li>Kosher Certificate</li> <li>Quotation</li> <li>Performa Invoice/ Contract</li> <li>Invoice/Commercial Invoice</li> <li>Packing List</li> <li>Bill of lading (BL)</li> <li>Air waybill (AWB)</li> <li>LR Lorry Receipt (LR)</li> <li>E-way bill</li> <li>COO - Certificate of origin</li> <li>Fumigation Certificate</li> <li>Phytosanitary certificate</li> <li>Insurance Certificate</li> <li>Testing Report of product</li> <li>Health Certificate</li> <li>MSDS Material Safety data sheet</li> <li>Third Party inspection Certificate</li> <li>Charter Engineer certificate</li> <li>Declaration if any</li> <li>Shipping Bill</li> <li>Bill of entry</li> <li>VGM copy (Verified Gross Mass)</li> <li>NOC From government</li> <li>NOC from Manufacturer</li> <li>GSP (Generalized System of Preferences)</li> <li>E-BRC Electronic Bank Realization Certificate</li> <li>Bill of exchange</li> <li>EIA Export Inspection Agency/council Certificate</li> <li>Pre-Shipment Documentation</li> <li>Post-Shipment Documentation</li> </ul>","tags":["Export","Import","ExIm"]},{"location":"Insights/Industry%20Insights/2022-01-04%20Export%20Import/#module-11-export-import-banking","title":"Module 11: Export Import Banking","text":"<ul> <li>Inside of Export import Banking</li> <li>How to get Finance for Export</li> <li>Roll of Bank in Export import</li> <li>AD Code Registration</li> <li>How to get AD code from bank</li> <li>Pre Shipment Finance</li> <li>Post Shipment Finance</li> <li>How to get finance by bank</li> <li>By financial Institute</li> <li>By Friends and Family</li> <li>What is FEMA?</li> <li>Roll of RBI and Bank</li> <li>RBI - Reserve Bank of India</li> <li>Payment Terms DA DP Advance</li> <li>Method of Payment Transfer</li> <li>Banking and Finance</li> <li>Bank Role in Foreign Trade</li> <li>Letter of Credit (L.C.) with crucial knowledge</li> <li>Type of Letter of Credit</li> <li>Irrevocable Letter of Credit</li> <li>Revocable Letter of Credit</li> <li>Standby Letter of Credit</li> <li>Transferable Letter of Credit</li> <li>Confirmed Letter of Credit</li> <li>Terms and condition in LC</li> <li>Letter of Credit Practical</li> <li>Letter of Credit Discounting</li> <li>Practical payment movement</li> <li>Swift Transfer, TT payment</li> <li>Forward contract</li> <li>EEFC- exchange earners' foreign currency account</li> <li>PCFC pre-shipment Credit in foreign currency</li> </ul>","tags":["Export","Import","ExIm"]},{"location":"Insights/Industry%20Insights/2022-01-04%20Export%20Import/#module-12-how-to-do-import","title":"Module 12 : How to do Import","text":"<ul> <li>How to do import practically</li> <li>Feasibility study in import</li> <li>Import Duty in Import</li> <li>How to calculate import duty</li> <li>How to Select Product for Import</li> <li>How to select Country for Import</li> <li>Practical Country selection</li> <li>Practical calculations of Import Duty</li> <li>How to pay import duty</li> <li>Custom examination</li> <li>Opportunities in Import</li> <li>Import business Cycle</li> <li>Anti-Dumping Duty</li> <li>Need of inspection</li> <li>High Sea Sales in Import</li> <li>Third Country Export</li> <li>How to make Order in Import</li> <li>How to make practical order in import</li> <li>Practical order process</li> </ul>","tags":["Export","Import","ExIm"]},{"location":"Insights/Industry%20Insights/2022-01-04%20Export%20Import/#module-13-import-logistic","title":"Module 13 : Import Logistic","text":"<ul> <li>Incoterms in Import Incoterms 2020</li> <li>Ex-works, FCA, FAS, FOB, CFR,</li> <li>CIF, CPT, CIP, DPU, DAP, DDP</li> <li>How to do by Sea Import</li> <li>How to do by Air Import</li> <li>How to do by Road Import</li> <li>Import logistic Pricing</li> <li>How to do import pricing</li> <li>Import pricing calculation</li> <li>How to pay import duty</li> <li>Types of container and Ship</li> <li>Standard Container</li> <li>Open Top Container</li> <li>Flat Rack Container</li> <li>Refrigerated and insulated Container</li> <li>Tank Container</li> <li>Size of Container</li> <li>LCL Import or FCL import</li> <li>Custom Process in import</li> <li>Custom Clearance Procedure</li> <li>Roll of CHA - Custom House Agent</li> <li>What is bill of Entry in Import?</li> <li>AD code Registration in Import</li> <li>Custom examination</li> <li>Indian Government Bodies</li> <li>DGFT Customs RBI</li> <li>What is bill of lading?</li> <li>What is Air Waybill?</li> <li>What is LR Lorry Receipt?</li> <li>FCL and LCL import</li> <li>Import Quotation from CHA</li> </ul>","tags":["Export","Import","ExIm"]},{"location":"Insights/Industry%20Insights/2022-01-04%20Export%20Import/#module-14-import-documentation","title":"Module 14 : Import Documentation","text":"<ul> <li>Different type of documents in Import</li> <li>What Document require for import</li> <li>Company Registration</li> <li>Current Bank count</li> <li>IEC Import Export code</li> <li>GST - Goods and Services Tax</li> <li>AD Code Registration</li> <li>FSSAI for Food Import</li> <li>Pollution Control Board certificate</li> <li>ADC for Pharma Import</li> <li>NOC from Government</li> <li>Special Permission from Government</li> <li>Bill of entry</li> <li>Bond &amp; Ex-Bond Bill of Entry</li> <li>Need Document from Supplier/Exporter</li> <li>Quotation</li> <li>Performa Invoice/ Contract</li> <li>Invoice/Commercial Invoice</li> <li>Packing List</li> <li>Bill of lading (BL)</li> <li>Air waybill (AWB)</li> <li>LR Lorry Receipt (LR)</li> <li>E-way bill</li> <li>COO - Certificate of origin</li> <li>Fumigation Certificate</li> <li>Phytosanitary certificate</li> <li>Insurance Certificate</li> <li>Testing Report of product</li> <li>Health Certificate</li> <li>MSDS Material Safety data sheet</li> <li>Third Party inspection Certificate</li> <li>Charter Engineer certificate</li> <li>Declaration if any</li> <li>GSP (Generalized System of Preferences)</li> </ul>","tags":["Export","Import","ExIm"]},{"location":"Insights/Industry%20Insights/2022-01-04%20Export%20Import/#module-15-import-banking","title":"Module 15 : Import Banking","text":"<ul> <li>How to do banking for Import</li> <li>How to Open Bank Account</li> <li>AD code letter from bank</li> <li>How to get AD code from bank</li> <li>How to get Finance for Import</li> <li>Ways of getting finance</li> <li>How to open LC for Import</li> <li>Irrevocable Letter of Credit</li> <li>Revocable Letter of Credit</li> <li>Standby Letter of Credit</li> <li>Transferable Letter of Credit</li> <li>Confirmed Letter of Credit</li> <li>Terms and condition in LC</li> <li>Letter of Credit Practical</li> <li>Letter of Credit Discounting</li> <li>Best Payment Term in Import</li> <li>Practical payment movement</li> <li>Swift Transfer, TT payment</li> <li>Forward contract</li> <li>How to send money in import</li> </ul> <p>Resource </p>","tags":["Export","Import","ExIm"]},{"location":"Insights/Industry%20Insights/2023-01-17%20Building%20Information%20Modeling/","title":"Building Information Modeling","text":"","tags":["Construction"]},{"location":"Insights/Industry%20Insights/2023-01-17%20Building%20Information%20Modeling/#bim-overview","title":"BIM Overview","text":"<p>Building Information Modeling (BIM) is a digital representation of a building or structure, which can be used to collect and manage a variety of data, including geometric data, spatial data, material data, equipment data, cost data, scheduling data, environmental data, safety data, maintenance data, and facility management data. BIM is a process that uses a combination of 3D modeling software, data management tools and workflows, to create a digital twin of the building throughout its entire lifecycle, from design and construction, to operation and maintenance. BIM is used to manage the entire building process and provides a comprehensive understanding of the building, it's components and systems, and how they all interact. BIM enables collaboration and coordination among architects, engineers, contractors and facility managers, and allows for greater efficiency, accuracy, and cost savings in the construction process. It can also be used to analyze and optimize the performance of a building over time, including energy efficiency, safety, and maintenance.</p>","tags":["Construction"]},{"location":"Insights/Industry%20Insights/2023-01-17%20Building%20Information%20Modeling/#data-in-building-information-modeling","title":"Data In Building Information Modeling","text":"<p>BIM is a digital representation of a building or structure, which can be used to collect and manage a variety of data, including:</p> <ol> <li>Geometric data: This includes information about the size, shape, and location of building components such as walls, floors, and roofs.</li> <li>Spatial data: This includes information about the layout and organization of a building, including rooms, corridors, and other spaces.</li> <li>Material data: This includes information about the materials used in construction, such as the type and size of building components, as well as information about their properties and performance.    </li> <li>Equipment data: This includes information about the mechanical, electrical, and plumbing systems used in a building, including details about equipment such as HVAC, lighting, and plumbing fixtures.</li> <li>Cost data: This includes information about the cost of construction, including estimates for materials, labor, and equipment.</li> <li>Scheduling data: This includes information about the construction schedule, including timelines for completion and milestones.</li> <li>Environmental data: This includes information about the energy performance of a building, including information about heating, cooling, and ventilation systems.</li> <li>Safety data : this includes information about the safety hazards and emergency plans for a building.</li> <li>Maintenance data: This includes information about the maintenance and repair needs of a building, including details about equipment and systems that require regular maintenance.</li> <li>Facility management data: This includes information about the operation and management of the building over time, including details about usage, occupancy, and energy consumption.</li> </ol>","tags":["Construction"]},{"location":"Insights/Industry%20Insights/2023-01-17%20Building%20Information%20Modeling/#machine-learning-in-building-information-modeling","title":"Machine Learning In Building Information Modeling","text":"<p>There are several ways that data collected from a BIM model can be used to build machine learning models and solve real-world problems in construction, including:</p> <ol> <li>Predictive maintenance: By analyzing data about the performance and usage of equipment and systems in a building, machine learning models can be used to predict when maintenance or repairs are needed, allowing for proactive maintenance and reducing downtime.</li> <li>Energy efficiency: By analyzing data about the energy performance of a building, machine learning models can be used to identify opportunities for energy savings and optimize the operation of heating, cooling, and ventilation systems.</li> <li>Quality control: By analyzing data about the construction process, machine learning models can be used to identify potential quality issues, such as defects in materials or deviations from plans, allowing for early detection and correction.</li> <li>Resource optimization: By analyzing data about the costs and schedule of construction, machine learning models can be used to optimize the allocation of resources, such as labor and materials, to minimize costs and maximize efficiency.</li> <li>Safety: By analyzing data about safety hazards and emergency plans, machine learning models can be used to predict and prevent accidents and improve overall safety.</li> <li>Workforce management: By analyzing data about the workforce, machine learning models can be used to optimize the scheduling of labor, track the skill sets of construction personnel and identify areas where training is needed.</li> <li>Simulation and visualization: By using data from BIM models, machine learning models can be used to simulate different scenarios and visualize the impacts of different design options, allowing for more informed decision making.</li> <li>Automation: By using data from BIM models, machine learning models can be used to automate certain processes such as scheduling, resource optimization, and quality control, reducing the need for manual intervention and increasing efficiency.</li> <li>Predictive modeling for construction: By using data from BIM models and other sources, machine learning models can be used to predict the outcomes of different construction scenarios, such as the potential for delays, cost overruns, and other issues.</li> <li>Facility management: By using data from BIM models, machine learning models can be used to optimize the operation and management of a building over time, including details about usage, occupancy, and energy consumption.</li> </ol>","tags":["Construction"]},{"location":"Insights/Industry%20Insights/2023-01-17%20Building%20Information%20Modeling/#iot-in-building-information-modeling","title":"IOT In Building Information Modeling","text":"<p>IoT (Internet of Things) technology can be integrated with BIM to enhance the capabilities and functionality of BIM models, some ways of using IoT in BIM are:</p> <ol> <li>Real-time monitoring: IoT devices can be installed in a building to monitor various parameters such as temperature, humidity, lighting, and occupancy, allowing for real-time data collection and analysis.    </li> <li>Automation: IoT devices can be used to automate various systems in a building, such as lighting, heating, and ventilation, allowing for greater energy efficiency and cost savings.</li> <li>Predictive maintenance: IoT devices can be used to monitor the performance of equipment and systems in a building, allowing for early detection of potential issues and proactive maintenance.</li> <li>Safety and security: IoT devices can be used to monitor the safety and security of a building, including fire alarms, motion sensors, and cameras, allowing for early detection of potential hazards and improved emergency response.</li> <li>Asset management: IoT devices can be used to track the location and condition of assets within a building, such as furniture, equipment, and tools, allowing for improved inventory management and cost savings.</li> <li>Remote access: IoT devices can be used to provide remote access to a building, allowing for remote monitoring and control of systems and equipment, as well as remote access for occupants and visitors.</li> <li>Building performance: IoT devices can be used to track and analyze the performance of a building over time, including energy consumption, occupancy, and other factors, allowing for improved building management and cost savings.</li> <li>BIM integration: IoT data can be integrated with BIM models to provide real-time information and improve the accuracy and functionality of the BIM model, providing more accurate simulations, visualizations and predictions.</li> <li>Smart City: By integrating IoT with BIM, the city can be more efficient in managing the energy consumption, tracking the performance of buildings, and optimizing the use of resources.</li> <li>Augmented Reality: By integrating IoT with BIM, the use of augmented reality can be implemented in the maintenance and repair processes, providing real-time information to the workers, and increasing the overall efficiency of the process.</li> </ol>","tags":["Construction"]},{"location":"Insights/Technical%20Insights/2022-01-04%20SQL%20Tutorial/","title":"SQL Tutorial","text":"<p>This tutorial is designed for people who want to answer questions with data. SQL is used for accessing, cleaning, and analyzing data that's stored in databases. It's very easy to learn, yet it's employed by the world's largest companies to solve incredibly challenging problems.</p> <p>Why should I learn SQL when I already know Pandas : 2022-12-15 Big Query Vs Tensorflow Transform</p>","tags":["SQL","Data Science","Bigquery"]},{"location":"Insights/Technical%20Insights/2022-01-04%20SQL%20Tutorial/#generic-sql-select-statement","title":"Generic SQL Select statement","text":"<p>The Generic sql select statement is made up of 4 elemental clauses  </p> <pre><code>select\n  -- (some columns)\nfrom\n  -- (some tables)\n  -- (a subquery)\n  -- (a\u00a0JOIN\u00a0clause)\nwhere\n  -- (A 'predicate' expression to be used as a filter)\ngroup by\n  -- (some columns)\n</code></pre> <p>Code snippets will be written in the Google BigQuery Standard SQL syntax</p>","tags":["SQL","Data Science","Bigquery"]},{"location":"Insights/Technical%20Insights/2022-01-04%20SQL%20Tutorial/#from-clause","title":"From Clause","text":"<p>The sources defined in the\u00a0FROM\u00a0clause determines the set of rows that the\u00a0SELECT\u00a0statement can operate on hence it is the first part of the query to be executed.</p>","tags":["SQL","Data Science","Bigquery"]},{"location":"Insights/Technical%20Insights/2022-01-04%20SQL%20Tutorial/#table-as-input","title":"Table as input","text":"<pre><code>with my_table as (select * from unnest([\n  struct('a' as strings, 1 as numbers),\n  struct('b' as strings, 2 as numbers),\n  struct('c' as strings, 3 as numbers)\n])) \n-- ^ The SELECT above is just defining some data to use\nselect\n  *\nfrom\n  my_table\n</code></pre>","tags":["SQL","Data Science","Bigquery"]},{"location":"Insights/Technical%20Insights/2022-01-04%20SQL%20Tutorial/#subquery-as-input","title":"Subquery as input","text":"<pre><code>select\n  *\nfrom\n  (select * from unnest([\n    struct('a' as strings, 1 as numbers),\n    struct('b' as strings, 2 as numbers),\n    struct('c' as strings, 3 as numbers)\n  ])) \n</code></pre> <p>Instead of referring to a table by name, here we are refer the result of a\u00a0SELECT\u00a0statement  which is a tables</p> <p>Above query can also be written using a WITH clause and multiple nested subqueries</p> <pre><code>with my_table as (select * from unnest([\n  struct('a' as strings, 1 as numbers),\n  struct('b' as strings, 2 as numbers),\n  struct('c' as strings, 3 as numbers)\n])) \nselect\n  *\nfrom\n  (\n    select * from (\n      select * from (\n        select * from my_table\n      )\n    )\n  )\n</code></pre>","tags":["SQL","Data Science","Bigquery"]},{"location":"Insights/Technical%20Insights/2022-01-04%20SQL%20Tutorial/#join-clause-as-input","title":"Join clause as input","text":"<p>To select rows from multiple tables, first the JOIN clause is used to  merge the tables together and the result of this clause is used as input to the FROM keyword</p> <pre><code>with table_1 as (select * from unnest([\n  struct('a' as strings, 1 as numbers),\n  struct('b' as strings, 2 as numbers),\n  struct('c' as strings, 3 as numbers)\n])),\ntable_2 as (select * from unnest([\n  struct('a' as strings_2, 4 as numbers_2),\n  struct('b' as strings_2, 5 as numbers_2),\n  struct('c' as strings_2, 6 as numbers_2)\n]))\nselect\n  numbers,\n  numbers_2\nfrom \n  table_1\n  left join table_2 on table_1.strings = table_2.strings_2\n</code></pre> <p>Deciphering the above SELECT\u00a0statement : - We want the\u00a0numbers\u00a0and\u00a0numbers_2\u00a0columns (the database is smart enough to know which tables those columns come from) - FROM\u00a0table_1\u00a0and\u00a0table_2 - The rows of\u00a0table_2\u00a0should be associated with those from\u00a0table_1\u00a0by comparing the columns\u00a0strings\u00a0and\u00a0strings_2</p>","tags":["SQL","Data Science","Bigquery"]},{"location":"Insights/Technical%20Insights/2022-01-04%20SQL%20Tutorial/#join-clause","title":"Join Clause","text":"<p>JOIN\u00a0clause is used in a SELECT\u00a0statement to retrieve rows from multiple tables at the same time. It tells the database exactly how it should match those tables together. Following are the 5 Types of\u00a0JOIN\u00a0clause used in SQL </p>","tags":["SQL","Data Science","Bigquery"]},{"location":"Insights/Technical%20Insights/2022-01-04%20SQL%20Tutorial/#left-join","title":"Left Join","text":"<pre><code>with left_table as (select * from unnest([\n  struct('a' as strings, 1 as numbers),\n  struct('b' as strings, 2 as numbers),\n  struct('c' as strings, 3 as numbers)\n])),\nright_table as (select * from unnest([\n  struct('a' as strings, 4 as numbers),\n  struct('b' as strings, 5 as numbers)\n]))\n\nselect\n  *\nfrom\n  left_table\nleft join\n  right_table on left_table.strings = right_table.strings\n</code></pre> <p>Result of this query is :</p> <ul> <li>All columns from\u00a0table_1\u00a0and\u00a0table_2</li> <li>All rows from\u00a0table_1</li> <li>All rows from\u00a0table_2\u00a0where\u00a0column_2\u00a0matches the column\u00a0column_1\u00a0from\u00a0table_1</li> <li>The table on the left of the join clause keeps all of its rows, even if there isn't a matching row in the table on the right. If there is no matching row in the table on the right, the columns from the right will be filled with\u00a0NULLs.</li> </ul>","tags":["SQL","Data Science","Bigquery"]},{"location":"Insights/Technical%20Insights/2022-01-04%20SQL%20Tutorial/#right-join","title":"Right Join","text":"<pre><code>with left_table as (select * from unnest([\n  struct('a' as strings, 1 as numbers),\n  struct('b' as strings, 2 as numbers)\n])),\nright_table as (select * from unnest([\n  struct('a' as strings, 4 as numbers),\n  struct('b' as strings, 5 as numbers),\n  struct('c' as strings, 6 as numbers)\n]))\n\nselect\n  *\nfrom\n  left_table\nright join\n  right_table on left_table.strings = right_table.strings\n</code></pre> <p>Result of this query is :</p> <ul> <li>All columns from\u00a0table_1\u00a0and\u00a0table_2</li> <li>All rows from\u00a0table_2</li> <li>All rows from\u00a0table_1\u00a0where\u00a0column_1\u00a0matches the column\u00a0column_2\u00a0from\u00a0table_2</li> <li>The table on the right of the join clause keeps all of its rows, even if there isn't a matching row in the table on the left. If there is no matching row in the table on the left, the columns from the left will be filled with\u00a0NULLs.</li> </ul>","tags":["SQL","Data Science","Bigquery"]},{"location":"Insights/Technical%20Insights/2022-01-04%20SQL%20Tutorial/#inner-join","title":"Inner Join","text":"<pre><code>with left_table as (select * from unnest([\n  struct('a' as strings, 1 as numbers),\n  struct('b' as strings, 2 as numbers),\n  struct('c' as strings, 3 as numbers)\n])),\nright_table as (select * from unnest([\n  struct('a' as strings, 4 as numbers),\n  struct('b' as strings, 5 as numbers),\n  struct('d' as strings, 6 as numbers)\n]))\n\nselect\n  *\nfrom\n  left_table\ninner join\n  right_table on left_table.strings = right_table.strings\n</code></pre> <p>Result of this query is :</p> <ul> <li>All columns from\u00a0table_1 and\u00a0table_2</li> <li>All rows from both tables where the values in\u00a0column_1\u00a0and\u00a0column_2\u00a0both match and exist</li> <li>Only the rows which match from both tables will be included in the result. The values 'c' and 'd' won't appear in the output, as those values don't exist in both tables.</li> </ul>","tags":["SQL","Data Science","Bigquery"]},{"location":"Insights/Technical%20Insights/2022-01-04%20SQL%20Tutorial/#outer-join","title":"Outer Join","text":"<pre><code>with left_table as (select * from unnest([\n  struct('a' as strings, 1 as numbers),\n  struct('b' as strings, 2 as numbers),\n  struct('c' as strings, 3 as numbers)\n])),\nright_table as (select * from unnest([\n  struct('a' as strings, 4 as numbers),\n  struct('b' as strings, 5 as numbers),\n  struct('d' as strings, 6 as numbers)\n]))\n\nselect\n  *\nfrom\n  left_table\nfull outer join\n  right_table on left_table.strings = right_table.strings\n</code></pre> <p>Result of this query is :</p> <ul> <li>All columns from\u00a0table_1\u00a0and\u00a0table_2</li> <li>All rows from both tables</li> <li>All rows will be included in the result, even if they don't exist in one of the tables. If there is no matching row in one of the tables, the missing columns will be filled with\u00a0NULLs. Values 'c' and 'd' will appear in the output, even though they only exist in one table each.</li> </ul>","tags":["SQL","Data Science","Bigquery"]},{"location":"Insights/Technical%20Insights/2022-01-04%20SQL%20Tutorial/#cross-join","title":"Cross Join","text":"<pre><code>with left_table as (select * from unnest([\n  struct('a' as strings, 1 as numbers),\n  struct('b' as strings, 2 as numbers)\n])),\nright_table as (select * from unnest([\n  struct('a' as strings, 4 as numbers),\n  struct('b' as strings, 5 as numbers),\n  struct('c' as strings, 6 as numbers)\n]))\n\nselect\n  *\nfrom\n  left_table\ncross join\n  right_table\n</code></pre> <p>Result of this query is :</p> <ul> <li>All columns from\u00a0table_1\u00a0and\u00a0table_2</li> <li>All\u00a0possible\u00a0combinations\u00a0of rows from both tables</li> <li>Number of rows returned is the product of the number of rows of the two tables. Number of rows in the output would be  2 x 3 = 6</li> </ul>","tags":["SQL","Data Science","Bigquery"]},{"location":"Insights/Technical%20Insights/2022-01-04%20SQL%20Tutorial/#where-clause","title":"Where Clause","text":"<p>The\u00a0WHERE\u00a0clause is followed by a\u00a0predicate\u00a0expression, an expression which returns\u00a0true\u00a0or\u00a0false.  For a given row, if the predicate returns\u00a0true then, that row is included in the output of the query else that row is excluded.</p>","tags":["SQL","Data Science","Bigquery"]},{"location":"Insights/Technical%20Insights/2022-01-04%20SQL%20Tutorial/#predicate-expression","title":"Predicate Expression","text":"<p>To create a predicate\u00a0expression we can use the following comparitives</p> <ul> <li>Inequality</li> <li>Equality</li> <li>IS NOT NULL</li> <li>IS NULL</li> <li>IN</li> <li>BETWEEN</li> <li>AND / OR</li> </ul> <pre><code>with my_table as (select * from unnest([null,1,2,3,4,null,5]) as numbers)\nselect numbers from my_table\nwhere\n  --- One of the below statements\n  numbers &gt; 2\n  numbers = 2\n  numbers &gt; (select 1+1)\n  numbers is null\n  numbers is not null\n  numbers in (1,2)\n  numbers between 2 and 4 -- Note - these limits are inclusive\n  numbers &gt; 2 and (numbers &lt; 3 or numbers &gt; 4)\n</code></pre> <p>Predicate Expression can also contain Functions </p> <pre><code>with my_table as (select * from unnest(['a', 'b', 'ab', 'aa', 'bb', 'cc']) as strings)\nselect strings from my_table\nwhere\n  -- Predicate expressions can contain functions\n  (strings like 'a%' or strings like 'b%') and length(strings) = 2\n</code></pre>","tags":["SQL","Data Science","Bigquery"]},{"location":"Insights/Technical%20Insights/2022-01-04%20SQL%20Tutorial/#date-and-time","title":"Date and Time","text":"<p>Date and time functionality and syntax differs between databases. In this article the code snippets are written in the Google BigQuery Standard SQL syntax</p>","tags":["SQL","Data Science","Bigquery"]},{"location":"Insights/Technical%20Insights/2022-01-04%20SQL%20Tutorial/#basic-datetime-data-types","title":"Basic Datetime data types","text":"<ul> <li>DATE\u00a0: calendar date</li> <li>DATETIME\u00a0: calendar date and time</li> <li>TIMESTAMP\u00a0: a particular moment in time, default timezone UTC </li> <li>TIME\u00a0: a time as seen on a watch</li> </ul> <pre><code>select\n  date('2020-01-01') as date,\n  datetime(2020, 1, 1, 0, 0, 0) as datetime,\n  timestamp('2020-01-01T00:00:00.000Z') as timestamp,\n  time(0, 0, 0) as time\n</code></pre>","tags":["SQL","Data Science","Bigquery"]},{"location":"Insights/Technical%20Insights/2022-01-04%20SQL%20Tutorial/#basic-functions","title":"Basic Functions","text":"<ul> <li>Cast is used to change datatype of a variable explicitly from one data type to another</li> </ul> <pre><code>select\n  -- string -&gt; date\n  cast('2020-01-01' as date) as date, \n  -- string -&gt; datetime,\n  cast('2020-01-01' as datetime) as datetime,\n  -- string -&gt; timestamp,\n  cast('2020-01-01' as timestamp) as timestamp,\n  -- string -&gt; datetime -&gt; date. \n  cast(cast('2020-01-01 00:00:00' as datetime) as date) as date_2,\n</code></pre> <ul> <li>parse_date is used to convert a string to Date</li> </ul> <pre><code>select\nparse_date('%Y-%m-%d', '2020-01-01') as iso_date\n</code></pre> <ul> <li>timestamp_seconds and date_from_unix_date are used to convert a number to Date</li> </ul> <pre><code>select\n  -- This is the number of seconds between the start of 1970 and 2020\n  -- This function returns a timestamp\n  timestamp_seconds(1577836800) as timestamp,\n  -- This is the number of days between the start of 1970 and 2020\n  -- This function returns a date\n  date_from_unix_date(18262) as date,\n</code></pre> <ul> <li>format_date is used to format dates and times</li> </ul> <pre><code>with my_date as (\n  select\n    date('2020-01-01') as date,\n)\nselect\n  format_date('%Y', date) as year_only,\n  format_date('%Y-%m', date) as year_month,\n  format_date('%b-%d', date) as month_day,\nfrom \n  my_date\n</code></pre> <ul> <li>date_add and date_sub are used in arithmetic operations in date time</li> </ul> <pre><code>with my_date as (\n  select\n    date('2020-01-01') as date,\n)\nselect\n  -- This is special BigQuery syntax for intervals\n  date_add(date, interval 1 day) as tomorrow,\n  date_sub(date, interval 1 day) as yesterday,\n  date_add(date, interval 10 year) as ten_years_away,\nfrom \n  my_date\n</code></pre>","tags":["SQL","Data Science","Bigquery"]},{"location":"Insights/Technical%20Insights/2022-12-15%20Big%20Query%20Vs%20Tensorflow%20Transform/","title":"Big Query Vs Tensorflow Transform","text":""},{"location":"Insights/Technical%20Insights/2022-12-15%20Big%20Query%20Vs%20Tensorflow%20Transform/#data-transformation","title":"Data Transformation","text":"<p>In this section we will see what are the google product which we can use for Data Transformation</p>"},{"location":"Insights/Technical%20Insights/2022-12-15%20Big%20Query%20Vs%20Tensorflow%20Transform/#bigquery","title":"BigQuery","text":"<p> What is BigQuery ?</p> <p>According to google : \"BigQuery is Google Cloud's fully managed, petabyte-scale, and cost-effective analytics data warehouse that lets you run analytics over vast amounts of data in near real time. With BigQuery, there's no infrastructure to set up or manage, letting you focus on finding meaningful insights using standard SQL and taking advantage of flexible pricing models across on-demand and flat-rate options\"</p>"},{"location":"Insights/Technical%20Insights/2022-12-15%20Big%20Query%20Vs%20Tensorflow%20Transform/#notes-about-bigquery","title":"Notes about BigQuery","text":"<ul> <li>Serverless Infrastructure with a Scale Friendly Pricing Structure. This also reduces chances of failure</li> <li>Its Flexible Architecture Speeds Up Queries</li> <li>SQL is very easy to work with and is quite popular</li> <li>Access the Data You Need on Demand</li> </ul>"},{"location":"Insights/Technical%20Insights/2022-12-15%20Big%20Query%20Vs%20Tensorflow%20Transform/#tensorflow-transform","title":"TensorFlow Transform","text":"<p> What is TensorFlow Transform ?</p> <p>TensorFlow Transform is\u00a0a library for preprocessing input data for TensorFlow and it lets you define both instance-level and full-pass data transformations through data preprocessing pipelines. These pipelines are efficiently executed with\u00a0Apache Beam\u00a0and they create as byproducts a TensorFlow graph to apply the same transformations during prediction as when the model is served.</p>"},{"location":"Insights/Technical%20Insights/2022-12-15%20Big%20Query%20Vs%20Tensorflow%20Transform/#notes-about-tensorflow-transform","title":"Notes about TensorFlow Transform","text":"<ul> <li>TensorFlow transform is a hybrid of\u00a0Apache Beam and TensorFlow</li> <li>Infrastructure must be defined in Dataflow and costs would depend on the cluster size </li> <li>Speed of execution depends highly on your Dataflow configurations </li> <li>It can be used to perform almost all types of transformation and It is as easy as writing python code</li> <li>It is not yet possible to do Window aggregations transformation with tf.Transform</li> </ul> Data preprocessing option Instance-level (stateless transformations) Full-pass during training and instance-level during serving    (stateful transformations) Real-time (window) aggregations during training and serving (streaming    transformations) BigQuery (SQL) Batch scoring: OK \u2014the same transformation implementation is      applied on data during training and batch scoring. Online prediction: Not recommended \u2014you can process training data,      but it results in training-serving skew because you process serving data      using different tools. Batch scoring: Not recommended . Online prediction: Not recommended . Although you can use statistics computed using BigQuery      for instance-level batch/online transformations, it isn't easy because      you must maintain a stats store to be populated during training and      used during prediction. Batch scoring: N/A \u2014aggregates like these are computed based on      real-time events. Online prediction: Not recommended \u2014you can process training data,      but it results in training-serving skew because you process serving data      using different tools. Dataflow (Apache Beam) Batch scoring: OK \u2014the same transformation implementation is      applied on data during training and batch scoring. Online prediction: OK \u2014if data at serving time comes from      Pub/Sub to be consumed by Dataflow.      Otherwise, results in training-serving skew. Batch scoring: Not recommended . Online predictions: Not recommended . Although you can use statistics computed using Dataflow      for instance-level batch/online transformations, it isn't easy      because you must maintain a stats store to be populated during training      and used during prediction. Batch scoring: N/A \u2014aggregates like these are computed      based on real-time events. Online prediction: OK \u2014the same Apache Beam transformation is      applied on data during training (batch) and serving (stream). Dataflow (Apache Beam + TFT) Batch scoring: OK \u2014the same transformation implementation is      applied to data during training and batch scoring. Online prediction: Recommended \u2014it avoids training-serving skew      and prepares training data up front. Batch scoring: Recommended . Online prediction: Recommended . Both uses are recommended because transformation logic and computed      statistics during training are stored as a TensorFlow      graph that's attached to the exported model for serving. Batch scoring: N/A \u2014aggregates like these are computed      based on real-time events. Online prediction: OK \u2014the same Apache Beam transformation is      applied on data during training (batch) and serving (stream). TensorFlow * ( input_fn &amp; serving_fn ) Batch scoring: Not recommended . Online prediction: Not recommended . For training efficiency in both cases, it's better to prepare the      training data up front. Batch scoring: Not Possible . Online prediction: Not Possible . Batch scoring: N/A \u2014aggregates like these are computed      based on real-time events. Online prediction: Not Possible ."},{"location":"Insights/Technical%20Insights/2022-12-15%20Big%20Query%20Vs%20Tensorflow%20Transform/#which-to-use","title":"Which to Use","text":"<ul> <li>If your data is in stored in BigQuery Tables It would be wise to perform as many transformations as possible in SQL, because of its low cost and very very high speed. </li> <li> <p>If the transformation are not possible using BigQuery then Switch to TensorFlow Transform </p> </li> <li> <p>Reference1</p> </li> <li>Reference2</li> </ul>"},{"location":"Insights/Technical%20Insights/2023-01-12%20Data%20Science%20Lifecycle/","title":"Data Science Lifecycle","text":""},{"location":"Insights/Technical%20Insights/2023-01-12%20Data%20Science%20Lifecycle/#understanding-business-requirements","title":"Understanding business requirements","text":"<p>It includes gathering information about the problem from the users and define the problem what is it.Sometimes it can become very tedious as we have to be very careful while gathering the information about the project.  </p>"},{"location":"Insights/Technical%20Insights/2023-01-12%20Data%20Science%20Lifecycle/#data-acquisition","title":"Data acquisition","text":"<p>It includes ETL(Extracts Transforms Loads) and most pouplar tool used for this step is SQL(DB,DW,log-files,Hadoop/Spark).Basically this is the step in which we understand where the data is stored and then obatins or loads the data which we required from various sources or files by using mainly sql.</p>"},{"location":"Insights/Technical%20Insights/2023-01-12%20Data%20Science%20Lifecycle/#data-preparation","title":"Data preparation","text":"<p>Now in this step we will have all the data we need whoch we have acquired from the previous step.Now our work is to clean and pre-pocessed the data,so that it can be made useful for making nay ML algo work.Form e.g removing any special character from the text data if any present,handling missing values or NaN values,standardization,featurization.For this we first try to figure what the data is by applying various libraries and tries to print the data and check if there is any incorrect data or the points which is not useful in our model and then after detecting we removes those points or clean the text data.  </p>"},{"location":"Insights/Technical%20Insights/2023-01-12%20Data%20Science%20Lifecycle/#exploratory-data-analysis","title":"Exploratory Data Analysis","text":"<p>In \u00a0this data visualization is done by using various plots.Most commonly used plots are scatter plots,box plot ,heat map,violon plot,bar plot,contour plot,line plot,pdf,cdf,histogram,geo-map,pair-plots,Q-Q plots,t-SNE etc.Basocally by doing we try figure out what are the features that are important and understand the content of the data more accurately and what trend it is following.  </p>"},{"location":"Insights/Technical%20Insights/2023-01-12%20Data%20Science%20Lifecycle/#modeling-evaluation-interpretation","title":"Modeling, Evaluation &amp; Interpretation","text":"<p>This is the step where we actually apply various ML algorithms like LR,NB,SVM etc and form the model by evaluating on datasets like D_train,D_CV,D_test and tuning hyperparamter to avoid overfitting or underfittig.Applying any model is not easy,first we have to understand that what are algos which will be perfect for the problem and dataset,and then identifying the right performnace metric is aso an imprtant task in modeling.So we have to try the ML algos with different metrics and try to figure out which one is doing best.  </p>"},{"location":"Insights/Technical%20Insights/2023-01-12%20Data%20Science%20Lifecycle/#communicate-results","title":"Communicate results","text":"<p>Clean &amp; simple : After making the model this step include communicating the results with the stakeholders or users or executives for which we are making this model.The results should be very clean and understandable,because there can be case where persons like users may not know machine learning.This is also one of the very important steps as it includes convicing the manager/users by communicating the results,if this step fails then all the hardwork of making nodel will go in vain,even if you have made the good project but bad communication can fail your success. </p>"},{"location":"Insights/Technical%20Insights/2023-01-12%20Data%20Science%20Lifecycle/#deployment","title":"Deployment","text":"<p>Once we got the approval from the previos step,now it's time the deploy the model which is s software engineering effort. It can be employed sometimes by machine learning engineer or by sofware engineer.</p>"},{"location":"Insights/Technical%20Insights/2023-01-12%20Data%20Science%20Lifecycle/#real-world-testing-ab-testing","title":"Real world testing-A/B testing","text":"<p>In this step the model is tested in the real world environment by using A/B testing.Bascially this is the process where we test our model on real world and get the results which will be helpful in the next step.</p>"},{"location":"Insights/Technical%20Insights/2023-01-12%20Data%20Science%20Lifecycle/#customer-buy-in","title":"Customer buy-in","text":"<p>In this step we try to convince the user in our model by showing the results of the model on real world environment which we have obtained in the previous step. </p>"},{"location":"Insights/Technical%20Insights/2023-01-12%20Data%20Science%20Lifecycle/#operations-retrain-models-handle-failures","title":"Operations : retrain models, handle failures","text":"<p>This includes identifying that when,how to retrain the models from time to time which \u00a0we have deployed already in tyhe real world.And if any failure cases occur in the future and handling that case is also includes in this step.</p>"},{"location":"Insights/Technical%20Insights/2023-01-12%20Data%20Science%20Lifecycle/#optimization-improve-models-more-data-more-features-optimize-code","title":"Optimization : improve models, more data, more features, optimize code","text":"<p>This is the last step in which we can add more data,more features and improving the models according to the requirement.This is the step which we need to keep doing from time to time according to the requirements and trend in the real world.</p>"},{"location":"Insights/Tips%20%26%20Tricks/2022-01-02%20Jupyter%20Lab/","title":"Jupyter Lab","text":"","tags":["Jupyter Lab"]},{"location":"Insights/Tips%20%26%20Tricks/2022-01-02%20Jupyter%20Lab/#set-password","title":"Set Password","text":"<pre><code>jupyter lab password\n</code></pre>","tags":["Jupyter Lab"]},{"location":"Insights/Tips%20%26%20Tricks/2022-01-02%20Jupyter%20Lab/#how-to-access-jupyter-notebook-remotely","title":"How to access Jupyter notebook remotely","text":"<pre><code>jupyter lab --ip 0.0.0.0 --port 8080 \n</code></pre> <pre><code>python -m ipykernel install --user --name=githib_env\n</code></pre>","tags":["Jupyter Lab"]},{"location":"Insights/Tips%20%26%20Tricks/2022-01-02%20Jupyter%20Lab/#install-extensions","title":"Install Extensions","text":"<pre><code>conda install nodejs\njupyter labextension install jupyterlab_tensorboard\njupyter labextension install jupyterlab_voyager\njupyter labextension install @mflevine/jupyterlab_html\njupyter labextension install @jupyterlab/toc\njupyter labextension install @jupyterlab/google-drive\npip install jupyterlab-drawio\npip install ipympl\npip install jupyterlab_execute_time\npip install jupyter_bokeh\npip install jupyterlab-git\npip install jupyterlab-tabular-data-editor\npip install jupyterlab-spreadsheet-editor\n</code></pre>","tags":["Jupyter Lab"]},{"location":"Insights/Tips%20%26%20Tricks/2022-01-02%20Jupyter%20Lab/#ipyml-usage","title":"ipyml Usage","text":"<pre><code>%matplotlib widget\n</code></pre>","tags":["Jupyter Lab"]},{"location":"Insights/Tips%20%26%20Tricks/2022-01-02%20Jupyter%20Lab/#kite-engine","title":"Kite Engine","text":"<pre><code>bash -c \"$(wget -q -O - https://linux.kite.com/dls/linux/current)\"\npip install \"jupyterlab-kite&gt;=2.0.2\"\n</code></pre>","tags":["Jupyter Lab"]},{"location":"Insights/Tips%20%26%20Tricks/2022-01-02%20Jupyter%20Lab/#embed-videos-in-jupyter-lab","title":"Embed Videos in Jupyter Lab","text":"<pre><code>from IPython.display import Video\nvideo_loc = \"path_to_video.mp4\"\nVideo(video_loc,embed=True)\n\n## Youtube Video\nfrom IPython.display import HTML\nHTML(\"\"\"&lt;iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/MKzkmyiNX0k\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen&gt;&lt;/iframe&gt;\"\"\")\n</code></pre>","tags":["Jupyter Lab"]},{"location":"Insights/Tips%20%26%20Tricks/2022-01-03%20Obsidian%20Tricks/","title":"Obsidian Tricks","text":"<ol> <li>How to convert Markdown files to Medium articles</li> <li> <p>Obsidian Plugins</p> <p></p> </li> <li> <p>Dataview Plugin</p> <p></p> </li> <li> <p>Useful Templates for Obsidian</p> <p></p> </li> <li> <p>Obsidian Data Views <pre><code>table  file.path as path\nwhere file.name = \"README\"\nsort path desc\n\ntable file.ctime as creation, file.mtime as modification,  split(file.path, \"/\")[1] as folder  , status from  \n\"Public\"\nwhere file.name != \"README\"\nsort folder desc\n</code></pre></p> </li> </ol>"},{"location":"Insights/Tips%20%26%20Tricks/2022-01-04%20Pop%20Os%20Setup/","title":"Pop Os Setup","text":"","tags":["PoP OS","linux","tools"]},{"location":"Insights/Tips%20%26%20Tricks/2022-01-04%20Pop%20Os%20Setup/#remove-top-bar","title":"Remove top bar","text":"<p>https://ubuntuhandbook.org/index.php/2020/08/top-panel-auto-hide-ubuntu-20-04/</p> <pre><code>cd ~\n# Basic Update\nsudo apt-get update &amp;&amp; sudo apt-get upgrade -y\nsudo apt-get install -y python3-pip htop python-is-python3 python3-venv python3-opencv\n\n# Install Google Chrome\nwget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\nsudo apt install -y ./google-chrome-stable_current_amd64.deb\nrm ./google-chrome-stable_current_amd64.deb\n\n# Install Sublime\nwget -qO - https://download.sublimetext.com/sublimehq-pub.gpg | sudo apt-key add -\nsudo apt-get install -y apt-transport-https\necho \"deb https://download.sublimetext.com/ apt/stable/\" | sudo tee /etc/apt/sources.list.d/sublime-text.list\nsudo apt-get update\nsudo apt-get install -y sublime-text\n\n# Install VLC\nsudo apt-get install -y vlc\n\n# Install Docker\nsudo apt-get install -y apt-transport-https ca-certificates curl gnupg lsb-release\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\necho \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt-get update\nsudo apt-get install -y docker-ce docker-ce-cli containerd.io\n\n# Install Heroku CLI\ncurl https://cli-assets.heroku.com/install.sh | sh \n\n# Install SQlite3\nsudo apt-get install sqlite3\nsudo apt-get install -y  sqlitebrowser\n\n# Git config\ngit config --global user.email \"harshmaheshwari3110@gmail.com\"\ngit config --global user.name \"Harsh-Maheshwari\"\n\n# Zipline-reloded config\nsudo apt-get install -y libatlas-base-dev python-dev gfortran pkg-config libfreetype6-dev hdf5-tools\n\n# Install TAlib for zipline\nwget http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz\ntar -xzf ta-lib-0.4.0-src.tar.gz\nrm ta-lib-0.4.0-src.tar.gz\ncd ta-lib/\nsudo ./configure\nsudo make\nsudo make install\necho 'export LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH' &gt;&gt; ~/.bashrc\n\n# Portfolio Setup\nPortfolio_dir=\"/media/harsh/DATA/Harsh/Portfolio\" \nmkdir $Portfolio_dir/python_environments/\ncd $Portfolio_dir/python_environments/\npython -m venv portfolio_env\necho 'source /media/harsh/DATA/Harsh/Portfolio/python_environments/portfolio_env/bin/activate' &gt;&gt; ~/.bashrc \necho 'export PATH=\"/home/harsh/.local/bin:$PATH\"' &gt;&gt; ~/.bashrc \necho 'Portfolio_dir=\"/media/harsh/DATA/Harsh/Portfolio/\"'&gt;&gt; ~/.bashrc\nsource $Portfolio_dir/python_environments/portfolio_env/bin/activate\npip3 install -r $Portfolio_dir/secondary_requirements.txt\npip3 install -r $Portfolio_dir/requirements.txt\nsource ~/.bashrc\n\n# Install CMake 3.20.5\nsudo apt-get install -y build-essential libssl-dev\nwget https://github.com/Kitware/CMake/releases/download/v3.20.0/cmake-3.20.0.tar.gz\ntar -zxvf cmake-3.20.0.tar.gz\nrm cmake-3.20.0.tar.gz\ncd cmake-3.20.0\n./bootstrap\nmake\nsudo make install\n\n\n#Adding SSH For Github\nssh-keygen -t ed25519 -C \"harshmaheshwari3110@gmail.com\"\neval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/id_ed25519\ncat ~/.ssh/id_ed25519.pub\n</code></pre>","tags":["PoP OS","linux","tools"]},{"location":"Insights/Tips%20%26%20Tricks/2022-01-04%20Pop%20Os%20Setup/#install-pantheon-file-system","title":"Install Pantheon File System","text":"<pre><code>sudo add-apt-repository ppa:elementary-os/stable\nsudo apt-get update\nsudo apt-get install pantheon-files\n</code></pre> <p>Github Desktop for linux https://github.com/shiftkey/desktop/releases</p>","tags":["PoP OS","linux","tools"]},{"location":"Insights/Tips%20%26%20Tricks/2022-01-05%20Markdown%20Tricks/","title":"Markdown Tricks","text":"","tags":["markdown","mkdocs"]},{"location":"Insights/Tips%20%26%20Tricks/2022-01-05%20Markdown%20Tricks/#jupyter-book","title":"Jupyter Book","text":"<p>https://jupyterbook.org/en/stable/intro.html</p>","tags":["markdown","mkdocs"]},{"location":"Insights/Tips%20%26%20Tricks/2022-01-05%20Markdown%20Tricks/#libraries-with-jupyter-book","title":"libraries with Jupyter Book","text":"<p>jupytext mkdocs-jupyter mkdocs-awesome-pages-plugin</p>","tags":["markdown","mkdocs"]},{"location":"Insights/Tips%20%26%20Tricks/2022-01-05%20Markdown%20Tricks/#mkdocs","title":"Mkdocs","text":"","tags":["markdown","mkdocs"]},{"location":"Insights/Tips%20%26%20Tricks/2022-01-05%20Markdown%20Tricks/#reproducible-reports-with-mkdocs","title":"Reproducible Reports with MkDocs","text":"","tags":["markdown","mkdocs"]},{"location":"Insights/Tips%20%26%20Tricks/2022-01-05%20Markdown%20Tricks/#mkdocs-setup","title":"Mkdocs Setup","text":"<pre><code>pip install -r requirements.txt # Installs python librarires\nmkdocs serve -f mkdocs.yml # Serves docs Folder locally \nmkdocs serve -f mkdocs.yml --dev-addr 0.0.0.0:80 # Serves docs Folder in the local network \nmkdocs build -f mkdocs.yml # Builds docs Site locally \n</code></pre>","tags":["markdown","mkdocs"]},{"location":"Insights/Tips%20%26%20Tricks/2022-01-05%20Markdown%20Tricks/#mkdocs-plugins","title":"Mkdocs Plugins","text":"<pre><code>https://github.com/mkdocs/mkdocs/wiki/MkDocs-Plugins#pdf--site-conversion\nhttps://github.com/greenape/mknotebooks\nhttps://facelessuser.github.io/pymdown-extensions/extras/slugs/\nhttps://chrieke.medium.com/the-best-mkdocs-plugins-and-customizations-fc820eb19759\n</code></pre>","tags":["markdown","mkdocs"]},{"location":"Insights/Tips%20%26%20Tricks/2022-01-05%20Markdown%20Tricks/#mknotebooks","title":"mknotebooks","text":"<p>Add this Css to get pandas dataframe to display properly <pre><code>table {\n    display: block;\n    max-width: -moz-fit-content;\n    max-width: fit-content;\n    margin: 0 auto;\n    overflow-x: auto;\n    white-space: nowrap;\n  }\n</code></pre></p>","tags":["markdown","mkdocs"]},{"location":"Insights/Tips%20%26%20Tricks/2022-01-05%20Markdown%20Tricks/#admonitions","title":"Admonitions","text":"<ul> <li>warning</li> <li>example</li> <li>note </li> <li>error</li> <li>info</li> <li>question     ```</li> </ul>","tags":["markdown","mkdocs"]},{"location":"Insights/Tips%20%26%20Tricks/2022-01-05%20Markdown%20Tricks/#collapsable","title":"Collapsable","text":"<pre><code>??? note annotate \"ABS\"\n    ### Try Heading\n    - **A** : AA\n</code></pre> <pre><code>!!! quote\n    Romain Clement\u2019s datasette-dashboards plugin lets you configure dashboards for Datasette using YAML, combining markdown blocks, Vega graphs and single number metrics using a layout powered by CSS grids. This is a beautiful piece of software design, with a very compelling live demo.\n\n    -- &lt;cite&gt;Simon Willison - Creator of Datasette ([source](https://simonwillison.net/2022/Apr/7/datasette-dashboards/))&lt;/cite&gt;\n</code></pre>","tags":["markdown","mkdocs"]},{"location":"Insights/Tips%20%26%20Tricks/2022-01-05%20Markdown%20Tricks/#toggles","title":"Toggles","text":"<pre><code>===  \"Client 1\"\n    A\n\n===  \"Client 2\"\n    B\n\n===  \"Client 3\"\n    C\n</code></pre>","tags":["markdown","mkdocs"]},{"location":"Insights/Tips%20%26%20Tricks/2022-01-05%20Markdown%20Tricks/#mixture","title":"Mixture","text":"<pre><code>!!! warning \"Automatically generated files\"\n    === \"Unordered List\"\n        ```markdown\n            * Sed sagittis eleifend rutrum       \n        ```\n    === \"Ordered List\"\n        ```markdown\n            1. Sed sagittis eleifend rutrum\n        ```\n</code></pre>","tags":["markdown","mkdocs"]},{"location":"Insights/Tips%20%26%20Tricks/2022-01-05%20Markdown%20Tricks/#markdown-badges","title":"Markdown Badges","text":"<p>https://github.com/Ileriayo/markdown-badges/edit/master/README.md</p>","tags":["markdown","mkdocs"]},{"location":"Insights/Tips%20%26%20Tricks/2022-01-05%20Markdown%20Tricks/#font-awesome","title":"Font Awesome","text":"<pre><code>[Github Repo] [repository]\n[:fontawesome-solid-globe: Live Demo][demo]\n\n[repository]: https://github.com/harsh-maheshwari/harsh-maheshwari \"GitHub Repository\"\n[demo]: https://datasette-dashboards-demo.vercel.app \"Live Demo\"\n</code></pre>","tags":["markdown","mkdocs"]},{"location":"Insights/Tips%20%26%20Tricks/2022-01-05%20Markdown%20Tricks/#fastpages","title":"FastPages","text":"<p>powered by fastpages</p> <ul> <li>Writing Blogs With Jupyter</li> <li>Writing Blogs With Markdown</li> <li>Writing Blog Posts With Word</li> </ul>","tags":["markdown","mkdocs"]},{"location":"Insights/Tips%20%26%20Tricks/2022-01-05%20Markdown%20Tricks/#links","title":"Links","text":"<p>Record an Unrecordable Video Use Screen Cast on Phone and Screen Video and Microphone Audio Recorder</p>","tags":["markdown","mkdocs"]},{"location":"Knowledge/","title":"Know Your Guide","text":"","tags":["Welcome","Data Science","Journey"]},{"location":"Knowledge/#hi-there-im-harsh-maheshwari","title":"Hi there, I'm Harsh Maheshwari  \ud83d\udc4b","text":"<pre><code>- Hindi: \u0939\u0930\u094d\u0937  \n- Meaning: Joy\n- Identity: Data Scientist and Automation Freak\n</code></pre>","tags":["Welcome","Data Science","Journey"]},{"location":"Knowledge/#why-i-do-what-i-do","title":"Why I Do What I Do","text":"<p>I am a soul searching the meaning of life in Science and Religion. I am passionate about work, curious about science, driven by Minimalism, aware of my limits. I love data, enjoy travel, value loyalty, desire independence, appreciate honesty, trust life and above all respect time. I am currently working as a Data Scientist at Blue Yonder. I am also available as a freelance Data Science Consultant and a mentor for upcoming data science aspirants. Lastly, I am still a learner. Every day I push myself to learn something new, whether that be about machine learning, Financial Markets, or miscellaneous facts about the universe and physics. </p> <p>I love to watch anime some of my favorites are Dragon Ball Super, Death Note and Vinland Saga. I believe these productions represent the real depth of human psychology, efforts and emotions. My ultimate goal is a healthy and wealthy life for me and people around me. I believe in team work, extreme determination and self-belief as ingredients to success.</p>","tags":["Welcome","Data Science","Journey"]},{"location":"Knowledge/#summaries","title":"Summaries","text":"","tags":["Welcome","Data Science","Journey"]},{"location":"Knowledge/#professional-summaries","title":"Professional Summaries","text":"Employer Profile Guide Start - End Blue Yonder Data Scientist Karthik Rampelli April 2022 - Present Australia New Zealand Bank Enterprise Data Automation Engineer YS Madhav July 2021 - March 2022 Axis Bank Business Intelligence Intern Santanu Dutta April 2020 - June 2020 School of Computing National University of Singapore Artificial Intelligence Research Scholar Professor Bryan Hooi November 2019 - December 2019 Innovation Cell IITB Computer Vision Lead Professor Dhwanil Shukla October 2019 - March 2020 PV Diagnostics Data Analyst Pranav Maheshwari November 2018 - December 2018","tags":["Welcome","Data Science","Journey"]},{"location":"Knowledge/#educational-summaries","title":"Educational Summaries","text":"Examination University Institute Year Grade Graduation Indian Institute of Technology Bombay Department of Chemical Engineering 2021 7.77/10 Exchange Semester Denmark Technical University Chemical Engineering 2019 Included in Graduation Intermediate Maharashtra State Board Somalwar Junior College 2017 88.92/100 Matriculation Central Board of Secondary Education Gondia Public School 2015 97.2/100","tags":["Welcome","Data Science","Journey"]},{"location":"Knowledge/#courses-completed-in-graduation","title":"Courses completed in Graduation","text":"Freshman Year Courses Sophomore Year Courses Junior  Year Courses Final Year Courses Introduction to Chemical Engineering Introduction to Numerical Analysis Introduction to Data Analysis Artificial Intelligence in Process Engineering Calculus Differential Equations II Process Control Techno-Commerical Aspects of Fine Chemicals Linear Algebra Computational Methods Lab Process Modelling and Identification Process Equipment Selection Differential Equations Optimization Models Mass Transfer I Process Economics Engineering Graphics &amp; Drawing Introducing to Machine Learning Mass Transfer II Design Lab I Computer Programming and Utilization Introduction to Transport Phenomena Chemical Processes Process Equipment Design Workshop Practice Chemical Engineering Thermodynamics I Chemical Reaction Engineering Chemical Process Design Biology Psychology Solid Mechanics Process Design Project Organic &amp; Inorganic Chemistry Technical Thermodynamics * Understanding Design Environmental Studies: Science and Engineering Physical Chemistry Heat Transmission * Economics Environmental Studies: Humanities Basics of Electricity &amp; Magnetism Fluid Mechanics * Quantum Physics and Application Materials Technology * General Electrical Engineering * Chemical Unit Operations * <p>* Courses completed at Denmark Technical University</p>","tags":["Welcome","Data Science","Journey"]},{"location":"Knowledge/#certifications","title":"Certifications","text":"<ol> <li>Data Science Foundations Data Engineering</li> <li>Eckovation Big Data Certificate</li> <li>Exploratory Data Analysis</li> <li>Getting and Cleaning Data</li> <li>Google Analytics Course Certificate</li> <li>Google Analytics Individual Qualification</li> <li>R Programming</li> <li>The Data Scientist Toolbox</li> </ol>","tags":["Welcome","Data Science","Journey"]},{"location":"Knowledge/#skills-badges","title":"Skills Badges","text":"","tags":["Welcome","Data Science","Journey"]},{"location":"Knowledge/#machine-learning-deep-learning","title":"Machine Learning &amp; Deep Learning","text":"","tags":["Welcome","Data Science","Journey"]},{"location":"Knowledge/#databases","title":"Databases","text":"","tags":["Welcome","Data Science","Journey"]},{"location":"Knowledge/#apache","title":"Apache","text":"","tags":["Welcome","Data Science","Journey"]},{"location":"Knowledge/#ml-infrastructure","title":"ML Infrastructure","text":"","tags":["Welcome","Data Science","Journey"]},{"location":"Knowledge/#ci","title":"CI","text":"","tags":["Welcome","Data Science","Journey"]},{"location":"Knowledge/#programming-languages","title":"Programming Languages","text":"","tags":["Welcome","Data Science","Journey"]},{"location":"Knowledge/#web-frameworks","title":"Web Frameworks","text":"","tags":["Welcome","Data Science","Journey"]},{"location":"Knowledge/#front-end","title":"Front End","text":"","tags":["Welcome","Data Science","Journey"]},{"location":"Knowledge/#my-journey","title":"My Journey","text":"","tags":["Welcome","Data Science","Journey"]},{"location":"Knowledge/#the-rough-game-of-jee-2015-2017","title":"The Rough Game of JEE 2015-2017","text":"<p>After Completing my standard 10<sup>th</sup> in 2015, I left home for the first time to become an engineer and started my journey at IIT Home, Nagpur. I spent most of the next two years studying for JEE Advanced, listening to music, and meditating. Yep, It Works! After achieving an AIR of 735 in JEE Mains and 1382 in JEE Advanced, I joined the Indian Institute of Technology Bombay as a Chemical Engineering Fresher.</p>","tags":["Welcome","Data Science","Journey"]},{"location":"Knowledge/#2017-june-2018-june","title":"2017 June - 2018 June","text":"<p>Freshman year, I explored college events like XLR8 [Made my first manually controlled car], volunteered in Mood Indigo [Cultural Fest of IIT Bombay], participated in the annual entrepreneurship Summit of IIT Bombay [E-Summit]. My team achieved 3<sup>rd</sup> position among 30+ teams from IIT Bombay in Andro-NG, an Android app development competition. Completed a yearlong volleyball training and Won 3<sup>rd</sup> position in Street Play Arcade and Gold in Literary Arts GC for Hostel 2 [The Wild Ones]. And like any other freshmen, attended Blockchain seminars, Finance workshops, Start-up Bootcamps, and much more, only to realize that nothing helps until you do it yourself.</p>","tags":["Welcome","Data Science","Journey"]},{"location":"Knowledge/#2018-june-2018-december","title":"2018 June - 2018 December","text":"<p>Sophomore year, Through some of the course works, I developed an interest in Machine Learning. My first-course project in machine learning was on Hate &amp; Offensive Speech Detection under Professor Sunita Sarawagi. I also joined Innovation Cell as a Finance and Infrastructure manager. In the December of 2018, I had my first professional work at PV-Diagnostics as a Data Analyst.</p>","tags":["Welcome","Data Science","Journey"]},{"location":"Knowledge/#the-international-year-of-2019","title":"The International year of 2019","text":"<p>One of the best parts of my college life includes the semester I spent in Denmark. I left India in January of 2019 to start my semester at the Technical University of Denmark as an exchange student. During my stay in Europe, I backpacked my way across 15 cities in 10 countries. I met many new people during my travel, learned a lot about European culture.\u00a0<code>#BackpackingThroughEurope</code>. As an exchange scholar, I worked in a Pilot-scale chemical plant on four different setups, namely Liquid-Liquid Extraction, Gas Flow in Pipes, Batch Distillation, Filtration in a Filter Press. During this semester, I understood the huge gap of professionalism in India Society and Education system. After a fantastic semester abroad, I came back to IIT Bombay and joined Innovation Cell again, but this time as a Computer Vision Engineer building deep learning models for advanced vehicles like automated drones for the Barcelona Smart Drone Challenge and self-driving cars. It was a learn and apply journey. Then in November of 2019, I was fortunate enough to grab a research opportunity at the National University of Singapore. I spent a month and a half researching deep learning methods and their application in Network Graphs and Link Prediction Problem in Singapore under the guidance of Professor Bryan Hooi.</p>","tags":["Welcome","Data Science","Journey"]},{"location":"Knowledge/#2020-the-year-of-coming-home-the-time-of-sars-cov-2","title":"2020 The Year of coming Home - The Time of SARS-CoV-2","text":"<p>After traveling so much, I knew the lifestyle I wanted. College experiences and internships helped me figure out I loved data science and deep learning. In subsequent semesters of junior year, I worked on many different projects involving deep learning and machine learning applications. I worked with professor Sharad Bhartiya on the Modelling &amp; Identification of Gas sensor data for a dynamic chemical gas mixture. In the summer of 2020, I grabbed an opportunity at Axis Bank to work in their Business Intelligence team. After two months of work, I created a micro-service on Kubernetes and Docker to predict customer income using Geospatial and Customer Information data. Due to the SARS-CoV-2 epidemic, many things changed, and quite a lot of effort got into adjusting to the new normal. Everyone was now working from home, and all colleges were now virtual. After a blissful summer, I started my final year working under Professor Sharad Bhartiya to study machine learning methods for the prediction of financial markets. Due to this project, my interest was peaking towards the financial markets, and my background in machine learning drove me towards algorithmic trading and quantitative finance. And then, in a LinkedIn post, I got to know about the book on generating Predictive Models to Extract Signals from Market by Stefan Jansen. It is one of the best books on applications of Machine learning in the Market. Placements for my batch began in the December of 2020. After three days of struggle, Australia and New Zealand bank Bangalore office accepted me for a Data Lifecycle Evolution role.</p>","tags":["Welcome","Data Science","Journey"]},{"location":"Knowledge/#the-last-semester-jan-2021-may-2021","title":"The Last Semester Jan 2021 - May 2021","text":"<p>During the last five months of my undergraduate degree, we were all still at home [Gift of SARS-CoV-2]. I will regret this to the end of time. But still, our professors made sure that we get the experience of the actual chemical industry as much as possible. Over the length of 5 months, I was involved in developing extensively detailed reports on producing three chemicals, namely Ethylene Oxide, Styrene, and Dihydromyrcene [my favorite]. These projects were part of our curriculum, and many professors contributed by helping us at various steps. As the end came near, everything became so nostalgic.</p>","tags":["Welcome","Data Science","Journey"]},{"location":"Knowledge/#the-first-job-june-2021-march-2022","title":"The First Job June 2021 - March 2022","text":"<p>I started working at ANZ Bank as a fresh graduate out of college. For the first few months I learned about the way data sourcing activities are taking place for the Enterprise data. The major technologies involved in this process were IBM Data Stage, Teradata and Control M. After understanding and manually going over the dataflow for the ANZ datalake, I was able to figure out many processes that could be automated in the daily workload. The next few months majority of my time spent taking input from teams regarding the usefulness and possible structure of such a system. After the initial approval I developed a complete Development and Test Automation Framework using Robot Framework and Python. I was able to minimize the effort to a level where we just had to fill an excel sheet and the rest would happen automatically. Starting from picking up data from a unix server to loading it into a Teradata Table using Data Stage. To verify if the data was loaded properly, set of python functions where developed which would log into Teradata to download the loaded data and compare with the original file. I can not go into more details about the product but this saved a lot of Development time and Testing time Other then my job work I lead a Tech4Grads initiative with the aim to bring technological awareness and cross collaboration in the Tech Domain. We ran projects like Building a Market Place and Tech Awareness. Both of these projects were developed as a POC under the Graduate community at ANZ and the aim was to bring about a cultural tradition in technology by new graduates that come to ANZ every year. One of the very interesting thing I learned at ANZ was Agile working and role of a scrum master in a team. For all of my next projects (at least personal projects) I have decided to use an Agile mindset and approach every thing for Agile First Approach.</p>","tags":["Welcome","Data Science","Journey"]},{"location":"Knowledge/#the-second-job-april-2022-present","title":"The Second Job April 2022 - Present","text":"","tags":["Welcome","Data Science","Journey"]},{"location":"Knowledge/#project-details","title":"Project Details","text":"","tags":["Welcome","Data Science","Journey"]},{"location":"Knowledge/#business-automation-and-efficiency","title":"Business Automation and Efficiency","text":"<p>Barcode Automation</p>","tags":["Welcome","Data Science","Journey"]},{"location":"Knowledge/#computer-vision-and-analytics","title":"Computer Vision and Analytics","text":"<p>Image Classification Face Detection and Recognition Object Detection Tracking  Semantic Segmentation Handwriting recognition Aerial detection Pose Detection</p>","tags":["Welcome","Data Science","Journey"]},{"location":"Knowledge/#data-science-and-visualization","title":"Data Science and Visualization","text":"<p>Customer Segmentation Customer Retention Sentiment Analysis Recommendation System Anomaly Detection</p>","tags":["Welcome","Data Science","Journey"]},{"location":"Knowledge/#graphs-and-network-analysis","title":"Graphs and Network Analysis","text":"<p>Link Prediction</p>","tags":["Welcome","Data Science","Journey"]},{"location":"Knowledge/#natural-language-processing","title":"Natural Language Processing","text":"<p>Document Analysis ChatBot and Virtual Assistant Voice Recognition and Authentication Audio based classification Text Classification and Summarization</p>","tags":["Welcome","Data Science","Journey"]},{"location":"Knowledge/#deep-learning-applications","title":"Deep Learning  Applications","text":"<p>Time Series Modelling</p>","tags":["Welcome","Data Science","Journey"]},{"location":"Knowledge/#business-automation-and-efficiency_1","title":"Business Automation and Efficiency","text":"","tags":["Welcome","Data Science","Journey"]},{"location":"Knowledge/#specific-projects","title":"Specific Projects","text":"","tags":["Welcome","Data Science","Journey"]},{"location":"Knowledge/#computer-vision-and-analytics_1","title":"Computer Vision and Analytics","text":"<ol> <li>Cifar 10</li> <li>Crack Detection</li> <li>Face Detection and Recognition with Emotion Detection</li> <li>Parking Spots Detection</li> <li>Barcelona Smart Drone Challenge</li> </ol>","tags":["Welcome","Data Science","Journey"]},{"location":"Knowledge/#data-science-and-visualization_1","title":"Data Science and Visualization","text":"<ol> <li>Customer Segmentation</li> <li>NYC Taxi Fare</li> <li>PV Diagnostics</li> <li>Scrapping Books</li> <li>Titanic</li> <li>Customer Retention</li> </ol>","tags":["Welcome","Data Science","Journey"]},{"location":"Knowledge/#graphs-and-network-analysis_1","title":"Graphs and Network Analysis","text":"<ol> <li>Link Prediction </li> </ol>","tags":["Welcome","Data Science","Journey"]},{"location":"Knowledge/#natural-language-processing_1","title":"Natural Language Processing","text":"<ol> <li>Hate Speech Detection</li> <li>Topic Modelling</li> <li>Question Answer Modeling </li> </ol>","tags":["Welcome","Data Science","Journey"]},{"location":"Knowledge/#deep-learning-applications_1","title":"Deep Learning  Applications","text":"<ol> <li>Process Modeling And System Identification</li> </ol> <p>Curious About The Website!! This is a mkdocs powered completely built using python and markdown, very easy to code and customize.</p>","tags":["Welcome","Data Science","Journey"]},{"location":"Knowledge/Machine%20Learning/2022-01-01%20Machine%20Learning%20Basics/","title":"Machine Learning Basics","text":"","tags":["Machine Learning","Data Science"]},{"location":"Knowledge/Machine%20Learning/2022-01-01%20Machine%20Learning%20Basics/#components-of-machine-learning","title":"Components of Machine Learning","text":"<p>Deep learning has helped beyond end-to-end training, we are experiencing a transition from parametric statistical descriptions to fully nonparametric models. When data are scarce, one needs to rely on simplifying assumptions about reality in order to obtain useful models. When data are abundant, this can be replaced by nonparametric models that fit reality more accurately. </p> <p>The Core components of a machine learning problem (like Regression, Classification, Tagging, Search/Ranking, Recommender Systems, Sequence Learning [Tagging and Parsing, Automatic Speech Recognition, Text to Speech, Machine Translation], Reinforcement Learning) always fall to the components described below </p> <ol> <li>The data that we can learn from.</li> <li>A model of how to transform the data.</li> <li>An objective function that quantifies how well (or badly) the model is doing.</li> <li>An algorithm to adjust the model\u2019s parameters to optimize the objective function.</li> </ol>","tags":["Machine Learning","Data Science"]},{"location":"Knowledge/Machine%20Learning/2022-01-01%20Machine%20Learning%20Basics/#data","title":"Data","text":"<ul> <li> <p>Dimensionality of data : If every example is characterized by the same number of numerical values i.e data has  fixed-length feature vectors then this length is the dimensionality of that data.</p> </li> <li> <p>Amount of Data: As the amount of data increases chances for a good model generally increases and the amount of assumptions made decreases. Many of the most exciting models in deep learning do not work without large datasets. If the amount of data is small then the results generally fall and are similar to traditional machine learning and Numerical approaches.</p> </li> <li> <p>Quality of Data <code>garbage in, garbage out</code>: Lots of data is a guarentee of nothing unless it is the right  data. What is right kind of data? Data without mistakes or human error, features present should be predictive of  the target quantity of interest, correct representation of the reality without under/over representation of  particular group(s). In sensitive applications of machine learning, like predictive policing, resume screening, and risk models used for lending, we must be especially alert to the consequences of garbage data. Failure can also occur when the data do not merely under-represent some groups but reflect societal prejudices bias.  Note that this can all happen without the data scientist actively conspiring, or even being aware.</p> </li> </ul>","tags":["Machine Learning","Data Science"]},{"location":"Knowledge/Machine%20Learning/2022-01-01%20Machine%20Learning%20Basics/#domains-of-data","title":"Domains of Data","text":"<ol> <li>Structured, unstructured, semi-structured</li> <li>Time-stamped data</li> <li>Machine data </li> <li>Spatiotemporal data</li> <li>Genomics data</li> <li>Qualitative/Categorical: Nominal, Ordinal</li> <li>Quantitative : Discrete [Interval], Continuous [Ratio]   </li> </ol>","tags":["Machine Learning","Data Science"]},{"location":"Knowledge/Machine%20Learning/2022-01-01%20Machine%20Learning%20Basics/#measurement-and-observation-of-data","title":"Measurement and Observation of Data","text":"<ul> <li>Nominal Data is observed but not measured, is unordered but non-equidistant, and have no meaningful zero</li> <li>Ordinal Data is observed but not measured, is ordered but non-equidistant, and has no meaningful zero.</li> <li>Interval Data are measured and ordered with the nearest items but have no meaningful zero.</li> <li>Ratio Data are measured and ordered with equidistant items and a meaningful zero </li> <li>If ordinal classes are numbered then is it discrete or ordinal? It is still ordinal. Even if the numbering is done, it doesn\u2019t convey the actual distances between the classes.</li> </ul>","tags":["Machine Learning","Data Science"]},{"location":"Knowledge/Machine%20Learning/2022-01-01%20Machine%20Learning%20Basics/#model","title":"Model","text":"<p>Models are the computational machinery for ingesting data of one type, and spitting out predictions of a possibly  different type. In particular, deep learning uses statistical models that can be estimated directly from data.</p>","tags":["Machine Learning","Data Science"]},{"location":"Knowledge/Machine%20Learning/2022-01-01%20Machine%20Learning%20Basics/#regression-models","title":"Regression Models","text":"<ul> <li>Ordinary Least Squares Regression (OLSR)</li> <li>Linear Regression</li> <li>Logistic Regression</li> <li>Stepwise Regression</li> <li>Multivariate Adaptive Regression Splines (MARS)</li> <li>Locally Estimated Scatterplot Smoothing (LOESS)</li> </ul>","tags":["Machine Learning","Data Science"]},{"location":"Knowledge/Machine%20Learning/2022-01-01%20Machine%20Learning%20Basics/#regularization-models","title":"Regularization Models","text":"<ul> <li>Ridge Regression</li> <li>Least Absolute Shrinkage and Selection Operator (LASSO)</li> <li>Elastic Net</li> <li>Least-Angle Regression (LARS)</li> </ul>","tags":["Machine Learning","Data Science"]},{"location":"Knowledge/Machine%20Learning/2022-01-01%20Machine%20Learning%20Basics/#instance-based-models","title":"Instance-based Models","text":"<ul> <li>k-Nearest Neighbor (kNN)</li> <li>Learning Vector Quantization (LVQ)</li> <li>Self-Organizing Map (SOM)</li> <li>Locally Weighted Learning (LWL)</li> </ul>","tags":["Machine Learning","Data Science"]},{"location":"Knowledge/Machine%20Learning/2022-01-01%20Machine%20Learning%20Basics/#geometric-models","title":"Geometric Models","text":"<ul> <li>Support Vector Machines (SVM)</li> </ul>","tags":["Machine Learning","Data Science"]},{"location":"Knowledge/Machine%20Learning/2022-01-01%20Machine%20Learning%20Basics/#decision-tree-models","title":"Decision Tree Models","text":"<ul> <li>Classification and Regression Tree (CART)</li> <li>Iterative Dichotomiser 3 (ID3)</li> <li>C4.5 and C5.0</li> <li>Chi-squared Automatic Interaction Detection (CHAID)</li> <li>Decision Stump</li> <li>M5</li> <li>Conditional Decision Trees</li> </ul>","tags":["Machine Learning","Data Science"]},{"location":"Knowledge/Machine%20Learning/2022-01-01%20Machine%20Learning%20Basics/#bayesian-models","title":"Bayesian Models","text":"<ul> <li>Naive Bayes</li> <li>Gaussian Naive Bayes</li> <li>Multinomial Naive Bayes</li> <li>Averaged One-Dependence Estimators (AODE)</li> <li>Bayesian Belief Network (BBN)</li> <li>Bayesian Network (BN)</li> </ul>","tags":["Machine Learning","Data Science"]},{"location":"Knowledge/Machine%20Learning/2022-01-01%20Machine%20Learning%20Basics/#clustering-models","title":"Clustering Models","text":"<ul> <li>k-Means</li> <li>k-Medians</li> <li>Expectation Maximisation (EM)</li> <li>Hierarchical Clustering</li> </ul>","tags":["Machine Learning","Data Science"]},{"location":"Knowledge/Machine%20Learning/2022-01-01%20Machine%20Learning%20Basics/#dimensionality-reduction-models","title":"Dimensionality Reduction Models","text":"<ul> <li>Principal Component Analysis (PCA)</li> <li>Principal Component Regression (PCR)</li> <li>Partial Least Squares Regression (PLSR)</li> <li>Sammon Mapping,</li> <li>Multidimensional Scaling (MDS)</li> <li>Projection Pursuit</li> <li>Linear Discriminant Analysis (LDA)</li> <li>Mixture Discriminant Analysis (MDA)</li> <li>Quadratic Discriminant Analysis (QDA)</li> <li>Flexible Discriminant Analysis (FDA)</li> </ul>","tags":["Machine Learning","Data Science"]},{"location":"Knowledge/Machine%20Learning/2022-01-01%20Machine%20Learning%20Basics/#ensemble-models","title":"Ensemble Models","text":"<ul> <li>Bootstrapped Aggregation (Bagging) : Random Forest</li> <li>Gradient Boosting Machines (Boosting) : AdaBoost, XGBoost, CatBoost,LightGBM</li> <li>Weighted Average (Blending)</li> <li>Stacked Generalization (Stacking)</li> <li>Cascading</li> </ul>","tags":["Machine Learning","Data Science"]},{"location":"Knowledge/Machine%20Learning/2022-01-01%20Machine%20Learning%20Basics/#artificial-neural-network-models","title":"Artificial Neural Network Models","text":"<ul> <li>Perceptron</li> <li>Multilayer Perceptrons (MLP)</li> <li>Back-Propagation</li> <li>Stochastic Gradient Descent</li> <li>Hopfield Network</li> <li>Radial Basis Function Network (RBFN</li> </ul>","tags":["Machine Learning","Data Science"]},{"location":"Knowledge/Machine%20Learning/2022-01-01%20Machine%20Learning%20Basics/#deep-learning-models","title":"Deep Learning Models","text":"<ul> <li>Convolutional Neural Network (CNN)</li> <li>Recurrent Neural Networks (RNNs)</li> <li>Long Short-Term Memory Networks (LSTMs)</li> <li>Stacked Auto-Encoders</li> <li>Deep Boltzmann Machine (DBM)</li> <li>Deep Belief Networks (DBN)</li> <li>Attention networks</li> <li>Transformers</li> </ul>","tags":["Machine Learning","Data Science"]},{"location":"Knowledge/Machine%20Learning/2022-01-01%20Machine%20Learning%20Basics/#association-rule-learning-algorithms","title":"Association Rule Learning Algorithms","text":"<ul> <li>Apriori algorithm</li> <li>Eclat algorithm</li> </ul>","tags":["Machine Learning","Data Science"]},{"location":"Knowledge/Machine%20Learning/2022-01-01%20Machine%20Learning%20Basics/#objective-functions","title":"Objective Functions","text":"<p>Objective functions define a formal mathematical system measures of how good (or bad) a models is at learning.  Note learning means improving at some task over time. Loss functions is another name for Objective Functions.</p> <ul> <li>Most common loss function to predict numerical values is squared error. </li> <li>Most common loss function for classification is to minimize error rate</li> <li>Training Data : It is used to learn the best values of our model\u2019s parameters by minimizing the loss incurred [Objective Function] on a sampled set consisting of some number of examples collected from the data for training </li> <li>Testing Data : It is used to test the models performance [Using Objective Function] on a sampled set consisting of some unseen examples collected from the data which were not used in traning  </li> </ul> <p>Results of a model should always be told for both traning and testing data. When a model performs well on the training set but fails to generalize to unseen data, it is said to be overfitted</p>","tags":["Machine Learning","Data Science"]},{"location":"Knowledge/Machine%20Learning/2022-01-01%20Machine%20Learning%20Basics/#different-types-of-objective-fuctions","title":"Different types of Objective Fuctions","text":"","tags":["Machine Learning","Data Science"]},{"location":"Knowledge/Machine%20Learning/2022-01-01%20Machine%20Learning%20Basics/#optimization-algorithms","title":"Optimization Algorithms","text":"<p>Algorithms are used for searching the best possible parameters for minimizing the loss function.  Popular optimization algorithms for deep learning are based on an approach called gradient descent.</p>","tags":["Machine Learning","Data Science"]},{"location":"Knowledge/Machine%20Learning/2022-01-01%20Machine%20Learning%20Basics/#different-types-of-optimization-algorithms","title":"Different types of Optimization Algorithms","text":"","tags":["Machine Learning","Data Science"]},{"location":"Knowledge/Machine%20Learning/2022-01-01%20Machine%20Learning%20Basics/#references","title":"References","text":"<ol> <li>D2L</li> <li>Wiki</li> <li>machinelearningmastery</li> </ol>","tags":["Machine Learning","Data Science"]},{"location":"Knowledge/Machine%20Learning/2022-01-02%20Linear%20Neural%20Networks/","title":"Linear Neural Networks","text":"<p>Regression refers to a set of methods for modeling the relationship between one or more independent variables and a dependent variable. </p>","tags":["Machine Learning","Data Science"]},{"location":"Knowledge/Machine%20Learning/2022-01-02%20Linear%20Neural%20Networks/#assumptions-in-linear-regression","title":"Assumptions in Linear Regression","text":"<p>we assume that the relationship between the independent variables x and the dependent variable y is linear, i.e., that y can be expressed as a weighted sum of the elements in x, given some noise on the observations.  Second, we assume that any noise is well-behaved (following a Gaussian distribution).</p>","tags":["Machine Learning","Data Science"]},{"location":"Knowledge/Machine%20Learning/2022-01-02%20Linear%20Neural%20Networks/#governing-equations","title":"Governing Equations","text":"\\[\\hat{y} =  w_{1}x_{1} + ... + w_{d}x_{d} + b$$ $$\\hat{y} = \\mathbf{w^Tx} + b\\] <p>If the Design matrix \\(\\mathbf{X}\\) contains one row for every example and one column for every feature then </p> \\[\\mathbf{\\hat{y}} = \\mathbf{Xw} + b\\] <p>Given features of a training dataset \\(\\mathbf{X}\\) and corresponding (known) labels \\(\\mathbf{y}\\), the goal of linear regression is to find the weight vector \\(\\mathbf{w}\\) and the bias term \\(\\mathbf{b}\\) that given features of a new data example sampled from the same distribution as \\(\\mathbf{X}\\), the new example's label will (in expectation) be predicted with the lowest error.</p>","tags":["Machine Learning","Data Science"]},{"location":"Knowledge/Machine%20Learning/2022-01-02%20Linear%20Neural%20Networks/#loss-function","title":"Loss Function","text":"<p>Before we start thinking about how to fit data with our model, we need to determine a measure of fitness. The loss function quantifies the distance between the real and predicted value of the target. The loss will usually be a non-negative number where smaller values are better and perfect predictions incur a loss of 0. The most popular loss function in regression problems is the squared error. When our prediction for an example i is \\(\\hat{y}^{(i)}\\) and the corresponding true label is \\(y^{(i)}\\), the squared error is given by :</p> \\[l^{i}(\\mathbf{w},b)=0.5(\\hat{y}^{i}-y^{i})^2\\] <p>To measure the quality of a model on the entire dataset of n examples, we simply average (or equivalently, sum) the losses on the training set.</p> \\[L(\\mathbf{w},b) = (1/n)* \\sum_{i=1}^{n} 0.5(\\mathbf{w^{T}x}^{i}+b-y^{i})^{2}\\] <p>When training the model, we want to find parameters \\((\\mathbf{w}^{*}; b^{*})\\) that minimize the total loss across all training examples:</p> \\[\\mathbf{w}^{*}; b^{*} = argmin$ $L(\\mathbf{w}; b)\\]","tags":["Machine Learning","Data Science"]},{"location":"Knowledge/Machine%20Learning/2022-01-02%20Linear%20Neural%20Networks/#analytic-solution","title":"Analytic Solution","text":"<p>linear regression can be solved analytically by applying a simple formula.</p> <p>Note</p> <p>Logistic Regression is Gaussian Naive Bayes</p>","tags":["Machine Learning","Data Science"]},{"location":"Knowledge/Machine%20Learning/2022-05-01%20Feature%20Engineering/","title":"Feature Engineering","text":"<p>Note</p> <p>Before creating new feature research the domain and do some literature review</p>","tags":["Machine Learning","Data Science","Feature Engineering"]},{"location":"Knowledge/Machine%20Learning/2022-05-01%20Feature%20Engineering/#handaling-different-data","title":"Handaling different data","text":"","tags":["Machine Learning","Data Science","Feature Engineering"]},{"location":"Knowledge/Machine%20Learning/2022-05-01%20Feature%20Engineering/#text-data","title":"Text Data","text":"","tags":["Machine Learning","Data Science","Feature Engineering"]},{"location":"Knowledge/Machine%20Learning/2022-05-01%20Feature%20Engineering/#text-processing","title":"Text Processing","text":"<ul> <li>Remove Html characters, punctuation</li> <li>Stop word removal</li> <li>Lower case conversion</li> <li>Stemming :  Porter Stemmer, Snowball Stemmer</li> <li>Lemmitizatiom : break a sentence into word  Ref1, Ref2 </li> </ul>","tags":["Machine Learning","Data Science","Feature Engineering"]},{"location":"Knowledge/Machine%20Learning/2022-05-01%20Feature%20Engineering/#text-to-vectors","title":"Text to Vectors","text":"","tags":["Machine Learning","Data Science","Feature Engineering"]},{"location":"Knowledge/Machine%20Learning/2022-05-01%20Feature%20Engineering/#bag-of-words-bow","title":"Bag of Words (BOW)","text":"<ul> <li>Semantic meaning of word is lost</li> <li>Binary/Boolean BOW</li> <li>Count BOW</li> <li>Uni-gram/ Bi-gram/N-gram</li> </ul>","tags":["Machine Learning","Data Science","Feature Engineering"]},{"location":"Knowledge/Machine%20Learning/2022-05-01%20Feature%20Engineering/#tf-idf-term-frequency-inverse-documents-frequency","title":"TF-IDF Term Frequency Inverse Documents Frequency","text":"<ul> <li>Semantic meaning of word is lost Ref1 Ref2</li> </ul>","tags":["Machine Learning","Data Science","Feature Engineering"]},{"location":"Knowledge/Machine%20Learning/2022-05-01%20Feature%20Engineering/#word2vec","title":"Word2Vec","text":"<ul> <li>Ways to train a Word2Vec Dictionary:  CBOW &amp; Skipgram</li> <li>CBOW predicts focus word given context words using an autoencoder model </li> <li>Skipgram predicts context word  given focus word using an autoencoder model with multiple softmax (=number of context words) (Computationally more expensive to train, but can work with smaller data and infrequent words)</li> <li>Algorithmic optimisations for Skipgram and CBOW<ul> <li>Hieraarchial Softmax - No of softmax needed = \\(log_2(Vocab\\  Size)\\)</li> <li>Negatice Sampling - Update only a sample (based on frequency) of words including the target word</li> </ul> </li> <li>In Word2Vec or Avg Word2Vec, Semantic information is learned by the vector </li> <li>References : Word Embedding  | Tensorflow Reference  |  TFIDF Weighted Word2Vec</li> </ul>","tags":["Machine Learning","Data Science","Feature Engineering"]},{"location":"Knowledge/Machine%20Learning/2022-05-01%20Feature%20Engineering/#positional-embedding","title":"Positional Embedding","text":"<ul> <li>Semantic and  information about the Position in a sentence is captured</li> <li>Keras Position Embedding</li> <li>Transformers  : Absolute Or Relative Positional Embedding</li> </ul>","tags":["Machine Learning","Data Science","Feature Engineering"]},{"location":"Knowledge/Machine%20Learning/2022-05-01%20Feature%20Engineering/#bert-distillbert","title":"BERT &amp; DistillBERT","text":"<ul> <li>Can be used as replacement for word2vec</li> </ul> <p>Note</p> <p>Transformers are more efficient in parallel processing than LSTMs Reference 1, Reference 2 </p>","tags":["Machine Learning","Data Science","Feature Engineering"]},{"location":"Knowledge/Machine%20Learning/2022-05-01%20Feature%20Engineering/#categorical-data","title":"Categorical Data","text":"<ul> <li>One hot encoding</li> <li>Mean value replacement or response coding </li> </ul>","tags":["Machine Learning","Data Science","Feature Engineering"]},{"location":"Knowledge/Machine%20Learning/2022-05-01%20Feature%20Engineering/#time-series-wave-form-data","title":"Time Series &amp; Wave Form Data","text":"","tags":["Machine Learning","Data Science","Feature Engineering"]},{"location":"Knowledge/Machine%20Learning/2022-05-01%20Feature%20Engineering/#moving-window","title":"Moving Window","text":"<ul> <li>Within a Moving Window of width \\(w\\) calculate mean, std dev, median, quantiles, max, min, max - min,  local minimas,  local maximas, zero crossing</li> </ul>","tags":["Machine Learning","Data Science","Feature Engineering"]},{"location":"Knowledge/Machine%20Learning/2022-05-01%20Feature%20Engineering/#fourier-transform","title":"Fourier Transform","text":"<p>Any repeating pattern with Amplitude \\(A\\), Time Period \\(T\\), Frequency \\(F\\) and phase \\(\\phi\\) can be broken down as sum of sine and cosines waves.  Fourier Transform is used to convert a wave from <code>Time domain</code> to <code>Frequency domain</code> which is very insightful in repeating patterns</p> <p> </p>","tags":["Machine Learning","Data Science","Feature Engineering"]},{"location":"Knowledge/Machine%20Learning/2022-05-01%20Feature%20Engineering/#discrete-fourier-transform-dft","title":"Discrete Fourier Transform (DFT)","text":"<ol> <li>The\u00a0input\u00a0is a sequence of numbers of the original variable (one value per time step)</li> <li>The\u00a0output\u00a0is\u00a0one value of amplitude (or signal strength) for each frequency. These are represented by the\u00a0Fourier Coefficients.  </li> <li>This new series is computed using the Fourier formula:</li> </ol> \\[X_k = \\sum_{n=0}^{N-1} x_ne^{-2 \\pi ikn/N}\\] <p>Now we obtain the frequencies that are present in the variable.  Each of the values in the outcome series is the strength of a specific frequency.  If the amplitude of a frequency is high, then that seasonality is important in the orginal time series (or waves). There exists an Computational optimized form of DFT called Fast Fourier Transform. It is computed using the\u00a0Cooley-Tukey FFT algorithm.</p>","tags":["Machine Learning","Data Science","Feature Engineering"]},{"location":"Knowledge/Machine%20Learning/2022-05-01%20Feature%20Engineering/#graph-data","title":"Graph Data","text":"","tags":["Machine Learning","Data Science","Feature Engineering"]},{"location":"Knowledge/Machine%20Learning/2022-05-01%20Feature%20Engineering/#node-level-features","title":"Node Level Features","text":"","tags":["Machine Learning","Data Science","Feature Engineering"]},{"location":"Knowledge/Machine%20Learning/2022-05-01%20Feature%20Engineering/#node-indegree-and-node-outdegree","title":"Node InDegree and Node OutDegree","text":"","tags":["Machine Learning","Data Science","Feature Engineering"]},{"location":"Knowledge/Machine%20Learning/2022-05-01%20Feature%20Engineering/#eigenvector-centrality","title":"Eigenvector Centrality","text":"","tags":["Machine Learning","Data Science","Feature Engineering"]},{"location":"Knowledge/Machine%20Learning/2022-05-01%20Feature%20Engineering/#clustering-coefficient","title":"Clustering Coefficient","text":"","tags":["Machine Learning","Data Science","Feature Engineering"]},{"location":"Knowledge/Machine%20Learning/2022-05-01%20Feature%20Engineering/#deepwalk","title":"DeepWalk","text":"","tags":["Machine Learning","Data Science","Feature Engineering"]},{"location":"Knowledge/Machine%20Learning/2022-05-01%20Feature%20Engineering/#graph-coloring","title":"Graph coloring","text":"","tags":["Machine Learning","Data Science","Feature Engineering"]},{"location":"Knowledge/Machine%20Learning/2022-05-01%20Feature%20Engineering/#hope","title":"HOPE","text":"","tags":["Machine Learning","Data Science","Feature Engineering"]},{"location":"Knowledge/Machine%20Learning/2022-05-01%20Feature%20Engineering/#page-rank","title":"Page Rank","text":"<p>Adar Index Kartz Centrality Shortest Path Connected-component HITS Score</p>","tags":["Machine Learning","Data Science","Feature Engineering"]},{"location":"Knowledge/Machine%20Learning/2022-05-01%20Feature%20Engineering/#graph-level-features","title":"Graph Level Features","text":"","tags":["Machine Learning","Data Science","Feature Engineering"]},{"location":"Knowledge/Machine%20Learning/2022-05-01%20Feature%20Engineering/#adjacency-matrix","title":"Adjacency Matrix","text":"","tags":["Machine Learning","Data Science","Feature Engineering"]},{"location":"Knowledge/Machine%20Learning/2022-05-01%20Feature%20Engineering/#laplacian-matrix","title":"Laplacian Matrix","text":"","tags":["Machine Learning","Data Science","Feature Engineering"]},{"location":"Knowledge/Machine%20Learning/2022-05-01%20Feature%20Engineering/#bag-of-nodes","title":"Bag of Nodes","text":"","tags":["Machine Learning","Data Science","Feature Engineering"]},{"location":"Knowledge/Machine%20Learning/2022-05-01%20Feature%20Engineering/#weisfeiler-lehman-kernel","title":"Weisfeiler-Lehman Kernel","text":"","tags":["Machine Learning","Data Science","Feature Engineering"]},{"location":"Knowledge/Machine%20Learning/2022-05-01%20Feature%20Engineering/#graphlet-kernels","title":"Graphlet Kernels","text":"","tags":["Machine Learning","Data Science","Feature Engineering"]},{"location":"Knowledge/Machine%20Learning/2022-05-01%20Feature%20Engineering/#path-based-kernels","title":"Path-based Kernels","text":"","tags":["Machine Learning","Data Science","Feature Engineering"]},{"location":"Knowledge/Machine%20Learning/2022-05-01%20Feature%20Engineering/#graphhopper-kernel-neural-message-passing-graph-convolutional-networks","title":"GraphHopper kernel,\u00a0Neural Message Passing, \u00a0Graph Convolutional Networks","text":"","tags":["Machine Learning","Data Science","Feature Engineering"]},{"location":"Knowledge/Machine%20Learning/2022-05-01%20Feature%20Engineering/#neighbourhood-overlap-features","title":"Neighbourhood Overlap Features","text":"","tags":["Machine Learning","Data Science","Feature Engineering"]},{"location":"Knowledge/Machine%20Learning/2022-05-01%20Feature%20Engineering/#local-overlap-measures","title":"Local Overlap Measures","text":"","tags":["Machine Learning","Data Science","Feature Engineering"]},{"location":"Knowledge/Machine%20Learning/2022-05-01%20Feature%20Engineering/#global-overlap-measures","title":"Global Overlap Measures","text":"","tags":["Machine Learning","Data Science","Feature Engineering"]},{"location":"Knowledge/Machine%20Learning/2022-05-01%20Feature%20Engineering/#_1","title":"Feature Engineering","text":"","tags":["Machine Learning","Data Science","Feature Engineering"]},{"location":"Knowledge/Machine%20Learning/2022-05-01%20Feature%20Engineering/#_2","title":"Feature Engineering","text":"","tags":["Machine Learning","Data Science","Feature Engineering"]},{"location":"Knowledge/Machine%20Learning/2022-05-01%20Feature%20Engineering/#_3","title":"Feature Engineering","text":"","tags":["Machine Learning","Data Science","Feature Engineering"]},{"location":"Knowledge/Machine%20Learning/2022-05-01%20Feature%20Engineering/#_4","title":"Feature Engineering","text":"","tags":["Machine Learning","Data Science","Feature Engineering"]},{"location":"Knowledge/Machine%20Learning/2022-05-01%20Feature%20Engineering/#sequence-data","title":"Sequence Data","text":"","tags":["Machine Learning","Data Science","Feature Engineering"]},{"location":"Knowledge/Machine%20Learning/2022-05-01%20Feature%20Engineering/#image-data","title":"Image Data","text":"<ul> <li>Colour Image Histogram - Tutorial</li> <li>Edge Histogams</li> <li>Haar Features</li> <li>SIFT Features : Very useful in Image Search with properties like scale invariance and rotation invariance</li> </ul>","tags":["Machine Learning","Data Science","Feature Engineering"]},{"location":"Knowledge/Machine%20Learning/2022-05-01%20Feature%20Engineering/#graph-data_1","title":"Graph Data","text":"","tags":["Machine Learning","Data Science","Feature Engineering"]},{"location":"Knowledge/Machine%20Learning/2022-05-01%20Feature%20Engineering/#feature-engineering","title":"Feature Engineering","text":"<ul> <li>Feature Binning and Indicator variables <ul> <li>Depending on domain knowledge sometime it might be useful to convert real valued / categorical feature to bucketed/bined feature based on some rule</li> <li>To find the Binning thresholds / rule for a real valued variable \\(X\\) train a decision tree using \\(X\\) and target \\(Y\\) and get the threshold from the decision trees (trained on entropy)</li> </ul> </li> <li>Interaction Variables : <ul> <li>Logical \\(N\\) way Interaction between \\(N\\) variables created using Decision Trees of Depth \\(N\\)</li> <li>Numerical \\(N\\) way Interaction between \\(N\\) variables created using Mathamteical Operations between these features</li> </ul> </li> <li>Feature Orthogonality : More orthogonal/different/uncorrelated features are better for learning a model</li> <li>Slicing Features : Features that help us divide the data into separate data generating process Eg: Device Feature : {Desktop, Mobile, Tablet} The way a customer would a product on mobile would be different and on computer would be different</li> </ul>","tags":["Machine Learning","Data Science","Feature Engineering"]},{"location":"Knowledge/Machine%20Learning/2022-05-01%20Feature%20Engineering/#references","title":"References","text":"<ol> <li>Regular Expression Blog </li> <li>Gensim</li> <li>Sliding Window Discrete Fourier Transform</li> <li>https://kenndanielso.github.io/mlrefined/</li> </ol>","tags":["Machine Learning","Data Science","Feature Engineering"]},{"location":"Knowledge/Machine%20Learning/2022-12-31%20Support%20Vector%20Machines/","title":"Support Vector Machines","text":"","tags":["Machine Learning","Support Vector Machines"]},{"location":"Knowledge/Machine%20Learning/2022-12-31%20Support%20Vector%20Machines/#geometric-intution","title":"Geometric Intution","text":"<p>SVMs try to find a separating hyper plane \\(\\pi\\) which maximises the margin, where margin is the distance between the planes \\(\\pi+\\) and \\(\\pi-\\). </p> <ul> <li>\\(\\pi+\\) is plane parralel to \\(\\pi\\) (separating plane) which passes through first positive point </li> <li>\\(\\pi-\\) is plane parralel to \\(\\pi\\) (separating plane) which passes through first negative point</li> <li>Points on through which \\(\\pi+\\) and \\(\\pi-\\) pass through are called support vectors</li> </ul>","tags":["Machine Learning","Support Vector Machines"]},{"location":"Knowledge/Machine%20Learning/2022-12-31%20Support%20Vector%20Machines/#convex-hull","title":"Convex Hull","text":"<p>Convex Hull is the smallest convex ploygon such that all the points are inside or on the polygon </p> <ul> <li>Construct a convex hull for positive and negative points separately</li> <li>Find the shortest line connecting the two convex hulls</li> <li>The perpendicular bisector of the shortest line is the margin maximising hyperplane</li> </ul>","tags":["Machine Learning","Support Vector Machines"]},{"location":"Knowledge/Machine%20Learning/2022-12-31%20Support%20Vector%20Machines/#decision-plane-support-vectors","title":"Decision Plane &amp; Support Vectors","text":"\\[\\pi : w^Tx + b = 0\\] <p>if support vectors are :</p> \\[\\pi+ : w^Tx + b = 1\\] \\[\\pi- : w^Tx + b = -1\\]","tags":["Machine Learning","Support Vector Machines"]},{"location":"Knowledge/Machine%20Learning/2022-12-31%20Support%20Vector%20Machines/#constraint-optimisation","title":"Constraint Optimisation","text":"","tags":["Machine Learning","Support Vector Machines"]},{"location":"Knowledge/Machine%20Learning/2022-12-31%20Support%20Vector%20Machines/#hard-margin-svm","title":"Hard Margin SVM:","text":"\\[(w^*, b^*) = argmax(w,b) \\frac{2}{||w||}\\] <p>such that   </p> \\[y_i(w^Tx_i+b) \\ge 1 \\ \\  \\forall  \\ \\   x_i\\] <ul> <li>All points should be correctly classified in the hard margin optimisation </li> <li>\\(\\frac{2}{||w||}\\) is the margin distance between \\(\\pi+\\) and \\(\\pi-\\)</li> </ul>","tags":["Machine Learning","Support Vector Machines"]},{"location":"Knowledge/Machine%20Learning/2022-12-31%20Support%20Vector%20Machines/#primal-form-soft-margin-svm","title":"Primal form - Soft Margin SVM","text":"\\[(w^*, b^*) = argmin(w,b) \\frac{||w||}{2} + C\\frac{1}{n} \\sum_i^n \\zeta_i\\] <p>such that </p> \\[y_i(w^Tx_i+b) \\ge 1 - \\zeta_i \\ \\ \\forall i\\] <p>and  </p> \\[\\zeta_i \\ge 0\\] <ul> <li>\\(0.5*||w||\\) is the inverse of margin</li> <li>\\(\\frac{1}{n} \\sum_i^n \\zeta_i\\)  is the average distance of missclassified points</li> <li>If the point is correctly classified : \\(\\zeta_i = 0\\) </li> <li>The point is further away from the correct hyperplane  as \\(\\zeta_i\\) increases for a point  </li> <li>\\(C\\) is multipled to the loss finction (Hinge Loss see ahead) hence as \\(C\\) increases there is a tendency to overfit and as \\(C\\) decreases there is a tendency to underfit.</li> <li>This is also called \\(C-soft-SVM\\)</li> </ul>","tags":["Machine Learning","Support Vector Machines"]},{"location":"Knowledge/Machine%20Learning/2022-12-31%20Support%20Vector%20Machines/#dual-form-soft-margin-svm","title":"Dual form - Soft Margin SVM","text":"\\[max_{\\alpha} \\  (\\sum_i^n \\alpha_i - 0.5\\sum_i^n\\sum_j^n\\ \\alpha_i\\alpha_j\\ y_iy_j\\ x_i^Tx_j)\\] <p>such that </p> \\[\\alpha_i\\ge0\\] \\[\\sum_i^n\\alpha_iy_i = 0\\] <p>To get the class label in this dual form we have this function</p> \\[f(x_q) = \\sum_i^n \\alpha_i y_i\\ x_i^Tx_q + b\\] <p>Note</p> <ul> <li>\\(x_i^Tx_q\\) is the cosine similarity if \\(x\\) is normalised.</li> <li>Theoritically we can replace  \\(x_i^Tx_q\\) with any kind of similarity function : kernel function</li> </ul>","tags":["Machine Learning","Support Vector Machines"]},{"location":"Knowledge/Machine%20Learning/2022-12-31%20Support%20Vector%20Machines/#kernel-trick","title":"Kernel Trick","text":"<p>Using Kernel tricks SVM can handle non linearly separaed datasets by the way of implicit feature transformation</p> \\[max_{\\alpha} \\  (\\sum_i^n \\alpha_i - 0.5\\sum_i^n\\sum_j^n\\ \\alpha_i\\alpha_j\\ y_iy_j\\ K(x_i, x_j)) \\] <p>such that </p> \\[\\alpha_i\\ge0\\] \\[\\sum_i^n\\alpha_iy_i = 0\\] <p>To get the class label in this dual form we have this function</p> \\[f(x_q) = \\sum_i^n \\alpha_i y_i\\ K(x_i, x_q) + b\\]","tags":["Machine Learning","Support Vector Machines"]},{"location":"Knowledge/Machine%20Learning/2022-12-31%20Support%20Vector%20Machines/#polynomial-kernel","title":"Polynomial kernel","text":"\\[K(x_i, x_j) = (x_i^Tx_j + c)^d = x_i'^Tx_j' \\] <ul> <li>\\(x_i'\\) is of a higher dimension than \\(x_i\\) and hence is a feature transformation from smaller dimension to larger dimension</li> </ul>","tags":["Machine Learning","Support Vector Machines"]},{"location":"Knowledge/Machine%20Learning/2022-12-31%20Support%20Vector%20Machines/#radial-basis-function-kernel","title":"Radial Basis Function Kernel :","text":"\\[K_{RBF}(x_i,x_j)= exp(\\frac{-||x_i-x_j||^2}{2\\sigma^2})\\] <p>Similarity between KNN and RBF Kernel  - For same value of \\(\\sigma\\)  as \\(distance \\ \\  d_{12} = ||x_i-x_j||^2\\) increases \\(K_{RBF}\\) (similarity) decreases - For same value of \\(d_{12}\\) as \\(\\sigma\\) increases \\(K_{RBF}\\)  (similarity) increases - Increase in \\(\\sigma\\)  is same as increase in \\(K\\) for K-NN - RBF SVM is nice approximation of KNN with lower computational complexity</p>","tags":["Machine Learning","Support Vector Machines"]},{"location":"Knowledge/Machine%20Learning/2022-12-31%20Support%20Vector%20Machines/#domain-specific-kernels","title":"Domain Specific Kernels","text":"<ul> <li>String Kernels : Text Classification </li> <li>Genome Lernels</li> <li>Graph Based Kernels</li> </ul>","tags":["Machine Learning","Support Vector Machines"]},{"location":"Knowledge/Machine%20Learning/2022-12-31%20Support%20Vector%20Machines/#training-and-runtime","title":"Training and Runtime","text":"","tags":["Machine Learning","Support Vector Machines"]},{"location":"Knowledge/Machine%20Learning/2022-12-31%20Support%20Vector%20Machines/#optimization-algorithms","title":"Optimization Algorithms","text":"<ul> <li>Stochaistic Gradient Decent </li> <li>Sequential Minimal Optimization - libsvm </li> </ul>","tags":["Machine Learning","Support Vector Machines"]},{"location":"Knowledge/Machine%20Learning/2022-12-31%20Support%20Vector%20Machines/#time-complexity","title":"Time Complexity","text":"<ul> <li>Training Time complexity ~ \\(O(n^2)\\) for Kernel SVMs (\\(n\\) is no of traning points) Very High </li> <li>Run Time complexity ~ \\(O(kd)\\) for Kernel SVMs (\\(k\\) is no of support vector points)</li> </ul>","tags":["Machine Learning","Support Vector Machines"]},{"location":"Knowledge/Machine%20Learning/2022-12-31%20Support%20Vector%20Machines/#nu-svm","title":"Nu-SVM","text":"<p>Instead of hyperparameter \\(C\\) another  hyperparameter \\(nu\\) is used which has the property:</p> <ul> <li>\\(nu \\ge fraction \\ of\\  errors\\)</li> <li>\\(nu \\le fraction \\ of\\  support \\ vectors\\)</li> </ul> <p>So \\(nu\\) can control the error  but does not control number of Support Vectors</p>","tags":["Machine Learning","Support Vector Machines"]},{"location":"Knowledge/Machine%20Learning/2022-12-31%20Support%20Vector%20Machines/#outlier-impact","title":"Outlier impact","text":"<ul> <li>Very little impact of outlier as only support vectors are used in calculation </li> <li>For RBF with small \\(\\sigma\\)  can be affected by outliers, similar to KNN with small K is affected by outliers</li> </ul>","tags":["Machine Learning","Support Vector Machines"]},{"location":"Knowledge/Machine%20Learning/2022-12-31%20Support%20Vector%20Machines/#cases","title":"Cases","text":"<ul> <li>Good Case <ul> <li>We know the right Kernel </li> <li>High Dimention Data </li> </ul> </li> <li>Bad Case <ul> <li>\\(n\\) is large imples high Training time </li> <li>\\(k\\) (No of Support vector) is large </li> </ul> </li> </ul>","tags":["Machine Learning","Support Vector Machines"]},{"location":"Knowledge/Machine%20Learning/2022-12-31%20Support%20Vector%20Machines/#good-questions-and-refernces","title":"Good Questions and Refernces","text":"<ol> <li>A tutorial on support vector regression</li> <li>svm-skilltest</li> <li>Give some situations where we use an SVM over a RandomForest and vice-versa</li> <li>What is convex hull ?</li> <li>What is a large margin classifier?</li> <li>Why SVM is an example of a large margin classifier?</li> <li>SVM being a large margin classifier, is it influenced by outliers? (Yes, if C is large, otherwise not)</li> <li>What is the role of C in SVM?</li> <li>In SVM, what is the angle between the decision boundary and theta?</li> <li>What is the mathematical intuition of a large margin classifier?</li> <li>What is a kernel in SVM? Why do we use kernels in SVM?</li> <li>What is a similarity function in SVM? Why it is named so?</li> <li>How are the landmarks initially chosen in an SVM? How many and where?</li> <li>Can we apply the kernel trick to logistic regression? Why is it not used in practice then?</li> <li>What is the difference between logistic regression and SVM without a kernel? (Only in implementation \u2013 one is much more efficient and has good optimization packages)</li> <li>How does the SVM parameter C affect the bias/variance trade off? (Remember C = 1/lambda; lambda increases means variance decreases)</li> <li>How does the SVM kernel parameter sigma^2 affect the bias/variance trade off?</li> <li>Can any similarity function be used for SVM? (No, have to satisfy Mercer\u2019s theorem)</li> <li>Logistic regression vs. SVMs: When to use which one? ( Let's say n and m are the number of features and training samples respectively. If n is large relative to m use log. Reg. or SVM with linear kernel, If n is small and m is intermediate, SVM with Gaussian kernel, If n is small and m is massive, Create or add more features then use log. Reg. or SVM without a kernel)</li> </ol>","tags":["Machine Learning","Support Vector Machines"]},{"location":"Knowledge/Machine%20Learning/2023-01-01%20Decision%20Trees/","title":"Decision Trees","text":"","tags":["Machine Learning","Decision Trees"]},{"location":"Knowledge/Machine%20Learning/2023-01-01%20Decision%20Trees/#interpretation-of-decision-trees","title":"Interpretation of Decision Trees","text":"<ul> <li>Programatic Interpretation : Decision Trees are nested if else based models. </li> <li>Geometric Interpretation : Decision Surfaces for decision trees are Set of axis parallel hyperplanes which divide the space into cuboids or hyper cuboids </li> </ul>","tags":["Machine Learning","Decision Trees"]},{"location":"Knowledge/Machine%20Learning/2023-01-01%20Decision%20Trees/#information-theory-and-entropy","title":"Information Theory and Entropy","text":"<p>Let \\(Y\\)  be a random variable taking valyes \\(y_1, y_2,y_3,y_4, ...\\) , Then \\(H(Y)\\),  Entropy of \\(Y\\) is given as </p> \\[H(Y) = - \\sum_i^k p(Y=y_i) \\ log_b(p(Y=y_i))\\] <ul> <li>Entropy is maximum when all posible values of Random Variable \\(Y\\) are equi probable</li> <li>More peaked distributions have less entropy  </li> </ul>","tags":["Machine Learning","Decision Trees"]},{"location":"Knowledge/Machine%20Learning/2023-01-01%20Decision%20Trees/#gini-impurity-vs-information-gain-vs-mse-reduction","title":"Gini Impurity vs Information Gain vs MSE Reduction","text":"<ul> <li>Information Gain \\(IG\\) = Entropy of parent \u2013 Weighted Average (weights based on number of data points in each child) Entropy of child nodes</li> </ul> \\[IG (Y, var) = H_D(Y)  - \\sum_i^K \\frac{|D_i|}{|D|} H_{D_i}(Y)\\] <ul> <li>Gini Impurity  \\(I_G\\) is used because in practice rather than Information Gain, because it does not contain a Log term which is computationally expensive </li> </ul> \\[ I_G(Y) = 1 - \\sum_i^k(p(Y=y_i))^2\\] <ul> <li>For Regression problems MSE Reduction is used, where predicted value for a node is the mean of  values in the y true values in the node</li> </ul> \\[MSE \\ Reduction = MSE\\ Parent - \\sum_{childs} MSE\\  Child\\]","tags":["Machine Learning","Decision Trees"]},{"location":"Knowledge/Machine%20Learning/2023-01-01%20Decision%20Trees/#important-points","title":"Important Points","text":"","tags":["Machine Learning","Decision Trees"]},{"location":"Knowledge/Machine%20Learning/2023-01-01%20Decision%20Trees/#outliers-and-depth","title":"Outliers and Depth","text":"<ul> <li>As depth of the tree increases the tree overfits, As depth of the tree decreases the tree underfits   </li> <li>Outliers with high depth will make the tree unstabke and effect of overfitting is high </li> </ul>","tags":["Machine Learning","Decision Trees"]},{"location":"Knowledge/Machine%20Learning/2023-01-01%20Decision%20Trees/#feature-standardisation","title":"Feature standardisation","text":"<p>Feature standardisation is not required in Decision Trees because we are not using distances anywhere</p>","tags":["Machine Learning","Decision Trees"]},{"location":"Knowledge/Machine%20Learning/2023-01-01%20Decision%20Trees/#high-dimentional-categorical-feature","title":"High Dimentional Categorical Feature","text":"<p>A problem with high dimentional categorical feature is that there would be very less examples in the child nodes and the tree would be very wide (if one hot encoded).  Lets say there are k target classes, then the solution is to replace each category \\(j\\) in the high dimentional categorical feature with a vector \\(v\\) of size k, where \\(v = \\{P(y=i|category=j)\\}_k\\) . This is called Mean value replacement or response coding  To Smooth the probability values we apply laplace smoothing. Note always use only the training data to get the response coded vectors. If the train data does not contains a category which is seen in cross validation data or test data Lalace smoothing will make it a constant value ( 1/k ) vector </p>","tags":["Machine Learning","Decision Trees"]},{"location":"Knowledge/Machine%20Learning/2023-01-01%20Decision%20Trees/#time-complexity","title":"Time Complexity","text":"<ul> <li>Trainining Complexity : \\(O(nlog(n)d)\\) , n = no of training examples , d = dimension </li> <li>Runtime Complexity : \\(O(depth)\\) - Low Latency even for big data sets</li> </ul>","tags":["Machine Learning","Decision Trees"]},{"location":"Knowledge/Machine%20Learning/2023-01-01%20Decision%20Trees/#special-cases","title":"Special Cases","text":"<ul> <li>Imbalanced Data : Balance the data </li> <li>Large Dimensionality : High train time complexity, so  do not use one hot encoding rather convert to probabilities for each class</li> <li>Given similarity matrx only, decision tress cannot work</li> <li>Logical feature interactions are already inbuilt in Decision Trees</li> <li>Feature Importance is calculated by calculating the average reduction in the entropy due to a feature </li> </ul>","tags":["Machine Learning","Decision Trees"]},{"location":"Knowledge/Machine%20Learning/2023-01-01%20Decision%20Trees/#reference-and-questions","title":"Reference and Questions","text":"<ol> <li>Decision Trees Lecture Notes</li> <li>How to Building a decision Tree?</li> <li>Importance of Splitting numerical features.?</li> <li>How to handle Overfitting and Underfitting in DT?</li> <li>How to implement Regression using Decision Trees?</li> <li>https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/</li> <li>https://vitalflux.com/category/career/interview-questions/</li> </ol>","tags":["Machine Learning","Decision Trees"]},{"location":"Knowledge/Machine%20Learning/2023-01-02%20Ensembles/","title":"Ensembles","text":"<p>Tip</p> <ul> <li>The more different the base models are the better we can combine them into an ensembels</li> <li>\\(Model \\ \\  Error =  Bias^2 + Variance\\)</li> </ul>","tags":["Machine Learning","Ensembles"]},{"location":"Knowledge/Machine%20Learning/2023-01-02%20Ensembles/#bootstrap-aggregation-bagging","title":"Bootstrap Aggregation - Bagging","text":"<ul> <li>Bootstrap sample : Given a dataset  \\(D_n\\) of n points, we sample with replacement  k datasets \\(\\{D_m^i\\}_k\\) of m points each</li> <li>Aggregation : Then build k models \\(\\{M_i\\}_k\\) using these \\(\\{D_m^i\\}\\) (different subset of data) and get the Majority Vote / Mean is used for final aggregated prediction</li> <li>Bagging is commonly used to reduce variance of the model caused due to a noisy dataset without impacting the bias.  This happens because we are creating many samples of the data and the noise will never affect each data smaple </li> <li>By creating many low-bias, high-variance models (overfit models for ex: High Depth Decision Trees) and then Aggregating them we can create a low-bias, reduced-variance model </li> </ul>","tags":["Machine Learning","Ensembles"]},{"location":"Knowledge/Machine%20Learning/2023-01-02%20Ensembles/#random-forrest","title":"Random Forrest","text":"<ul> <li>Random Forrest  =  Bootstrap Sampling (Row Sampling with Replacement) + Feature Sampling (Column Sampling) + Decision Tress + Aggregation </li> <li>Out of bag error is cross validation error on Out of bag points (\\(D_n - D_m^i\\))</li> <li>Hyperparametrs : Row sampling rate, column sampling rate and no of base learners, Fix the the first two as resonable fraction and use CV to get the third </li> <li>Feature importance is calculated as overall reduction in entropy because of feature at various levels of the base models </li> <li>Prune the dataset: Using an extremely large dataset may prove to create results that is less indicative of the data provided than a smaller set that more accurately represents what is being focused on. Continue pruning the data at each node split rather than just in the original bagging process.</li> <li>Decide on accuracy or speed: Depending on the desired results, increasing or decreasing the number of trees within the forest can help. Increasing the number of trees generally provides more accurate results while decreasing the number of trees will provide quicker results.</li> </ul>","tags":["Machine Learning","Ensembles"]},{"location":"Knowledge/Machine%20Learning/2023-01-02%20Ensembles/#extremely-randamised-trees","title":"Extremely Randamised Trees","text":"<ul> <li>For any numerical feature it uses a random sample of values in that feature to calculate the threshold</li> <li>Extremely Randamised Trees have increased randomisation and hence more reduced variance</li> </ul>","tags":["Machine Learning","Ensembles"]},{"location":"Knowledge/Machine%20Learning/2023-01-02%20Ensembles/#time-complexity","title":"Time Complexity","text":"<ul> <li>Training complexity : \\(O(nlog(n)dk)\\)  Trivially parrallelizable as we can train multiple independed models</li> <li>Runtime complexity : \\(O(depth*k)\\) </li> </ul>","tags":["Machine Learning","Ensembles"]},{"location":"Knowledge/Machine%20Learning/2023-01-02%20Ensembles/#boosting","title":"Boosting","text":"<p>Algorithm</p> <ul> <li>Base learners : High Bias, Low variance ==&gt; Large Training Error </li> <li>Step 0 : Train a model \\(M_0\\) on \\(\\{x_i, y_i\\}_n\\)  to learn a function \\(F_0(x) = \\alpha_0h_0(x)\\) <ul> <li>Calculate Residual Error  \\({error_i}_0 = y_i - F_0(x_i)\\)</li> </ul> </li> <li>Step 1 : Train a model \\(M_1\\) on \\(\\{x_i, {error_i}_0\\}_n\\) to learn a function \\(F_1(x) = \\alpha_0h_0(x) + \\alpha_1h_1(x)\\) <ul> <li>Calculate Residual Error \\({error_i}_1 = y_i - F_1(x_i)\\)</li> </ul> </li> <li>Step 2 : Train a model \\(M_2\\) on \\(\\{x_i, {error_i}_1\\}_n\\) to learn a function \\(F_2(x) = \\alpha_0h_0(x) + \\alpha_1h_1(x) + \\alpha_2h_2(x)\\)</li> <li>At the end of Step k we get a model \\(F_k(x) = \\sum_{j=0}^k \\alpha_jh_j(x)\\)  with low training residual error i.e. low bias </li> </ul>","tags":["Machine Learning","Ensembles"]},{"location":"Knowledge/Machine%20Learning/2023-01-02%20Ensembles/#gradient-boosting-decision-tree-gbdt","title":"Gradient Boosting Decision Tree  (GBDT)","text":"<p>GBDT with MSE Loss</p> <ul> <li>Model @ end of Stage K : \\(F_k(x) = \\sum_{j=0}^k \\alpha_jh_j(x)\\)</li> <li>Residuals @ end of Stage : K \\(err_i = y_i - F_k(x_i)\\)</li> <li>Loss Functions : \\(L(y_i, F_k(x_i)) = (y_i - F_k(x_i))^2\\)</li> <li>Negative Gradients : \\(-\\frac{\\partial{L}}{\\partial{F_k(x_i)}} = 2(y_i - F_k(x_i))\\)</li> <li>Optimisation Problem : \\(\\alpha_k= argmin_\\gamma \\sum_{i=1}^n L(y_i, F_{k-1}(x_i) + \\gamma h_k(x_i))\\)</li> <li>\\(y_i, \\ \\ F_{k-1}(x_i) , \\ \\ h_k(x_i)\\)  are already known so above optimisation problem is very easy</li> <li>Pseudo Residual : Negative Gradients is proportional to Residuals and is used in place of the actual residual</li> </ul> <ul> <li>GBDT is very easily overfit, As the number of base models (M) increase the bias decreases but there is a change that variance might increase. </li> <li>Shrinkage/ Learning rate : \\(\\nu\\) weight for each \\(\\alpha_jh_j(x)\\) term in the final model (aside from M) is used to control the Overfitting </li> <li>Scikit learn Implemntation is slow and not optimised : GradientBoostingClassifier = GBDT + Row Sampling </li> <li>XGBoost :  GBDT + Row Sampling + Column Sampling By Tree (during creating the tree) + Column Sampling By Level (during computation of node split) + reg_alpha (L1 reguraliser) + reg_lambda (L2 reguraliser)</li> <li>XGBoost Implemntation is better optimised and is faster than scikit learn and can also be used under the scikit learn environment</li> </ul>","tags":["Machine Learning","Ensembles"]},{"location":"Knowledge/Machine%20Learning/2023-01-02%20Ensembles/#adaptive-boosting-adaboost","title":"Adaptive Boosting (AdaBoost)","text":"<ul> <li>Weight on incorrectly classified points are increased exponentially - Reference</li> <li>Used most successfully in computer vision application </li> </ul>","tags":["Machine Learning","Ensembles"]},{"location":"Knowledge/Machine%20Learning/2023-01-02%20Ensembles/#time-complexity_1","title":"Time Complexity","text":"<ul> <li>Training  : \\(O(nlog(n)d*M)\\), Since it is a step by step series model it is not easy to parallelise </li> <li>Runtime :  \\(O(depth*M)\\) , Can be used in lowlatency application</li> <li>GBDT generally takes more time to train than Random Forest but in runtime both are almost same in latency</li> </ul>","tags":["Machine Learning","Ensembles"]},{"location":"Knowledge/Machine%20Learning/2023-01-02%20Ensembles/#stacking","title":"Stacking","text":"<p>Stacking combines multiple classification models via a meta-classifier. The individual classification models are trained based on the complete training set; then, the meta-classifier is fitted based on the outputs <code>meta-features</code>  of the individual classification models in the ensemble. The meta-classifier can either be trained on the predicted class labels or probabilities from the ensemble</p> <p></p> <ul> <li>The more different the base models are the better we can combine them with stacking</li> <li>mlxtend is used to implement stacking classifier </li> </ul>","tags":["Machine Learning","Ensembles"]},{"location":"Knowledge/Machine%20Learning/2023-01-02%20Ensembles/#cascading","title":"Cascading","text":"<ul> <li>Cascading is a Sequence of models within if else conditions to decide on final prediction, Typically used when the cost of making a mistake is high </li> <li>Example: Fraud Detection / Cancer Detection (False : 0, True: 1)  </li> </ul> <pre><code>graph LR\n  A[Total Data] --&gt; B[M1];\n  B[M1] --&gt; C{Prob. of 0 &gt; 0.99};\n  C --&gt; |Yes|D[Yq=0];\n  C --&gt; |No|E[M2];\n  L[Data Filtered by M1] --&gt; E[M2] \n  E[M2] --&gt; F{Prob. of 0 &gt; 0.99};\n  F --&gt; |Yes|G[Yq=0];\n  F --&gt; |No|H[M3];\n  M[Data Filtered by M2] --&gt; H[M3] \n  H[M3 : Human] --&gt; I{Knows};\n  I --&gt; |Yes|J[Yq=0];\n  I --&gt; |No|K[Yq=1];</code></pre>","tags":["Machine Learning","Ensembles"]},{"location":"Knowledge/Machine%20Learning/2023-01-02%20Ensembles/#references","title":"References","text":"<ol> <li>Pseudo-residuals for log-loss and classification </li> <li>Mathematics for using pseudo-residuals</li> </ol>","tags":["Machine Learning","Ensembles"]},{"location":"Knowledge/Machine%20Learning/2023-01-03%20Clustering/","title":"Clustering","text":"","tags":["Machine Learning","Clustering"]},{"location":"Knowledge/Machine%20Learning/2023-01-03%20Clustering/#business-applications","title":"Business  Applications","text":"<ul> <li>E-commerce : Group similar customers based on there purchasing behaviour, money spending patterns, products used, geogrophical location etc.</li> <li>Image Segmentation : Grouping/ Clustering similar pixels</li> <li>Manual labeling  by clustering large data to smaller number of clusters and labeling just the clusters</li> </ul>","tags":["Machine Learning","Clustering"]},{"location":"Knowledge/Machine%20Learning/2023-01-03%20Clustering/#dunn-index","title":"Dunn Index","text":"<ul> <li>Small Intra Clustering Distance and Large Inter Clustering Distance</li> <li>For \\({C_k}\\) clusters with \\(d(i,j)\\) as Inter-cluster distance between \\(C_i\\) &amp; \\(C_j\\) , \\(d'(i)\\) as Intra-cluster distance of cluser \\(C_i\\), Larger Dunn Index means better clustering</li> </ul> \\[Dunn \\ Index = \\frac{min_{1\\le i \\le j \\le k} \\ \\  d(i,j)}{max_{1 \\le i \\le k} \\ d'(i)}\\]","tags":["Machine Learning","Clustering"]},{"location":"Knowledge/Machine%20Learning/2023-01-03%20Clustering/#density-based-clustering","title":"Density Based Clustering","text":"<ul> <li>Cluster points give rise to dense regions </li> <li>Noise points give rise to sparse regions</li> </ul>","tags":["Machine Learning","Clustering"]},{"location":"Knowledge/Machine%20Learning/2023-01-03%20Clustering/#density-based-spatial-clustering-of-noise-dbscan","title":"Density based spatial clustering of noise (DBSCAN)","text":"<ul> <li>Density at point <code>p</code> is the number of points within a hypersphere of radius <code>Eps</code> around <code>p</code></li> <li>Dense region is a hypersphere of radius <code>Eps</code> that contains at least <code>Min Points</code> number of points</li> <li>Core point : If <code>p</code> has &gt;= <code>Min Points</code> in an <code>Eps</code> radius around it </li> <li>Border Point : if <code>p</code> is not a core point but <code>p</code> belongs to the neighbourhood  of <code>q</code> (core point) i.e <code>dist(p,q) &lt;= Eps</code> and <code>q</code> is core</li> <li>Noise point : if <code>p</code> is neither Core or Border is noise point</li> <li>Density Edge: if <code>p</code> &amp; <code>q</code> are core points and <code>dist(p,q) &lt;= Eps</code> then edge <code>p-q</code> is a density edge</li> <li>Density connected points: If there exists a path formed by Density Edges to reach from a point <code>p</code> to <code>q</code> then these are Density connected points</li> </ul>","tags":["Machine Learning","Clustering"]},{"location":"Knowledge/Machine%20Learning/2023-01-03%20Clustering/#dbscan-algorithm","title":"DBSCAN Algorithm","text":"<ol> <li>\\(\\forall \\ x_i \\ \\epsilon \\ D\\) label all points as core, border or noise point using range queries build on KD-Tree</li> <li>Remove all noise point from the data </li> <li>For each core point not assigned to a cluster <ol> <li>Create a new cluster with point <code>p</code></li> <li>Add all the points that are density connected to <code>p</code> into this cluster</li> </ol> </li> <li>For each border point assign it to the neareast core point's cluster</li> </ol>","tags":["Machine Learning","Clustering"]},{"location":"Knowledge/Machine%20Learning/2023-01-03%20Clustering/#hyper-parameters-tuning","title":"Hyper parameters Tuning","text":"<ol> <li><code>Min Points</code> ~~ 2*dimensionality </li> <li>Choose larger value of <code>Min Points</code> if dataset has noisy points</li> <li><code>Eps</code> \\(\\forall x_i\\) calculate \\(d_i\\)  i.e. distance of \\(x_i\\) from the <code>Min Points</code> th  nearest neighbour</li> <li>Sort \\(d_i\\)  in increasing order and pick <code>Eps</code> as the \\(d_i\\) with the elbow in the plot</li> </ol>","tags":["Machine Learning","Clustering"]},{"location":"Knowledge/Machine%20Learning/2023-01-03%20Clustering/#complexity","title":"Complexity","text":"<ul> <li>Time Complexity : \\(O(nLog(n))\\)</li> <li>Space Complexity : \\(O(n)\\)</li> </ul> <p>Note</p> <ul> <li>Advatages of DBSCAN</li> <li>Disadvantage of DBSCAN</li> </ul>","tags":["Machine Learning","Clustering"]},{"location":"Knowledge/Machine%20Learning/2023-01-03%20Clustering/#k-means","title":"K-Means","text":"<p>K-Means gives us k sets of points from which k clusters are derived</p>","tags":["Machine Learning","Clustering"]},{"location":"Knowledge/Machine%20Learning/2023-01-03%20Clustering/#optimisation-task","title":"Optimisation Task :","text":"<p>For \\(D_i = \\{x_1, x_2, ... , x_n \\}\\) find \\(\\{C_j\\}_k\\) and  there by \\(\\{S_j\\}_k\\)  Objective function : </p> \\[argmin_{\\{ C_j \\}_k}  \\sum_{i=1}^k \\sum_{x\\epsilon S_i} ||x-C_i||^2\\] <p>such that      \\(\\forall_i \\  x_i \\  \\epsilon \\  S_j\\)          and         \\(\\forall_{i,j} \\  S_i \\cap  S_j = \\phi\\) where \\(C_i\\) is the Centroid of the ith cluster of points \\(S_i\\) and is given by </p> \\[C_i = \\frac{1}{|S_i|} \\sum_{x_j \\ \\epsilon \\  S_i} x_j\\]","tags":["Machine Learning","Clustering"]},{"location":"Knowledge/Machine%20Learning/2023-01-03%20Clustering/#lloyds-algorithm","title":"Lloyd's Algorithm","text":"<p>Lloyd's algorithm is used to solve the above optimisation approximately as it is very hard to optimise accurately</p> <ol> <li>Initialization : Randomly pick k points from \\(D\\) and call them \\(\\{C_j\\}_k\\) i.e. \\(C_1, C_2, ..., C_k\\)</li> <li>Assignent : For each \\(x_i\\) in \\(D\\) select the neareast \\(C_j\\)  by calculating the \\(dist(x_i, C_j) \\ \\ \\forall \\ j \\epsilon [1,k]\\)  and add \\(x_i\\) to \\(S_j\\) </li> <li>Recompute/Update Centroid : recalculate centroids using : \\(C_i = \\frac{1}{|S_i|} \\sum_{x_j \\ \\epsilon \\  S_i} x_j\\) </li> <li>Repeat Assignent and Update until the distance/change between old and new centroid is negligible and we can say that the algorithm has converged</li> </ol>","tags":["Machine Learning","Clustering"]},{"location":"Knowledge/Machine%20Learning/2023-01-03%20Clustering/#k-mean","title":"K-mean++","text":"<ul> <li>Initialization Sensitivity : Final clusters depend on the initialisation done in the first step</li> <li>Repeat K means with different initializations and pick the best clustering based on smaller intra-cluster and larger intra cluster distance</li> <li>Smart Initialization : k-mean ++<ul> <li>Pick the first centroid \\(C_1\\) randomly from \\(D\\)</li> <li>Create a distribution  \\(\\forall \\ x_i \\ \\epsilon \\ D\\) given by \\(d_i = dist(x_i, nearest \\ \\ Centroid)^2\\)</li> <li>Pick a point from \\(D-C_1\\) with probability proportional to \\(d_i\\) and annotate it as \\(C_2\\)</li> </ul> </li> </ul>","tags":["Machine Learning","Clustering"]},{"location":"Knowledge/Machine%20Learning/2023-01-03%20Clustering/#limitations","title":"Limitations","text":"<ul> <li>Small number of outliers can affect the result of K-mean and K-means++ hugely</li> <li>k-means has problens when clusters are of different sizes because k means tries to create clusters of similar sizes</li> <li>k-means has problens when clusters are of different densities because k means tries to create clusters of similar densities</li> <li>k-means has problens with Non-globular (Non-Convex) clusters</li> <li>Good hack to solve these limitations  is to increase k and then combining similar clusters</li> </ul>","tags":["Machine Learning","Clustering"]},{"location":"Knowledge/Machine%20Learning/2023-01-03%20Clustering/#k-medoids","title":"K-Medoids","text":"<p>Instead of giving a mean value as centroid which may or maynot be in the dataset, we can give a already interpretable data point as the centroid. </p>","tags":["Machine Learning","Clustering"]},{"location":"Knowledge/Machine%20Learning/2023-01-03%20Clustering/#partitioning-around-medoids-pam","title":"Partitioning around medoids (PAM)","text":"<ol> <li>Initialization : Same as K-means++</li> <li>Assignment : Closest Medoid (Same as K-means)</li> <li>Recompute / Update: Swap each medoid with a non medoid point and if the loss decreases keep the swap else undo the swap. If Swap is success than do the assignment again<ul> <li>\\(Loss = \\sum_{j=1}^k \\sum_{x_i \\epsilon S_j} ||x_i - m_j||^2\\)</li> <li>The distance metric can be replaced with a kernel, which is again a benefit over K-means</li> </ul> </li> </ol>","tags":["Machine Learning","Clustering"]},{"location":"Knowledge/Machine%20Learning/2023-01-03%20Clustering/#determing-the-right-k","title":"Determing the right K","text":"<ul> <li>Elbow-Method /Knee-Method :  Calculate Loss for different values of k and choose the k after which decrease in loss is insignificant (basically the elbow point)</li> </ul> \\[Loss = \\sum_{i=1}^k \\sum_{x\\epsilon S_i} ||x-C_i||^2\\]","tags":["Machine Learning","Clustering"]},{"location":"Knowledge/Machine%20Learning/2023-01-03%20Clustering/#time-complexity","title":"Time Complexity","text":"<p>Training Time complexity : \\(O(nkdi)\\)</p> <ul> <li>n = number of points</li> <li>k = No of clusters</li> <li>d = dimensions of data</li> <li>i = no of iteration</li> <li>Typically k and i are small  so ~~ \\(O(nd)\\)</li> </ul>","tags":["Machine Learning","Clustering"]},{"location":"Knowledge/Machine%20Learning/2023-01-03%20Clustering/#hierarchical-clustering","title":"Hierarchical Clustering","text":"","tags":["Machine Learning","Clustering"]},{"location":"Knowledge/Machine%20Learning/2023-01-03%20Clustering/#agglomerative-divisive-clustering","title":"Agglomerative &amp; Divisive Clustering","text":"<ul> <li>Agglomerative clustering assumes all points are individual clusters and starts grouping these clusterns in each iteration based on some notion of similarity or distances</li> <li>Divisive Clustering is just the reverse process and it starts with one big cluster  and divides into smaller clusters</li> <li>Computaion Steps<ul> <li>Compute the proximity matrix </li> <li>Let each data point be a cluster </li> <li>Repeat <ul> <li>Merge the two closest clusters </li> <li>Update the proximity matrix </li> </ul> </li> <li>Until only a single cluster remains</li> </ul> </li> </ul>","tags":["Machine Learning","Clustering"]},{"location":"Knowledge/Machine%20Learning/2023-01-03%20Clustering/#inter-cluster-similarity-or-proximity","title":"Inter Cluster Similarity or proximity","text":"<ol> <li>Single Link Agglomeration (Min Method) </li> </ol> \\[Sim(C_1,C_2) = min \\ Sim(p_i,p_j) \\ \\ \\forall \\  p_i \\epsilon C_1 ;\\  p_j \\epsilon C_2\\] <ol> <li>Min breaks when data has outliers</li> <li>Complete Link Agglomeration (Max Method) </li> </ol> \\[Sim(C_1,C_2) = max \\ Sim(p_i,p_j) \\ \\ \\forall \\  p_i \\epsilon C_1 ;\\  p_j \\epsilon C_2\\] <ol> <li>Max breaks when data has large clusters or differnt size clusters or non circular clusters</li> <li>Group Avg Method (Compromise between min and max) </li> </ol> \\[Sim(C_1,C_2) = \\frac{\\sum_{p_i \\epsilon C_1, p_j \\epsilon C_2} \\ Sim(p_i,p_j)}{|C_1| * |C_2|}\\] <ol> <li>All Min, Max and Avg methods can be kernelised</li> <li>Centroid Method </li> </ol> \\[Sim(C_1,C_2) = Distance / Similarty \\ between \\ centroids\\] <ol> <li>Wards method is same as Group Avg with similarity as distance squared</li> </ol>","tags":["Machine Learning","Clustering"]},{"location":"Knowledge/Machine%20Learning/2023-01-03%20Clustering/#complexity_1","title":"Complexity","text":"<ul> <li>Space comlexity : \\(O(n^2)\\) </li> <li>Time complexity : \\(O(n3)\\)  to \\(O(n2Log(n))\\)</li> <li> <p>Both are very high and increase with n which is why it is not used very much in big data</p> </li> <li> <p>Really good clustering lecture</p> </li> <li>Visualising Clusters</li> <li>How to choose the right clustering algorithm</li> </ul>","tags":["Machine Learning","Clustering"]},{"location":"Knowledge/Machine%20Learning/2023-01-11%20Useful%20Concepts%20in%20Machine%20Learning/","title":"Useful Concepts in Machine Learning","text":"","tags":["Machine Learning"]},{"location":"Knowledge/Machine%20Learning/2023-01-11%20Useful%20Concepts%20in%20Machine%20Learning/#calibration-of-models","title":"Calibration of Models","text":"<p>When performing classification we often want not only to predict the class label, but also obtain a probability of the respective label. So after training a model, if this model is  returning class labels and not returning the actual probabilities then we train a calibration moldel to calculate the probabilities. - Using \\(D_{train}\\) learn a function \\(f(x)\\) and using \\(D_{cross \\ validation}: \\{x_i, y_i\\}\\)  create a table of values  \\(x_i, \\hat{y_i}, y_i\\), sorted in increasing order of \\(\\hat{y_i}\\)  - Break the table into \\(k\\) chunks of size \\(m\\) and calculate \\(mean \\  y_i \\ \\forall \\ j \\ \\epsilon \\  k\\)  and call it  \\({y_{mean}^j}\\) &amp; \\(mean \\  \\hat{y_i} \\ \\forall \\ j \\ \\epsilon \\  k\\)  and call it  \\({\\hat{y}_{mean}^j}\\) for each chunk - \\(D_{calibration} : \\{\\ \\hat{y}_{mean}^j, y_{mean}^j \\}\\) , Calibration Plot :  \\(y_{axis}  =  y_{mean}^j, \\  x_{axis} = \\ \\hat{y}_{mean}^j\\) - Now a Calibration function is trained to map  \\(\\hat{y}_{mean}^j\\) to \\(y_{mean}^j\\) (where  \\(y_{mean}^j\\) is the probability if positive class)        - Platt Scaling Callibration : Works only if the calibration plot looks like sigmoid - Isotonic Regression Callibration : Learns Piece wise linear models, Works in almost all cases but needs more data than plat scaling - CalibratedClassifierCV\u00a0 - Probability calibration of classifiers - Predicting Good Probabilities With Supervised Learning</p>","tags":["Machine Learning"]},{"location":"Knowledge/Machine%20Learning/2023-01-11%20Useful%20Concepts%20in%20Machine%20Learning/#random-sampling-consensus-ransac","title":"Random Sampling Consensus (RANSAC)","text":"<ul> <li>Get a Random sample from \\(D_{Train}\\)  call it \\(D_0\\) and build a Model \\(M_0\\) using \\(D_0\\)</li> <li>Compute outliers dataset \\(O_o\\) using abolute error based on the \\(M_0\\) prediction </li> <li>Now get the filtered data \\(D_{train}^1 = D_{Train} - O_o\\)</li> <li>Repeat the above steps to get \\(D_{train}^2, D_{train}^3 ...\\)</li> <li>When the \\(M_i\\) and \\(M_{i+1}\\) are very same then stop iterating and the \\(M_{i+1}\\) is a very robust model</li> </ul>","tags":["Machine Learning"]},{"location":"Knowledge/Machine%20Learning/2023-01-11%20Useful%20Concepts%20in%20Machine%20Learning/#loss-minimisation-framework","title":"Loss Minimisation Framework","text":"<p>Loss functions for classification <code>gray : Zero-one loss</code> <code>green: Savage loss</code> <code>orange: Logistic loss</code> <code>purple: Exponential loss</code> <code>brown: Tangent loss</code> <code>blue: Square loss</code></p> <ul> <li>Logistic  Regression : Logistic Loss (approximate of 0-1 loss) + Regulariser</li> <li>Linear  Regression : Linear Loss + Regulariser</li> <li>SVM  Regression : Hinge Loss + Regulariser</li> </ul> <p>Hinge Loss  :   \\(max(0, 1- y_i(w^Tx_i+b)) = \\zeta_i\\) </p> <p>Reference</p>","tags":["Machine Learning"]},{"location":"Knowledge/Machine%20Learning/2023-01-11%20Useful%20Concepts%20in%20Machine%20Learning/#overfitting-underfitting-variance-bias-and-generalisation","title":"Overfitting, Underfitting, Variance, Bias and Generalisation","text":"<p>In general     Overfitting results in High Variance      Underfitting results in High Bias</p> <p>If the data has high number of a constant value in prediction like : maximum 0s  then      Overfitting can result in High Bias</p>","tags":["Machine Learning"]},{"location":"Knowledge/Machine%20Learning/2023-01-11%20Useful%20Concepts%20in%20Machine%20Learning/#todo","title":"ToDo","text":"<ul> <li>A/B Testing</li> <li>A-A-B Testing</li> <li>VC_dimension)</li> </ul>","tags":["Machine Learning"]},{"location":"Knowledge/Machine%20Learning/Image%20Data/","title":"Image Data","text":""},{"location":"Knowledge/Machine%20Learning/Image%20Data/#image-tasks","title":"Image Tasks","text":"<ul> <li>Object Detection : Bounding box based : Faster and easier</li> <li>Image Segmentation / Semantic Segmentation : Pixel level classification : Slower and harder</li> </ul>"},{"location":"Knowledge/Machine%20Learning/Image%20Data/#deep-learning-based-methods","title":"Deep Learning based methods","text":"<ol> <li>UNets</li> <li>Fully Convolutional Network</li> <li>Mask RCNN</li> <li>SegNet</li> <li>R-CNN, Fast R-CNN, Faster R-CNN</li> <li>SSD</li> <li>Retina Net</li> <li>Yolo</li> </ol>"},{"location":"Knowledge/Machine%20Learning/Image%20Data/#unets","title":"UNets","text":"<ul> <li>https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/</li> <li>Up-Conv</li> <li>Crop &amp; Copy</li> </ul>"},{"location":"Knowledge/Machine%20Learning/Image%20Data/#performace-metrics","title":"Performace Metrics","text":"<ul> <li>Jaccard Similarity (Intersection Over Union)</li> <li>AU-PR curve, mAP (mean Average Precision) </li> <li></li> </ul>"},{"location":"Knowledge/Programming/2022-12-16%20Speed%20Up%20Your%20Python%20Code/","title":"Speed Up Your Python Code","text":"","tags":["python"]},{"location":"Knowledge/Programming/2022-12-16%20Speed%20Up%20Your%20Python%20Code/#learn-to-use-itertools-library","title":"Learn to use itertools library","text":"<p>Python\u2019s Itertools is a module that provides various functions that work on iterators to produce complex iterators. This module works as a fast, memory-efficient tool that is used either by themselves or in combination to form iterator algebra.</p>","tags":["python"]},{"location":"Knowledge/Programming/2022-12-16%20Speed%20Up%20Your%20Python%20Code/#use-proper-data-structure","title":"Use proper data structure","text":"<p>Use of proper data structure has a significant effect on runtime. Python has list, tuple, set and dictionary as the built-in data structures.</p> <p>However, most of the people use the list in all cases. But it is not a right choice. Use proper data structures depending on your task. </p> <p>Especially use a tuple instead of a list. Because iterating over tuple is easier than iterating over a list.</p>","tags":["python"]},{"location":"Knowledge/Programming/2022-12-16%20Speed%20Up%20Your%20Python%20Code/#use-generators","title":"Use generators","text":"<p>If you have a large amount of data in your list and you need to use one data at a time and for once then use generators. It will save you time. </p>","tags":["python"]},{"location":"Knowledge/Programming/2022-12-16%20Speed%20Up%20Your%20Python%20Code/#use-list-comprehensions","title":"Use list comprehensions","text":"<pre><code>cube_numbers = []\nfor n in range(0,10):\n    if n % 2 == 1:\n        cube_numbers.append(n**3)\n</code></pre> <p>Don't use such loops, Instead use list comprehension approach as shown below</p> <pre><code>cube_numbers = [n**3 for n in range(1,10) if n%2 == 1]\n</code></pre>","tags":["python"]},{"location":"Knowledge/Programming/2022-12-16%20Speed%20Up%20Your%20Python%20Code/#use-sets-and-unions","title":"Use sets and unions","text":"<p><pre><code>a = [1,2,3,4,5]\nb = [2,3,4,5,6]\n\noverlaps = []\nfor x in a:\n  for y in b:\n    if x==y:\n      overlaps.append(x)\n\nprint(overlaps)\n</code></pre> This will print the list [2, 3, 4, 5]</p> <p>Try not using such for nested loops Rather use sets and unions when ever you can</p> <p><pre><code>a = [1,2,3,4,5]\nb = [2,3,4,5,6]\n\noverlaps = set(a) &amp; set(b)\n\nprint(overlaps)\n</code></pre> This will print the dictionary {2, 3, 4, 5}</p>","tags":["python"]},{"location":"Knowledge/Programming/2022-12-16%20Speed%20Up%20Your%20Python%20Code/#use-join-to-concatenate-strings","title":"Use <code>join()</code> to concatenate strings","text":"<pre><code>new = \"This\" + \"is\" + \"going\" + \"to\" + \"require\" + \"a\" + \"new\" + \"string\" + \"for\" + \"every\" + \"word\"\n</code></pre> <p>Don't use \"+\" to join strings rather use the str.join method</p> <pre><code>new = \" \".join([\"This\", \"will\", \"only\", \"create\", \"one\", \"string\", \"and\", \"we\", \"can\", \"add\", \"spaces.\"])\n</code></pre>","tags":["python"]},{"location":"Knowledge/Programming/2022-12-16%20Speed%20Up%20Your%20Python%20Code/#use-multiple-assignments","title":"Use multiple assignments","text":"<p>Do not assaign variables like this:</p> <pre><code>a = 2\nb = 3\nc = 5\nd = 7\n</code></pre> <p>Instead, assign variables like this: <pre><code> a, b, c, d = 2, 3, 5, 7\n</code></pre></p>","tags":["python"]},{"location":"Knowledge/Programming/2022-12-16%20Speed%20Up%20Your%20Python%20Code/#do-not-use-dot-operation","title":"Do not use dot operation","text":"<p>Because when you call a function using . (dot) it first calls getattribute() or getattr() which then use dictionary operation which costs time. So, try using from module import function.</p> <p><pre><code>import math\nval = math.sqrt(60)\n</code></pre> Rather import at like this: <pre><code>from math import sqrt\nval = sqrt(60)\n</code></pre></p>","tags":["python"]},{"location":"Knowledge/Programming/2022-12-17%20Data%20Structures%20%26%20Algorithms/","title":"Data Structures & Algorithms","text":"","tags":["Data Structures","python"]},{"location":"Knowledge/Programming/2022-12-17%20Data%20Structures%20%26%20Algorithms/#space-time-complexity","title":"Space Time Complexity","text":"<p>Aim of space time complexity theory is  to measure or quantify how much time and space a program or an algorithm takes in execution.</p> <ul> <li>Given a list has <code>n</code> elements, </li> <li>Question : How many comparisons are needed in the worst case to find if a number <code>x</code> is in a list </li> <li>Solution we would apply to solve this Query: Go one by one and check each element in the list and compare </li> <li>In the worst case we would not have that element in the list and we would transverse the complete list</li> <li>Hence we would have to perform  <code>n</code>  comparisons.</li> </ul> <pre><code>n = 4\nl = [1,2,3,4]\nx = 5\nfor i in l:\n    if i == x:\n        print('the element exists')\n</code></pre> <p>Time Complexity for above problem is directly proportional to the length of the list </p>","tags":["Data Structures","python"]},{"location":"Knowledge/Programming/2022-12-17%20Data%20Structures%20%26%20Algorithms/#further-reading","title":"Further Reading :","text":"<ol> <li>Data Structures and Algorithms - https://jovian.ai/learn/data-structures-and-algorithms-in-python - https://www.youtube.com/watch?v=pkYVOmU3MgA - https://distill.pub/ - https://www.geeksforgeeks.org/data-structures/?ref=shm - Space &amp; Time Complexity</li> </ol>","tags":["Data Structures","python"]}]}